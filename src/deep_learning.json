[
{"title": "Reduced-order deep learning for flow dynamics. The interplay between deep learning and model reduction", "authors": "Wang, M; Cheung, SW; Leung, WT; Chung, ET; Efendiev, Y; Wheeler, M", "published_date": "January 2020", "doi": "10.1016/j.jcp.2019.108939", "abstract": " ", "publication_location": "Journal of Computational Physics", "link": "http://dx.doi.org/10.1016/j.jcp.2019.108939", "citations": "2", "readership": "21", "tweets": "1", "news_mentions": " "},
{"title": "Parallelism in Deep Learning Accelerators", "authors": " ", "published_date": "January 1, 2020", "doi": "10.1109/ASP-DAC47756.2020.9045206", "abstract": "© 2020 IEEE. Deep learning is the core of artificial intelligence and it achieves state-of-the-art in a wide range of applications. The intensity of computation and data in deep learning processing poses significant challenges to the conventional computing platforms. Thus, specialized accelerator architectures are proposed for the acceleration of deep learning. In this paper, we classify the design space of current deep learning accelerators into three levels, (1) processing engine, (2) memory and (3) accelerator, and present a constructive view from a perspective of parallelism in the three levels.", "publication_location": "Proceedings of the Asia and South Pacific Design Automation Conference, Asp Dac", "link": "http://dx.doi.org/10.1109/ASP-DAC47756.2020.9045206", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning in ophthalmology: The technical and clinical considerations.", "authors": "Ting, DSW; Peng, L; Varadarajan, AV; Keane, PA; Burlina, PM; Chiang, MF; Schmetterer, L; Pasquale, LR; Bressler, NM; Webster, DR; Abramoff, M; Wong, TY", "published_date": "September 2019", "doi": "10.1016/j.preteyeres.2019.04.003", "abstract": "The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.", "publication_location": "Prog Retin Eye Res", "link": "http://dx.doi.org/10.1016/j.preteyeres.2019.04.003", "citations": "44", "readership": "187", "tweets": "5", "news_mentions": " "},
{"title": "Artificial intelligence and deep learning in ophthalmology.", "authors": "Ting, DSW; Pasquale, LR; Peng, L; Campbell, JP; Lee, AY; Raman, R; Tan, GSW; Schmetterer, L; Keane, PA; Wong, TY", "published_date": "February 2019", "doi": "10.1136/bjophthalmol-2018-313173", "abstract": "Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI 'black-box' algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.", "publication_location": "British Journal of Ophthalmology", "link": "http://dx.doi.org/10.1136/bjophthalmol-2018-313173", "citations": "97", "readership": "365", "tweets": "26", "news_mentions": "2"},
{"title": "On Deep Learning for Medical Image Analysis.", "authors": "Carin, L; Pencina, MJ", "published_date": "September 18, 2018", "doi": "10.1001/jama.2018.13316", "abstract": " ", "publication_location": "Jama", "link": "http://dx.doi.org/10.1001/jama.2018.13316", "citations": "12", "readership": "245", "tweets": "47", "news_mentions": " "},
{"title": "Preconditioned spectral descent for deep learning", "authors": "Carlson, DE; Collins, E; Hsieh, YP; Carin, L; Cevher, V", "published_date": "January 1, 2015", "doi": " ", "abstract": "Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that exploits the so far unused \"geometry\" in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADAgrad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning for retinopathy of prematurity screening.", "authors": "Ting, DSW; Wu, W-C; Toth, C", "published_date": "November 23, 2018", "doi": "10.1136/bjophthalmol-2018-313290", "abstract": " ", "publication_location": "British Journal of Ophthalmology", "link": "http://dx.doi.org/10.1136/bjophthalmol-2018-313290", "citations": "12", "readership": "27", "tweets": "9", "news_mentions": " "},
{"title": "Deep Learning with Hierarchical Convolutional Factor Analysis.", "authors": "Chen, B; Polatkan, G; Sapiro, G; Blei, D; Dunson, D; Carin, L", "published_date": "January 2013", "doi": " ", "abstract": "Unsupervised multi-layered (\"deep\") models are considered for general data, with a particular focus on imagery. The model is represented using a hierarchical convolutional factor-analysis construction, with sparse factor loadings and scores. The computation of layer-dependent model parameters is implemented within a Bayesian setting, employing a Gibbs sampler and variational Bayesian (VB) analysis, that explicitly exploit the convolutional nature of the expansion. In order to address large-scale and streaming data, an online version of VB is also developed. The number of basis functions or dictionary elements at each layer is inferred from the data, based on a beta-Bernoulli implementation of the Indian buffet process. Example results are presented for several image-processing applications, with comparisons to related models in the literature.", "publication_location": "Ieee Transactions on Pattern Analysis and Machine Intelligence", "link": "http://www.ncbi.nlm.nih.gov/pmc/articles/23319498", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Monitoring significant ST changes through deep learning.", "authors": "Xiao, R; Xu, Y; Pelter, MM; Fidler, R; Badilini, F; Mortara, DW; Hu, X", "published_date": "November 2018", "doi": "10.1016/j.jelectrocard.2018.07.026", "abstract": " ", "publication_location": "Journal of Electrocardiology", "link": "http://dx.doi.org/10.1016/j.jelectrocard.2018.07.026", "citations": "5", "readership": "17", "tweets": "3", "news_mentions": " "},
{"title": "Prediction of Discretization of GMsFEM Using Deep Learning", "authors": "Wang, M; Cheung, SW; Chung, ET; Efendiev, Y; Leung, WT; Wang, Y", "published_date": " ", "doi": "10.3390/math7050412", "abstract": "In this paper, we propose a deep-learning-based approach to a class of multiscale problems. The generalized multiscale finite element method (GMsFEM) has been proven successful as a model reduction technique of flow problems in heterogeneous and high-contrast porous media. The key ingredients of GMsFEM include mutlsicale basis functions and coarse-scale parameters, which are obtained from solving local problems in each coarse neighborhood. Given a fixed medium, these quantities are precomputed by solving local problems in an offline stage, and result in a reduced-order model. However, these quantities have to be re-computed in case of varying media (various permeability fields). The objective of our work is to use deep learning techniques to mimic the nonlinear relation between the permeability field and the GMsFEM discretizations, and use neural networks to perform fast computation of GMsFEM ingredients repeatedly for a class of media. We provide numerical experiments to investigate the predictive power of neural networks and the usefulness of the resultant multiscale model in solving channelized porous media flow problems.", "publication_location": "Mathematics", "link": "http://dx.doi.org/10.3390/math7050412", "citations": "1", "readership": "11", "tweets": "3", "news_mentions": " "},
{"title": "ReRAM-based accelerator for deep learning", "authors": "Li, B; Song, L; Chen, F; Qian, X; Chen, Y; Li, H", "published_date": "April 19, 2018", "doi": "10.23919/DATE.2018.8342118", "abstract": "© 2018 EDAA. Big data computing applications such as deep learning and graph analytic usually incur a large amount of data movements. Deploying such applications on conventional von Neumann architecture that separates the processing units and memory components likely leads to performance bottleneck due to the limited memory bandwidth. A common approach is to develop architecture and memory co-design methodologies to overcome the challenge. Our research follows the same strategy by leveraging resistive memory (ReRAM) to further enhance the performance and energy efficiency. Specifically, we employ the general principles behind processing-in-memory to design efficient ReRAM based accelerators that support both testing and training operations. Related circuit and architecture optimization will be discussed too.", "publication_location": "Proceedings of the 2018 Design, Automation and Test in Europe Conference and Exhibition, Date 2018", "link": "http://dx.doi.org/10.23919/DATE.2018.8342118", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Using Deep Learning to Automate Goldmann Applanation Tonometry Readings.", "authors": "Spaide, T; Wu, Y; Yanagihara, RT; Feng, S; Ghabra, O; Yi, JS; Chen, PP; Moses, F; Lee, AY; Wen, JC", "published_date": "April 25, 2020", "doi": "10.1016/j.ophtha.2020.04.033", "abstract": "PURPOSE: To develop an objective and automated method for measuring intraocular pressure using deep learning and fixed-force Goldmann applanation tonometry (GAT) techniques. DESIGN: Prospective cross-sectional study. PARTICIPANTS: Patients from an academic glaucoma practice. METHODS: Intraocular pressure was estimated by analyzing videos recorded using a standard slit-lamp microscope and fixed-force GAT. Video frames were labeled to identify the outline of the reference tonometer and the applanation mires. A deep learning model was trained to localize and segment the tonometer and mires. Intraocular pressure values were calculated from the deep learning-predicted tonometer and mire diameters using the Imbert-Fick formula. A separate test set was collected prospectively in which standard and automated GAT measurements were collected in random order by 2 independent masked observers to assess the deep learning model as well as interobserver variability. MAIN OUTCOME MEASURES: Intraocular pressure measurements between standard and automated methods were compared. RESULTS: Two hundred sixty-three eyes of 135 patients were included in the training and validation videos. For the test set, 50 eyes from 25 participants were included. Each eye was measured by 2 observers, resulting in 100 videos. Within the test set, the mean difference between automated and standard GAT results was -0.9 mmHg (95% limits of agreement [LoA], -5.4 to 3.6 mmHg). Mean difference between the 2 observers using standard GAT was 0.09 mmHg (LoA,-3.8 to 4.0 mmHg). Mean difference between the 2 observers using automated GAT videos was -0.3 mmHg (LoA, -4.1 to 3.5 mmHg). The coefficients of repeatability for automated and standard GAT were 3.8 and 3.9 mmHg, respectively. The bias for even-numbered measurements was reduced when using automated GAT. CONCLUSIONS: Preliminary measurements using deep learning to automate GAT demonstrate results comparable with those of standard GAT. Automated GAT has the potential to improve on our current GAT measurement standards significantly by reducing bias and improving repeatability. In addition, ocular pulse amplitudes could be observed using this technique.", "publication_location": "Ophthalmology", "link": "http://dx.doi.org/10.1016/j.ophtha.2020.04.033", "citations": " ", "readership": " ", "tweets": "36", "news_mentions": " "},
{"title": "Deep learning for identifying radiogenomic associations in breast cancer.", "authors": "Zhu, Z; Albadawy, E; Saha, A; Zhang, J; Harowicz, MR; Mazurowski, MA", "published_date": "June 2019", "doi": "10.1016/j.compbiomed.2019.04.018", "abstract": "RATIONALE AND OBJECTIVES: To determine whether deep learning models can distinguish between breast cancer molecular subtypes based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). MATERIALS AND METHODS: In this institutional review board-approved single-center study, we analyzed DCE-MR images of 270 patients at our institution. Lesions of interest were identified by radiologists. The task was to automatically determine whether the tumor is of the Luminal A subtype or of another subtype based on the MR image patches representing the tumor. Three different deep learning approaches were used to classify the tumor according to their molecular subtypes: learning from scratch where only tumor patches were used for training, transfer learning where networks pre-trained on natural images were fine-tuned using tumor patches, and off-the-shelf deep features where the features extracted by neural networks trained on natural images were used for classification with a support vector machine. Network architectures utilized in our experiments were GoogleNet, VGG, and CIFAR. We used 10-fold crossvalidation method for validation and area under the receiver operating characteristic (AUC) as the measure of performance. RESULTS: The best AUC performance for distinguishing molecular subtypes was 0.65 (95% CI:[0.57,0.71]) and was achieved by the off-the-shelf deep features approach. The highest AUC performance for training from scratch was 0.58 (95% CI:[0.51,0.64]) and the best AUC performance for transfer learning was 0.60 (95% CI:[0.52,0.65]) respectively. For the off-the-shelf approach, the features extracted from the fully connected layer performed the best. CONCLUSION: Deep learning may play a role in discovering radiogenomic associations in breast cancer.", "publication_location": "Comput Biol Med", "link": "http://dx.doi.org/10.1016/j.compbiomed.2019.04.018", "citations": "19", "readership": "66", "tweets": "5", "news_mentions": " "},
{"title": "Identifying Smoking Environments From Images of Daily Life With Deep Learning.", "authors": "Engelhard, MM; Oliver, JA; Henao, R; Hallyburton, M; Carin, LE; Conklin, C; McClernon, FJ", "published_date": "August 2, 2019", "doi": "10.1001/jamanetworkopen.2019.7939", "abstract": "Importance: Environments associated with smoking increase a smoker's craving to smoke and may provoke lapses during a quit attempt. Identifying smoking risk environments from images of a smoker's daily life provides a basis for environment-based interventions. Objective: To apply a deep learning approach to the clinically relevant identification of smoking environments among settings that smokers encounter in daily life. Design, Setting, and Participants: In this cross-sectional study, 4902 images of smoking (n = 2457) and nonsmoking (n = 2445) locations were photographed by 169 smokers from Durham, North Carolina, and Pittsburgh, Pennsylvania, areas from 2010 to 2016. These images were used to develop a probabilistic classifier to predict the location type (smoking or nonsmoking location), thus relating objects and settings in daily environments to established smoking patterns. The classifier combines a deep convolutional neural network with an interpretable logistic regression model and was trained and evaluated via nested cross-validation with participant-wise partitions (ie, out-of-sample prediction). To contextualize model performance, images taken by 25 randomly selected participants were also classified by smoking cessation experts. As secondary validation, craving levels reported by participants when viewing unfamiliar environments were compared with the model's predictions. Data analysis was performed from September 2017 to May 2018. Main Outcomes and Measures: Classifier performance (accuracy and area under the receiver operating characteristic curve [AUC]), comparison with 4 smoking cessation experts, contribution of objects and settings to smoking environment status (standardized model coefficients), and correlation with participant-reported craving. Results: Of 169 participants, 106 (62.7%) were from Durham (53 [50.0%] female; mean [SD] age, 41.4 [12.0] years) and 63 (37.3%) were from Pittsburgh (31 [51.7%] female; mean [SD] age, 35.2 [13.8] years). A total of 4902 images were available for analysis, including 3386 from Durham (mean [SD], 31.9 [1.3] images per participant) and 1516 from Pittsburgh (mean [SD], 24.1 [0.5] images per participant). Images were evenly split between the 2 classes, with 2457 smoking images (50.1%) and 2445 nonsmoking images (49.9%). The final model discriminated smoking vs nonsmoking environments with a mean (SD) AUC of 0.840 (0.024) (accuracy [SD], 76.5% [1.6%]). A model trained only with images from Durham participants effectively classified images from Pittsburgh participants (AUC, 0.757; accuracy, 69.2%), and a model trained only with images from Pittsburgh participants effectively classified images from Durham participants (AUC, 0.821; accuracy, 75.0%), suggesting good generalizability between geographic areas. Only 1 expert's performance was a statistically significant improvement compared with the classifier (α = .05). Median self-reported craving was significantly correlated with model-predicted smoking environment status (ρ = 0.894; P = .003). Conclusions and Relevance: In this study, features of daily environments predicted smoking vs nonsmoking status consistently across participants. The findings suggest that a deep learning approach can identify environments associated with smoking, can predict the probability that any image of daily life represents a smoking environment, and can potentially trigger environment-based interventions. This work demonstrates a framework for predicting how daily environments may influence target behaviors or symptoms that may have broad applications in mental and physical health.", "publication_location": "Jama Network Open", "link": "http://dx.doi.org/10.1001/jamanetworkopen.2019.7939", "citations": "1", "readership": "12", "tweets": "19", "news_mentions": " "},
{"title": "Automatic feature learning for glaucoma detection based on deep learning", "authors": "Chen, X; Xu, Y; Yan, S; Wong, DWK; Wong, TY; Liu, J", "published_date": "January 1, 2015", "doi": "10.1007/978-3-319-24574-4_80", "abstract": "© Springer International Publishing Switzerland 2015. Glaucoma is a chronic and irreversible eye disease in which the optic nerve is progressively damaged, leading to deterioration in vision and quality of life. In this paper, we present an Automatic feature Learning for glAucomaDetection based onDeep LearnINg (ALADDIN),with deep convolutional neural network (CNN) for feature learning. Different from the traditional convolutional layer that uses linear filters followed by a nonlinear activation function to scan the input, the adopted network embeds micro neural networks (multilayer perceptron) with more complex structures to abstract the data within the receptive field. Moreover, a contextualizing deep learning structure is proposed in order to obtain a hierarchical representation of fundus images to discriminate between glaucoma and non-glaucoma pattern,where the network takes the outputs fromother CNN as the context information to boost the performance. Extensive experiments are performed on the ORIGA and SCES datasets. The results showarea under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.838 and 0.898 in the two databases,much better than state-of-the-art algorithms. The method could be used for glaucoma diagnosis.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-319-24574-4_80", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning in radiology: An overview of the concepts and a survey of the state of the art with focus on MRI.", "authors": "Mazurowski, MA; Buda, M; Saha, A; Bashir, MR", "published_date": "April 2019", "doi": "10.1002/jmri.26534", "abstract": "Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep-learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mainly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we discuss the general context of radiology and opportunities for application of deep-learning algorithms. We also introduce basic concepts of deep learning, including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review a broad range of deep-learning algorithms being utilized. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future. Level of Evidence: 3 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2019;49:939-954.", "publication_location": "J Magn Reson Imaging", "link": "http://dx.doi.org/10.1002/jmri.26534", "citations": "53", "readership": "366", "tweets": "19", "news_mentions": " "},
{"title": "AutoAudio: Deep Learning for Automatic Audiogram Interpretation", "authors": " ", "published_date": "May 5, 2020", "doi": "10.1101/2020.04.30.20086637", "abstract": "Objectives: Hearing loss is the leading human sensory system loss, and one of the leading causes for years lived with disability with significant effects on quality of life, social isolation, and overall health. Coupled with a forecast of increased hearing loss burden worldwide, national and international health organizations have urgently recommended that access to hearing evaluation be expanded to meet demand. Methods: The objective of this study was to develop, AutoAudio,a novel deep learning proof-of-concept model that accurately and quickly interprets diagnostic audiograms. Adult audiogram reports representing normal, conductive, mixed and sensorineural morphologies were used to train different neural network architectures. Image augmentation techniques were used to increase the training image set size. Classification accuracy on a separate test set was used to assess model performance.  Results: The architecture with the highest out-of-training set accuracy was ResNet-101 at 97.5%. Neural network training time varied between 2 to 7 hours depending on the depth of the neural network architecture. Each neural network architecture produced misclassifications that arose from failures of the model to correctly label the audiogram with the appropriate hearing loss type. The most commonly misclassified hearing loss type were mixed losses.  Conclusion: Re engineering the process of hearing testing with a machine learning innovation may help enhance access to the growing worldwide population that is expected to require audiologist services. Our results suggest that deep learning may be a transformative technology that enables automatic and accurate audiogram interpretation.", "publication_location": " ", "link": "http://dx.doi.org/10.1101/2020.04.30.20086637", "citations": " ", "readership": " ", "tweets": "6", "news_mentions": " "},
{"title": "Deep learning for accelerated all-dielectric metasurface design.", "authors": "Nadell, CC; Huang, B; Malof, JM; Padilla, WJ", "published_date": "September 2019", "doi": "10.1364/oe.27.027523", "abstract": "Deep learning has risen to the forefront of many fields in recent years, overcoming challenges previously considered intractable with conventional means. Materials discovery and optimization is one such field, but significant challenges remain, including the requirement of large labeled datasets and one-to-many mapping that arises in solving the inverse problem. Here we demonstrate modeling of complex all-dielectric metasurface systems with deep neural networks, using both the metasurface geometry and knowledge of the underlying physics as inputs. Our deep learning network is highly accurate, achieving an average mean square error of only 1.16 × 10-3 and is over five orders of magnitude faster than conventional electromagnetic simulation software. We further develop a novel method to solve the inverse modeling problem, termed fast forward dictionary search (FFDS), which offers tremendous controls to the designer and only requires an accurate forward neural network model. These techniques significantly increase the viability of more complex all-dielectric metasurface designs and provide opportunities for the future of tailored light matter interactions.", "publication_location": "Optics Express", "link": "http://dx.doi.org/10.1364/oe.27.027523", "citations": "16", "readership": "125", "tweets": "32", "news_mentions": "10"},
{"title": "Deep Learning to Classify Radiology Free-Text Reports.", "authors": "Chen, MC; Ball, RL; Yang, L; Moradzadeh, N; Chapman, BE; Larson, DB; Langlotz, CP; Amrhein, TJ; Lungren, MP", "published_date": "March 2018", "doi": "10.1148/radiol.2017171115", "abstract": "Purpose To evaluate the performance of a deep learning convolutional neural network (CNN) model compared with a traditional natural language processing (NLP) model in extracting pulmonary embolism (PE) findings from thoracic computed tomography (CT) reports from two institutions. Materials and Methods Contrast material-enhanced CT examinations of the chest performed between January 1, 1998, and January 1, 2016, were selected. Annotations by two human radiologists were made for three categories: the presence, chronicity, and location of PE. Classification of performance of a CNN model with an unsupervised learning algorithm for obtaining vector representations of words was compared with the open-source application PeFinder. Sensitivity, specificity, accuracy, and F1 scores for both the CNN model and PeFinder in the internal and external validation sets were determined. Results The CNN model demonstrated an accuracy of 99% and an area under the curve value of 0.97. For internal validation report data, the CNN model had a statistically significant larger F1 score (0.938) than did PeFinder (0.867) when classifying findings as either PE positive or PE negative, but no significant difference in sensitivity, specificity, or accuracy was found. For external validation report data, no statistical difference between the performance of the CNN model and PeFinder was found. Conclusion A deep learning CNN model can classify radiology free-text reports with accuracy equivalent to or beyond that of an existing traditional NLP model. © RSNA, 2017 Online supplemental material is available for this article.", "publication_location": "Radiology", "link": "http://dx.doi.org/10.1148/radiol.2017171115", "citations": "53", "readership": "123", "tweets": "43", "news_mentions": " "},
{"title": "Management of Thyroid Nodules Seen on US Images: Deep Learning May Match Performance of Radiologists.", "authors": "Buda, M; Wildman-Tobriner, B; Hoang, JK; Thayer, D; Tessler, FN; Middleton, WD; Mazurowski, MA", "published_date": "September 2019", "doi": "10.1148/radiol.2019181343", "abstract": "BackgroundManagement of thyroid nodules may be inconsistent between different observers and time consuming for radiologists. An artificial intelligence system that uses deep learning may improve radiology workflow for management of thyroid nodules.PurposeTo develop a deep learning algorithm that uses thyroid US images to decide whether a thyroid nodule should undergo a biopsy and to compare the performance of the algorithm with the performance of radiologists who adhere to American College of Radiology (ACR) Thyroid Imaging Reporting and Data System (TI-RADS).Materials and MethodsIn this retrospective analysis, studies in patients referred for US with subsequent fine-needle aspiration or with surgical histologic analysis used as the standard were evaluated. The study period was from August 2006 to May 2010. A multitask deep convolutional neural network was trained to provide biopsy recommendations for thyroid nodules on the basis of two orthogonal US images as the input. In the training phase, the deep learning algorithm was first evaluated by using 10-fold cross-validation. Internal validation was then performed on an independent set of 99 consecutive nodules. The sensitivity and specificity of the algorithm were compared with a consensus of three ACR TI-RADS committee experts and nine other radiologists, all of whom interpreted thyroid US images in clinical practice.ResultsIncluded were 1377 thyroid nodules in 1230 patients with complete imaging data and conclusive cytologic or histologic diagnoses. For the 99 test nodules, the proposed deep learning algorithm achieved 13 of 15 (87%: 95% confidence interval [CI]: 67%, 100%) sensitivity, the same as expert consensus (P > .99) and higher than five of nine radiologists. The specificity of the deep learning algorithm was 44 of 84 (52%; 95% CI: 42%, 62%), which was similar to expert consensus (43 of 84; 51%; 95% CI: 41%, 62%; P = .91) and higher than seven of nine other radiologists. The mean sensitivity and specificity for the nine radiologists was 83% (95% CI: 64%, 98%) and 48% (95% CI: 37%, 59%), respectively.ConclusionSensitivity and specificity of a deep learning algorithm for thyroid nodule biopsy recommendations was similar to that of expert radiologists who used American College of Radiology Thyroid Imaging and Reporting Data System guidelines.© RSNA, 2019Online supplemental material is available for this article.", "publication_location": "Radiology", "link": "http://dx.doi.org/10.1148/radiol.2019181343", "citations": "14", "readership": "48", "tweets": "52", "news_mentions": "4"},
{"title": "Deep Learning of the Nonlinear Schrödinger Equation in Fiber-Optic Communications", "authors": "Hager, C; Pfister, HD", "published_date": "August 15, 2018", "doi": "10.1109/ISIT.2018.8437734", "abstract": "© 2018 IEEE. An important problem in fiber-optic communications is to invert the nonlinear Schrödinger equation in real time to reverse the deterministic effects of the channel. Interestingly, the popular split-step Fourier method (SSFM) leads to a computation graph that is reminiscent of a deep neural network. This observation allows one to leverage tools from machine learning to reduce complexity. In particular, the main disadvantage of the SSFM is that its complexity using M steps is at least M times larger than a linear equalizer. This is because the linear SSFM operator is a dense matrix. In previous work, truncation methods such as frequency sampling, wavelets, or least-squares have been used to obtain 'cheaper' operators that can be implemented using filters. However, a large number of filter taps are typically required to limit truncation errors. For example, Ip and Kahn showed that for a 10 Gbaud signal and 2000 km optical link, a truncated SSFM with 25 steps would require 70-tap filters in each step and 100 times more operations than linear equalization. We find that, by jointly optimizing all filters with deep learning, the complexity can be reduced significantly for similar accuracy. Using optimized 5-tap and 3-tap filters in an alternating fashion, one requires only around 2-6 times the complexity of linear equalization, depending on the implementation.", "publication_location": "Ieee International Symposium on Information Theory   Proceedings", "link": "http://dx.doi.org/10.1109/ISIT.2018.8437734", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "ICHNet: Intracerebral hemorrhage (ICH) segmentation using deep learning", "authors": "Islam, M; Sanghani, P; See, AAQ; James, ML; King, NKK; Ren, H", "published_date": "January 1, 2019", "doi": "10.1007/978-3-030-11723-8_46", "abstract": "© Springer Nature Switzerland AG 2019. We develop a deep learning approach for automated intracerebral hemorrhage (ICH) segmentation from 3D computed tomography (CT) scans. Our model, ICHNet, evolves by integrating dilated convolution neural network (CNN) with hypercolumn features where a modest number of pixels are sampled and corresponding features from multiple layers are concatenated. Due to freedom of sampling pixels rather than image patch, this model trains within the brain region and ignores the CT background padding. This boosts the convergence time and accuracy by learning only healthy and defected brain tissues. To overcome the class imbalance problem, we sample an equal number of pixels from each class. We also incorporate 3D conditional random field (3D CRF) to smoothen the predicted segmentation as a post-processing step. ICHNet demonstrates 87.6% Dice accuracy in hemorrhage segmentation, that is comparable to radiologists.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-030-11723-8_46", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic Kellgren-Lawrence grade estimation driven deep learning algorithms", "authors": "Li, N; Swiecicki, A; Said, N; O'Donnell, J; Jiranek, WA; Mazurowski, MA", "published_date": "January 1, 2020", "doi": "10.1117/12.2551392", "abstract": "© 2020 SPIE. Knee osteoarthritis (OA) is a prevalent and disabling degenerative joint disease. Objectively identifying knee OA severity is challenging given significant inter-reader variability due to human interpretation factors. The Kellgren-Lawrence (KL) grading system is a commonly used scale to quantitatively characterize the severity of knee OA in knee radiographs. It is important to reliably identify severe knee OA since total knee arthroplasty (TKA) can provide significant improvement in patient quality of life for patients with severe knee OA. In this study, we demonstrate a deep learning approach to automatically assessing KL grades. Our approach uses faster R-CNN object detection network to identify the knee region and deep convolutional neural network for classification. We used a dataset of 7962 knee radiographs for each posteroanterior (PA) and lateral (LAT) views, to develop and evaluate our approach. Images with their corresponding KL grades were obtained from the Multicenter Osteoarthritis Study (MOST) dataset. Our network showed multi-class classification accuracy of 69.15 % when the assessment was made based on PA views and accuracy of 56.68 % when LAT views were used. The developed network may play a significant role in surgical decision-making regarding knee replacement surgery.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2551392", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning", "authors": "Song, L; Qian, X; Li, H; Chen, Y", "published_date": "May 5, 2017", "doi": "10.1109/HPCA.2017.55", "abstract": "© 2017 IEEE. Convolution neural networks (CNNs) are the heart of deep learning applications. Recent works PRIME [1] and ISAAC [2] demonstrated the promise of using resistive random access memory (ReRAM) to perform neural computations in memory. We found that training cannot be efficiently supported with the current schemes. First, they do not consider weight update and complex data dependency in training procedure. Second, ISAAC attempts to increase system throughput with a very deep pipeline. It is only beneficial when a large number of consecutive images can be fed into the architecture. In training, the notion of batch (e.g. 64) limits the number of images can be processed consecutively, because the images in the next batch need to be processed based on the updated weights. Third, the deep pipeline in ISAAC is vulnerable to pipeline bubbles and execution stall. In this paper, we present PipeLayer, a ReRAM-based PIM accelerator for CNNs that support both training and testing. We analyze data dependency and weight update in training algorithms and propose efficient pipeline to exploit inter-layer parallelism. To exploit intra-layer parallelism, we propose highly parallel design based on the notion of parallelism granularity and weight replication. With these design choices, PipeLayer enables the highly pipelined execution of both training and testing, without introducing the potential stalls in previous work. The experiment results show that, PipeLayer achieves the speedups of 42.45x compared with GPU platform on average. The average energy saving of PipeLayer compared with GPU implementation is 7.17x.", "publication_location": "Proceedings   International Symposium on High Performance Computer Architecture", "link": "http://dx.doi.org/10.1109/HPCA.2017.55", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "TernGrad: Ternary gradients to reduce communication in distributed deep learning", "authors": "Wen, W; Xu, C; Yan, F; Wu, C; Wang, Y; Chen, Y; Li, H", "published_date": "January 1, 2017", "doi": " ", "abstract": "© 2017 Neural information processing systems foundation. All rights reserved. High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1, 0, 1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet doesn't incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available[1].", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "How to obtain and run light and efficient deep learning networks", "authors": "Chen, F; Wen, W; Song, L; Zhang, J; Li, HH; Chen, Y", "published_date": "November 1, 2019", "doi": "10.1109/ICCAD45719.2019.8942106", "abstract": "© 2019 IEEE. As the model size of deep neural networks (DNNs) grows for better performance, the increase in computational cost associated with training and testing makes it extremely difficulty to deploy DNNs on end/edge devices with limited resources while also satisfying the response time requirement. To address this challenge, model compression which compresses model size and thus reduces computation cost is widely adopted in deep learning society. However, the practical impacts of hardware design are often ignored in these algorithm-level solutions, such as the increase of the random accesses to memory hierarchy and the constraints of memory capacity. On the other side, limited understanding about the computational needs at algorithm level may lead to unrealistic assumptions during the hardware designs. In this work, we will discuss this mismatch and provide how our approach addresses it through an interactive design practice across both software and hardware levels.", "publication_location": "Ieee/Acm International Conference on Computer Aided Design, Digest of Technical Papers, Iccad", "link": "http://dx.doi.org/10.1109/ICCAD45719.2019.8942106", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A Deep Learning Approach to Unsupervised Ensemble Learning", "authors": "Cheng, X; Shaham, U; Dror, O; Jaffe, A; Nadler, B; Chang, J; Kluger, Y", "published_date": "June 2016", "doi": " ", "abstract": " ", "publication_location": "PMLR", "link": "http://proceedings.mlr.press/v48/shaham16.html", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic feature learning to grade nuclear cataracts based on deep learning", "authors": "Gao, X; Lin, S; Wong, TY", "published_date": "January 1, 2015", "doi": "10.1007/978-3-319-16808-1_42", "abstract": "© Springer International Publishing Switzerland 2015. Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease. Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this work, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters learned from image patches are fed into a convolutional neural network, followed by a set of recursive neural networks to further extract higher-order features. With these features, support vector regression is applied to determine the cataract grade. The proposed system is validated on a large population-based dataset of 5378 images, where it outperforms the state-of-the-art by yielding with respect to clinical grading a mean absolute error (ε) of 0.322, a 68.6% exact integral agreement ratio (R0), a 86.5% decimal grading error ≤0.5 (Re0.5), and a 99.1% decimal grading error ≤1.0 (Re1.0).", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-319-16808-1_42", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic Feature Learning to Grade Nuclear Cataracts Based on Deep Learning.", "authors": "Gao, X; Lin, S; Wong, TY", "published_date": "November 2015", "doi": "10.1109/TBME.2015.2444389", "abstract": "GOAL: Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease. METHODS: Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this study, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters are first acquired through clustering of image patches from lenses within the same grading class. The learned filters are fed into a convolutional neural network, followed by a set of recursive neural networks, to further extract higher order features. With these features, support vector regression is applied to determine the cataract grade. RESULTS: The proposed system is validated on a large population-based dataset of [Formula: see text] images, where it outperforms the state of the art by yielding with respect to clinical grading a mean absolute error ( ε) of 0.304, a 70.7% exact integral agreement ratio ( R0), an 88.4% decimal grading error ≤ 0.5 ( Re0.5 ), and a 99.0% decimal grading error ≤ 1.0 ( Re1.0 ). SIGNIFICANCE: The proposed method is useful for assisting and improving clinical management of the disease in the context of large-population screening and has the potential to be applied to other eye diseases.", "publication_location": "Ieee Trans Biomed Eng", "link": "http://dx.doi.org/10.1109/TBME.2015.2444389", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Augmentation of CBCT Reconstructed From Under-Sampled Projections Using Deep Learning.", "authors": "Jiang, Z; Chen, Y; Zhang, Y; Ge, Y; Yin, F-F; Ren, L", "published_date": "November 2019", "doi": "10.1109/TMI.2019.2912791", "abstract": "Edges tend to be over-smoothed in total variation (TV) regularized under-sampled images. In this paper, symmetric residual convolutional neural network (SR-CNN), a deep learning based model, was proposed to enhance the sharpness of edges and detailed anatomical structures in under-sampled cone-beam computed tomography (CBCT). For training, CBCT images were reconstructed using TV-based method from limited projections simulated from the ground truth CT, and were fed into SR-CNN, which was trained to learn a restoring pattern from under-sampled images to the ground truth. For testing, under-sampled CBCT was reconstructed using TV regularization and was then augmented by SR-CNN. Performance of SR-CNN was evaluated using phantom and patient images of various disease sites acquired at different institutions both qualitatively and quantitatively using structure similarity (SSIM) and peak signal-to-noise ratio (PSNR). SR-CNN substantially enhanced image details in the TV-based CBCT across all experiments. In the patient study using real projections, SR-CNN augmented CBCT images reconstructed from as low as 120 half-fan projections to image quality comparable to the reference fully-sampled FDK reconstruction using 900 projections. In the tumor localization study, improvements in the tumor localization accuracy were made by the SR-CNN augmented images compared with the conventional FDK and TV-based images. SR-CNN demonstrated robustness against noise levels and projection number reductions and generalization for various disease sites and datasets from different institutions. Overall, the SR-CNN-based image augmentation technique was efficient and effective in considerably enhancing edges and anatomical structures in under-sampled 3D/4D-CBCT, which can be very valuable for image-guided radiotherapy.", "publication_location": "Ieee Trans Med Imaging", "link": "http://dx.doi.org/10.1109/TMI.2019.2912791", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Towards decentralized deep learning with differential privacy", "authors": " ", "published_date": "January 1, 2019", "doi": "10.1007/978-3-030-23502-4_10", "abstract": "© Springer Nature Switzerland AG 2019. In distributed machine learning, while a great deal of attention has been paid on centralized systems that include a central parameter server, decentralized systems have not been fully explored. Decentralized systems have great potentials in the future practical use as they have multiple useful attributes such as less vulnerable to privacy and security issues, better scalability, and less prone to single point of bottleneck and failure. In this paper, we focus on decentralized learning systems and aim to achieve differential privacy with good convergence rate and low communication cost. To achieve this goal, we propose a new algorithm, Leader-Follower Elastic Averaging Stochastic Gradient Descent (LEASGD), driven by a novel Leader-Follower topology and differential privacy model. We also provide a theoretical analysis of the convergence rate of LEASGD and the trade-off between the performance and privacy in the private setting. We evaluate LEASGD in real distributed testbed with poplar deep neural network models MNIST-CNN, MNIST-RNN, and CIFAR-10. Extensive experimental results show that LEASGD outperforms state-of-the-art decentralized learning algorithm DPSGD by achieving nearly 40% lower loss function within same iterations and by 30% reduction of communication cost. Moreover, it spends less differential privacy budget and has final higher accuracy result than DPSGD under private setting.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-030-23502-4_10", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Reduced-order Deep Learning for Flow Dynamics", "authors": " ", "published_date": " ", "doi": " ", "abstract": "In this paper, we investigate neural networks applied to multiscalesimulations and discuss a design of a novel deep neural network model reductionapproach for multiscale problems. Due to the multiscale nature of the medium,the fine-grid resolution gives rise to a huge number of degrees of freedom. Inpractice, low-order models are derived to reduce the computational cost. In ourpaper, we use a non-local multicontinuum (NLMC) approach, which represents thesolution on a coarse grid [18]. Using multi-layer learning techniques, weformulate and learn input-output maps constructed with NLMC on a coarse grid.We study the features of the coarse-grid solutions that neural networks capturevia relating the input-output optimization to $l_1$ minimization of PDEsolutions. In proposed multi-layer networks, we can learn the forward operatorsin a reduced way without computing them as in POD like approaches. We presentsoft thresholding operators as activation function, which our studies show tohave some advantages. With these activation functions, the neural networkidentifies and selects important multiscale features which are crucial inmodeling the underlying flow. Using trained neural network approximation of theinput-output map, we construct a reduced-order model for the solutionapproximation. We use multi-layer networks for the time stepping andreduced-order modeling, where at each time step the appropriate important modesare selected. For a class of nonlinear problems, we suggest an efficientstrategy. Numerical examples are presented to examine the performance of ourmethod.", "publication_location": " ", "link": "http://arxiv.org/abs/1901.10343v2", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Artificial Intelligence With Deep Learning Technology Looks Into Diabetic Retinopathy Screening.", "authors": " ", "published_date": "December 2016", "doi": "10.1001/jama.2016.17563", "abstract": " ", "publication_location": "Jama", "link": "http://dx.doi.org/10.1001/jama.2016.17563", "citations": "93", "readership": "166", "tweets": "69", "news_mentions": "6"},
{"title": "Deep Learning of Pulmonary Function in CT Images Based On Radiomic Filtering", "authors": "Lafata, K; Cai, J; Liu, J; Sidhu, K; Yin, F", "published_date": "June 1, 2018", "doi": " ", "abstract": " ", "publication_location": "Medical Physics", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000434978000142&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning for coastal resource conservation: automating detection of shellfish reefs", "authors": "Ridge, JT; Gray, PC; Windle, AE; Johnston, DW", "published_date": " ", "doi": "10.1002/rse2.134", "abstract": " ", "publication_location": "Remote Sensing in Ecology and Conservation", "link": "http://dx.doi.org/10.1002/rse2.134", "citations": "1", "readership": " ", "tweets": "46", "news_mentions": " "},
{"title": "Variational Autoencoder for Deep Learning of Images, Labels and Captions.", "authors": "Pu, Y; Gan, Z; Henao, R; Yuan, X; Li, C; Stevens, A; Carin, L", "published_date": "2016", "doi": " ", "abstract": " ", "publication_location": "Nips", "link": "http://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Comparison of Deep Learning and Classical Image Processing for Skin Segmentation", "authors": "Jin, FQ; Postiglione, M; Knight, AE; Cardones, AR; Nightingale, KR; Palmeri, ML", "published_date": "October 1, 2019", "doi": "10.1109/ULTSYM.2019.8926233", "abstract": "© 2019 IEEE. Skin stiffness correlates with the progression of sclerotic skin diseases. Ultrasound shear wave elasticity imaging techniques can measure skin stiffness, but an accurate skin thickness measurement is required to compute the elastic modulus. We explored different automated methods to segment the skin for use in real-time skin elastography. Local gradient-based methods could not robustly segment the skin on our B-mode images, so we developed a new thresholding method to detect the edges of the skin. We also used our thresholding method to generate labels to train a deep neural network. We compared the performance of thresholding and the trained network for central thickness estimation on a held-out test set. Our thresholding method correctly segmented 58% of images, and the neural network correctly segmented 82%. More than half of thresholding failures on the test set were from overestimation of the bottom skin boundary. The neural network had significantly less overestimation failures and similar rates of failure due to bubbles and underestimation.", "publication_location": "Ieee International Ultrasonics Symposium, Ius", "link": "http://dx.doi.org/10.1109/ULTSYM.2019.8926233", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Unmasking the Tissue Microecology of Ductal Carcinoma In Situ with Deep Learning", "authors": "Narayanan, PL; Raza, SEA; Hall, A; Marks, JR; King, L; Dowsett, M; Gusterson, B; Maley, C; Hwang, ES; Yuan, Y", "published_date": "September 1, 2019", "doi": " ", "abstract": " ", "publication_location": "The Journal of Pathology", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000486167400085&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A Deep Learning Approach to Analysis of Kidney Transplant Frozen Sections", "authors": "Davis, R; Lafata, K; Li, X; Souma, N; Howell, D; Shen, X; Barisoni, L", "published_date": "March 1, 2020", "doi": " ", "abstract": " ", "publication_location": "Modern Pathology : an Official Journal of the United States and Canadian Academy of Pathology, Inc", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000518328903352&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A Deep Learning Approach to Analysis of Kidney Transplant Frozen Sections", "authors": "Davis, R; Lafata, K; Li, X; Souma, N; Howell, D; Shen, X; Barisoni, L", "published_date": "March 1, 2020", "doi": " ", "abstract": " ", "publication_location": "Laboratory Investigation", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000518328803351&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning with hierarchical convolutional factor analysis.", "authors": "Chen, B; Polatkan, G; Sapiro, G; Blei, D; Dunson, D; Carin, L", "published_date": "August 2013", "doi": "10.1109/tpami.2013.19", "abstract": "Unsupervised multilayered (“deep”) models are considered for imagery. The model is represented using a hierarchical convolutional factor-analysis construction, with sparse factor loadings and scores. The computation of layer-dependent model parameters is implemented within a Bayesian setting, employing a Gibbs sampler and variational Bayesian (VB) analysis that explicitly exploit the convolutional nature of the expansion. To address large-scale and streaming data, an online version of VB is also developed. The number of dictionary elements at each layer is inferred from the data, based on a beta-Bernoulli implementation of the Indian buffet process. Example results are presented for several image-processing applications, with comparisons to related models in the literature.", "publication_location": "Ieee Transactions on Pattern Analysis and Machine Intelligence", "link": "http://dx.doi.org/10.1109/tpami.2013.19", "citations": "62", "readership": "219", "tweets": "2", "news_mentions": " "},
{"title": "AccPar: Tensor partitioning for heterogeneous deep learning accelerators", "authors": "Song, L; Chen, F; Zhuo, Y; Qian, X; Li, H; Chen, Y", "published_date": "February 1, 2020", "doi": "10.1109/HPCA47549.2020.00036", "abstract": "© 2020 IEEE. Deep neural network (DNN) accelerators as an example of domain-specific architecture have demonstrated great success in DNN inference. However, the architecture acceleration for equally important DNN training has not yet been fully studied. With data forward, error backward and gradient calculation, DNN training is a more complicated process with higher computation and communication intensity. Because the recent research demonstrates a diminishing specialization return, namely, 'accelerator wall', we believe that a promising approach is to explore coarse-grained parallelism among multiple performance-bounded accelerators to support DNN training. Distributing computations on multiple heterogeneous accelerators to achieve high throughput and balanced execution, however, remaining challenging. We present AccPar, a principled and systematic method of determining the tensor partition among heterogeneous accelerator arrays. Compared to prior empirical or unsystematic methods, AccPar considers the complete tensor partition space and can reveal previously unknown new parallelism configurations. AccPar optimizes the performance based on a cost model that takes into account both computation and communication costs of a heterogeneous execution environment. Hence, our method can avoid the drawbacks of existing approaches that use communication as a proxy of the performance. The enhanced flexibility of tensor partitioning in AccPar allows the flexible ratio of computations to be distributed among accelerators with different performances. The proposed search algorithm is also applicable to the emerging multi-path patterns in modern DNNs such as ResNet. We simulate AccPar on a heterogeneous accelerator array composed of both TPU-v2 and TPU-v3 accelerators for the training of large-scale DNN models such as Alexnet, Vgg series, and Resnet series. The average performance improvements of the state-of-the-art 'one weird trick' (OWT) and HYPAR, and AccPar, normalized to the baseline data parallelism scheme where each accelerator replicates the model and processes different input data in parallel, are 2.98×, 3.78×, and 6.30×, respectively.", "publication_location": "Proceedings   2020 Ieee International Symposium on High Performance Computer Architecture, Hpca 2020", "link": "http://dx.doi.org/10.1109/HPCA47549.2020.00036", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "MRI-Based Deep Learning Segmentation and Radiomics of Sarcoma in Mice.", "authors": "Holbrook, MD; Blocker, SJ; Mowery, YM; Badea, A; Qi, Y; Xu, ES; Kirsch, DG; Johnson, GA; Badea, CT", "published_date": "March 2020", "doi": "10.18383/j.tom.2019.00021", "abstract": "Small-animal imaging is an essential tool that provides noninvasive, longitudinal insight into novel cancer therapies. However, considerable variability in image analysis techniques can lead to inconsistent results. We have developed quantitative imaging for application in the preclinical arm of a coclinical trial by using a genetically engineered mouse model of soft tissue sarcoma. Magnetic resonance imaging (MRI) images were acquired 1 day before and 1 week after radiation therapy. After the second MRI, the primary tumor was surgically removed by amputating the tumor-bearing hind limb, and mice were followed for up to 6 months. An automatic analysis pipeline was used for multicontrast MRI data using a convolutional neural network for tumor segmentation followed by radiomics analysis. We then calculated radiomics features for the tumor, the peritumoral area, and the 2 combined. The first radiomics analysis focused on features most indicative of radiation therapy effects; the second radiomics analysis looked for features that might predict primary tumor recurrence. The segmentation results indicated that Dice scores were similar when using multicontrast versus single T2-weighted data (0.863 vs 0.861). One week post RT, larger tumor volumes were measured, and radiomics analysis showed greater heterogeneity. In the tumor and peritumoral area, radiomics features were predictive of primary tumor recurrence (AUC: 0.79). We have created an image processing pipeline for high-throughput, reduced-bias segmentation of multiparametric tumor MRI data and radiomics analysis, to better our understanding of preclinical imaging and the insights it provides when studying new cancer therapies.", "publication_location": "Tomography", "link": "http://dx.doi.org/10.18383/j.tom.2019.00021", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Artificial Intelligence using Deep Learning System for Glaucoma Suspect Detection", "authors": "Hamzah, H; Lim, G; Quang, DN; Mani, B; Hsu, W; Lee, ML; Cheng, C-Y; Wong, TY; Ting, D", "published_date": "July 1, 2018", "doi": " ", "abstract": " ", "publication_location": "Investigative Ophthalmology & Visual Science", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000442932802238&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Variational autoencoder for deep learning of images, labels and captions", "authors": "Pu, Y; Gan, Z; Henao, R; Yuan, X; Li, C; Stevens, A; Carin, L", "published_date": "January 1, 2016", "doi": " ", "abstract": "© 2016 NIPS Foundation - All Rights Reserved. A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "The hierarchical beta process for convolutional factor analysis and deep learning", "authors": "Chen, B; Polatkan, G; Sapiro, G; Dunson, DB; Carin, L", "published_date": "October 7, 2011", "doi": " ", "abstract": "A convolutional factor-analysis model is developed, with the number of filters (factors) inferred via the beta process (BP) and hierarchical BP, for single-task and multi-task learning, respectively. The computation of the model parameters is implemented within a Bayesian setting, employing Gibbs sampling; we explicitly exploit the convolutional nature of the expansion to accelerate computations. The model is used in a multi-level (\"deep\") analysis of general data, with specific results presented for image-processing data sets, e.g., classification. Copyright 2011 by the author(s)/owner(s).", "publication_location": "Proceedings of the 28th International Conference on Machine Learning, Icml 2011", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning for segmentation of brain tumors: Can we train with images from different institutions?", "authors": "Paredes, D; Saha, A; Mazurowski, MA", "published_date": "January 1, 2017", "doi": "10.1117/12.2255696", "abstract": "© 2017 SPIE. Deep learning and convolutional neural networks (CNNs) in particular are increasingly popular tools for segmentation and classification of medical images. CNNs were shown to be successful for segmentation of brain tumors into multiple regions or labels. However, in the environment which fosters data-sharing and collection of multi-institutional datasets, a question arises: does training with data from another institution with potentially different imaging equipment, contrast protocol, and patient population impact the segmentation performance of the CNN? Our study presents preliminary data towards answering this question. Specifically, we used MRI data of glioblastoma (GBM) patients for two institutions present in The Cancer Imaging Archive. We performed a process of training and testing CNN multiple times such that half of the time the CNN was tested on data from the same institution that was used for training and half of the time it was tested on another institution, keeping the training and testing set size constant. We observed a decrease in performance as measured by Dice coefficient when the CNN was trained with data from a different institution as compared to training with data from the same institution. The changes in performance for the entire tumor and for four different labels within the tumor were: 0.72 to 0.65 (p=0.06), 0.61 to 0.58 (p=0.49), 0.54 to 0.51 (p=0.82), 0.31 to 0.24 (p<0.03), and 0.43 to 0.31(p<0.003) respectively. In summary, we found that while data across institutions can be used for development of CNNs, this might be associated with a decrease in performance.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2255696", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Dose Prediction for Prostate Radiation Treatment: Feasibility of a Distance-Based Deep Learning Model", "authors": "Maryam, TH; Ru, B; Xie, T; Hadzikadic, M; Wu, QJ; Ge, Y", "published_date": "November 1, 2019", "doi": "10.1109/BIBM47256.2019.8983412", "abstract": "© 2019 IEEE. This study aims to demonstrate the feasibility of using a novel distance-based representation of 3D CT-scan images to train a deep learning model for dose predictions in radiation treatment planning. The distance representation is inspired by previous knowledge of the domain to increase the generalizability of the deep learning models for radiation treatment planning. Conventional knowledge-based planning methods extract engineered features from 3D CT-scan images, as well as other patients' features, to predict the best achievable dose in a cancerous area and other organs at risk. Recent studies have shown higher accuracy in voxel-level dose prediction using deep learning models compared to the conventional machine learning approaches. Since the data resources for training these models are limited, most of the studies use 2D contour information to represent the patient anatomy. This representation loses volumetric information, and it is sensitive to small changes in patient orientation and translation. The distance-based representation introduced in this paper is inspired by the domain knowledge and is able to maintain the volumetric distance information despite the 2D slicing of 3D CT-image. According to prior studies in the radiation treatment planning domain, there is a strong association between the organs-at-risk distance from the cancerous volume and the patient's vulnerability to receive excessive dose. Therefore, the contour value in prior representation was replaced by voxel distance from cancerous volume. This modification in representation makes it transition and orientation invariant and adds potential robustness to patient positioning differences during the imaging/planning process. We evaluated the distance-based deep learning models through experiments for prediction of prostate cancer patients' vulnerability and voxel-level dose distribution using convolutional neural network and U-net models, respectively. The results were compared with contour-based U-net model as well as conventional machine learning with engineered representations. We found that the performance was comparable or higher than the prior state-of-the-art results for prostate-cancer dose distribution prediction.", "publication_location": "Proceedings   2019 Ieee International Conference on Bioinformatics and Biomedicine, Bibm 2019", "link": "http://dx.doi.org/10.1109/BIBM47256.2019.8983412", "citations": " ", "readership": " ", "tweets": "3", "news_mentions": " "},
{"title": "DarNet: A deep learning solution for distracted driving detection", "authors": " ", "published_date": "December 11, 2017", "doi": "10.1145/3154448.3154452", "abstract": "© 2017 ACM. Distracted driving is known to be the leading cause of motor vehicle accidents. With the increase in the number of IoT devices available within vehicles, there exists an abundance of data for monitoring driver behavior. However, designing a system around this goal presents two key challenges - how to concurrently collect data spanning multiple IoT devices, and how to jointly analyze this multimodal input. To that end, we present a unified data collection and analysis framework, DarNet, capable of detecting and classifying distracted driving behavior. DarNet consists of two primary components: A data collection system and an analytics engine. Our system takes advantage of advances in machine learning (ML) to classify driving behavior based on input sensor data. In our system implementation, we collect image data from an inward facing camera, and Inertial Measurement Unit (IMU) data from a mobile device, both located within the vehicle. Using deep learning techniques, we show that DarNet achieves a Top-1 classification percentage of 87.02% on our collected dataset, significantly outperforming our baseline model of 73.88%. Additionally, we address the privacy concerns associated with collecting image data by presenting an alternative framework designed to operate on down-sampled data which produces a Top-1 classification percentage of 80.00%.", "publication_location": "Middleware 2017   Proceedings of the 2017 International Middleware Conference (Industrial Track)", "link": "http://dx.doi.org/10.1145/3154448.3154452", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning approaches for plethysmography signal quality assessment in the presence of atrial fibrillation.", "authors": "Pereira, T; Ding, C; Gadhoumi, K; Tran, N; Colorado, RA; Meisel, K; Hu, X", "published_date": "December 27, 2019", "doi": "10.1088/1361-6579/ab5b84", "abstract": "OBJECTIVE:Photoplethysmography (PPG) monitoring has been implemented in many portable and wearable devices we use daily for health and fitness tracking. Its simplicity and cost-effectiveness has enabled a variety of biomedical applications, such as continuous long-term monitoring of heart arrhythmias, fitness, and sleep tracking, and hydration monitoring. One major issue that can hinder PPG-based applications is movement artifacts, which can lead to false interpretations. In many implementations, noisy PPG signals are discarded. Misinterpreted or discarded PPG signals pose a problem in applications where the goal is to increase the yield of detecting physiological events, such as in the case of paroxysmal atrial fibrillation (AF)-a common episodic heart arrhythmia and a leading risk factor for stroke. In this work, we compared a traditional machine learning and deep learning approaches for PPG quality assessment in the presence of AF, in order to find the most robust method for PPG quality assessment. APPROACH:The training data set was composed of 78 278 30 s long PPG recordings from 3764 patients using bedside patient monitors. Two different representations of PPG signals were employed-a time-series based (1D) one and an image-based (2D) one. Trained models were tested on an independent set of 2683 30 s PPG signals from 13 stroke patients. MAIN RESULTS:ResNet18 showed a higher performance (0.985 accuracy, 0.979 specificity, and 0.988 sensitivity) than SVM and other deep learning approaches. 2D-based models were generally more accurate than 1D-based models. SIGNIFICANCE:2D representation of PPG signal enhances the accuracy of PPG signal quality assessment.", "publication_location": "Physiological Measurement", "link": "http://dx.doi.org/10.1088/1361-6579/ab5b84", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Noise and spatial resolution properties of a commercially available deep learning-based CT reconstruction algorithm.", "authors": "Solomon, J; Lyu, P; Marin, D; Samei, E", "published_date": "June 7, 2020", "doi": "10.1002/mp.14319", "abstract": "PURPOSE: To characterize the noise and spatial resolution properties of a commercially available deep learning-based computed tomography (CT) reconstruction algorithm. METHODS: Two phantom experiments were performed. The first used a multisized image quality phantom (Mercury v3.0, Duke University) imaged at five radiation dose levels (CTDIvol : 0.9, 1.2, 3.6, 7.0, and 22.3 mGy) with a fixed tube current technique on a commercial CT scanner (GE Revolution CT). Images were reconstructed with conventional (FBP), iterative (GE ASiR-V), and deep learning-based (GE True Fidelity) reconstruction algorithms. Noise power spectrum (NPS), high-contrast (air-polyethylene interface), and intermediate-contrast (water-polyethylene interface) task transfer functions (TTF) were measured for each dose level and phantom size and summarized in terms of average noise frequency (fav ) and frequency at which the TTF was reduced to 50% (f50% ), respectively. The second experiment used a custom phantom with low-contrast rods and lung texture sections for the assessment of low-contrast TTF and noise spatial distribution. The phantom was imaged at five dose levels (CTDIvol : 1.0, 2.1, 3.0, 6.0, and 10.0 mGy) with 20 repeated scans at each dose, and images reconstructed with the same reconstruction algorithms. The local noise stationarity was assessed by generating spatial noise maps from the ensemble of repeated images and computing a noise inhomogeneity index,                         η                     , following AAPM TG233 methods. All measurements were compared among the algorithms. RESULTS: Compared to FBP, noise magnitude was reduced on average (± one standard deviation) by 74 ± 6% and 68 ± 4% for ASiR-V (at \"100%\" setting) and True Fidelity (at \"High\" setting), respectively. The noise texture from ASiR-V had substantially lower noise frequency content with 55 ± 4% lower NPS fav compared to FBP while True Fidelity had only marginally different noise frequency content with 9 ± 5% lower NPS fav compared to FBP. Both ASiR-V and True Fidelity demonstrated locally nonstationary noise in a lung texture background at all radiation dose levels, with higher noise near high-contrast edges of vessels and lower noise in uniform regions. At the 1.0 mGy dose level                         η                     values were 314% and 271% higher in ASiR-V and True Fidelity compared to FBP, respectively. High-contrast spatial resolution was similar between all algorithms for all dose levels and phantom sizes (<3% difference in TTF f50% ). Compared to FBP, low-contrast spatial resolution was lower for ASiR-V and True Fidelity with a reduction of TTF f50% of up to 42% and 36%, respectively. CONCLUSIONS: The deep learning-based CT reconstruction demonstrated a strong noise magnitude reduction compared to FBP while maintaining similar noise texture and high-contrast spatial resolution. However, the algorithm resulted in images with a locally nonstationary noise in lung textured backgrounds and had somewhat degraded low-contrast spatial resolution similar to what has been observed in currently available iterative reconstruction techniques.", "publication_location": "Med Phys", "link": "http://dx.doi.org/10.1002/mp.14319", "citations": " ", "readership": " ", "tweets": "5", "news_mentions": " "},
{"title": "Real-World Integration of a Sepsis Deep Learning Technology Into Routine Clinical Care: Implementation Study.", "authors": "Sendak, MP; Ratliff, W; Sarro, D; Alderton, E; Futoma, J; Gao, M; Nichols, M; Revoir, M; Yashar, F; Miller, C; Kester, K; Sandhu, S; Corey, K; Brajer, N; Tan, C; Lin, A; Brown, T; Engelbosch, S; Anstrom, K; Elish, MC; Heller, K; Donohoe, R; Theiling, J; Poon, E; Balu, S; Bedoya, A; O'Brien, C", "published_date": "July 15, 2020", "doi": "10.2196/15182", "abstract": "BACKGROUND:Successful integrations of machine learning into routine clinical care are exceedingly rare, and barriers to its adoption are poorly characterized in the literature. OBJECTIVE:This study aims to report a quality improvement effort to integrate a deep learning sepsis detection and management platform, Sepsis Watch, into routine clinical care. METHODS:In 2016, a multidisciplinary team consisting of statisticians, data scientists, data engineers, and clinicians was assembled by the leadership of an academic health system to radically improve the detection and treatment of sepsis. This report of the quality improvement effort follows the learning health system framework to describe the problem assessment, design, development, implementation, and evaluation plan of Sepsis Watch. RESULTS:Sepsis Watch was successfully integrated into routine clinical care and reshaped how local machine learning projects are executed. Frontline clinical staff were highly engaged in the design and development of the workflow, machine learning model, and application. Novel machine learning methods were developed to detect sepsis early, and implementation of the model required robust infrastructure. Significant investment was required to align stakeholders, develop trusting relationships, define roles and responsibilities, and to train frontline staff, leading to the establishment of 3 partnerships with internal and external research groups to evaluate Sepsis Watch. CONCLUSIONS:Machine learning models are commonly developed to enhance clinical decision making, but successful integrations of machine learning into routine clinical care are rare. Although there is no playbook for integrating deep learning into clinical care, learnings from the Sepsis Watch integration can inform efforts to develop machine learning technologies at other health care delivery systems.", "publication_location": "Jmir Medical Informatics", "link": "http://dx.doi.org/10.2196/15182", "citations": "2", "readership": " ", "tweets": "10", "news_mentions": " "},
{"title": "Predicting Alzheimer's disease progression using multi-modal deep learning approach.", "authors": "Lee, G; Nho, K; Kang, B; Sohn, K-A; Kim, D; for Alzheimer’s Disease Neuroimaging Initiative,", "published_date": "February 13, 2019", "doi": "10.1038/s41598-018-37769-z", "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative condition marked by a decline in cognitive functions with no validated disease modifying treatment. It is critical for timely treatment to detect AD in its earlier stage before clinical manifestation. Mild cognitive impairment (MCI) is an intermediate stage between cognitively normal older adults and AD. To predict conversion from MCI to probable AD, we applied a deep learning approach, multimodal recurrent neural network. We developed an integrative framework that combines not only cross-sectional neuroimaging biomarkers at baseline but also longitudinal cerebrospinal fluid (CSF) and cognitive performance biomarkers obtained from the Alzheimer's Disease Neuroimaging Initiative cohort (ADNI). The proposed framework integrated longitudinal multi-domain data. Our results showed that 1) our prediction model for MCI conversion to AD yielded up to 75% accuracy (area under the curve (AUC) = 0.83) when using only single modality of data separately; and 2) our prediction model achieved the best performance with 81% accuracy (AUC = 0.86) when incorporating longitudinal multi-domain data. A multi-modal deep learning approach has potential to identify persons at risk of developing AD who might benefit most from a clinical trial or as a stratification approach within clinical trials.", "publication_location": "Scientific Reports", "link": "http://dx.doi.org/10.1038/s41598-018-37769-z", "citations": "27", "readership": "160", "tweets": "9", "news_mentions": " "},
{"title": "A Deep Learning Algorithm to Quantify Neuroretinal Rim Loss From Optic Disc Photographs.", "authors": "Thompson, AC; Jammal, AA; Medeiros, FA", "published_date": "May 2019", "doi": "10.1016/j.ajo.2019.01.011", "abstract": "PURPOSE: To train a deep learning (DL) algorithm that quantifies glaucomatous neuroretinal damage on fundus photographs using the minimum rim width relative to Bruch membrane opening (BMO-MRW) from spectral-domain optical coherence tomography (SDOCT). DESIGN: Cross-sectional study. METHODS: A total of 9282 pairs of optic disc photographs and SDOCT optic nerve head scans from 927 eyes of 490 subjects were randomly divided into the validation plus training (80%) and test sets (20%). A DL convolutional neural network was trained to predict the SDOCT BMO-MRW global and sector values when evaluating optic disc photographs. The predictions of the DL network were compared to the actual SDOCT measurements. The area under the receiver operating curve (AUC) was used to evaluate the ability of the network to discriminate glaucomatous visual field loss from normal eyes. RESULTS: The DL predictions of global BMO-MRW from all optic disc photographs in the test set (mean ± standard deviation [SD]: 228.8 ± 63.1 μm) were highly correlated with the observed values from SDOCT (mean ± SD: 226.0 ± 73.8 μm) (Pearson's r = 0.88; R2 = 77%; P < .001), with mean absolute error of the predictions of 27.8 μm. The AUCs for discriminating glaucomatous from healthy eyes with the DL predictions and actual SDOCT global BMO-MRW measurements were 0.945 (95% confidence interval [CI]: 0.874-0.980) and 0.933 (95% CI: 0.856-0.975), respectively (P = .587). CONCLUSIONS: A DL network can be trained to quantify the amount of neuroretinal damage on optic disc photographs using SDOCT BMO-MRW as a reference. This algorithm showed high accuracy for glaucoma detection, and may potentially eliminate the need for human gradings of disc photographs.", "publication_location": "American Journal of Ophthalmology", "link": "http://dx.doi.org/10.1016/j.ajo.2019.01.011", "citations": "15", "readership": "46", "tweets": "11", "news_mentions": " "},
{"title": "Deep learning analysis of breast MRIs for prediction of occult invasive disease in ductal carcinoma in situ.", "authors": "Zhu, Z; Harowicz, M; Zhang, J; Saha, A; Grimm, LJ; Hwang, ES; Mazurowski, MA", "published_date": "December 2019", "doi": "10.1016/j.compbiomed.2019.103498", "abstract": "PURPOSE: To determine whether deep learning-based algorithms applied to breast MR images can aid in the prediction of occult invasive disease following the diagnosis of ductal carcinoma in situ (DCIS) by core needle biopsy. MATERIALS AND METHODS: Our study is retrospective. The data was collected from 2000 to 2014. In this institutional review board-approved study, we analyzed dynamic contrast-enhanced fat-saturated T1-weighted MRI sequences from 131 patients with a core needle biopsy-confirmed diagnosis of DCIS. We explored two different deep learning approaches to predict whether there was an occult invasive component in the analyzed tumors that was ultimately identified at surgical excision. In the first approach, we adopted the transfer learning strategy. Specifically, we used the pre-trained GoogleNet. In the second approach, we used a pre-trained network to extract deep features, and a support vector machine (SVM) that utilizes these features to predict the upstaging of DCIS. We used nested 10-fold cross validation and the area under the ROC curve (AUC) to estimate the performance of the predictive models. RESULTS: The best classification performance was obtained using the deep features approach with GoogleNet model pre-trained on ImageNet as the feature extractor and a polynomial kernel SVM used as the classifier (AUC = 0.70, 95% CI: 0.58-0.79). For the transfer learning based approach, the highest AUC obtained was 0.68 (95% CI: 0.57-0.77). CONCLUSIONS: Convolutional neural networks might be used to identify occult invasive disease in patients diagnosed with DCIS by core needle biopsy.", "publication_location": "Comput Biol Med", "link": "http://dx.doi.org/10.1016/j.compbiomed.2019.103498", "citations": "5", "readership": "26", "tweets": "4", "news_mentions": " "},
{"title": "Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists.", "authors": "Rajpurkar, P; Irvin, J; Ball, RL; Zhu, K; Yang, B; Mehta, H; Duan, T; Ding, D; Bagul, A; Langlotz, CP; Patel, BN; Yeom, KW; Shpanskaya, K; Blankenberg, FG; Seekins, J; Amrhein, TJ; Mong, DA; Halabi, SS; Zucker, EJ; Ng, AY; Lungren, MP", "published_date": "November 2018", "doi": "10.1371/journal.pmed.1002686", "abstract": "BACKGROUND: Chest radiograph interpretation is critical for the detection of thoracic diseases, including tuberculosis and lung cancer, which affect millions of people worldwide each year. This time-consuming task typically requires expert radiologists to read the images, leading to fatigue-based diagnostic error and lack of diagnostic expertise in areas of the world where radiologists are not available. Recently, deep learning approaches have been able to achieve expert-level performance in medical image interpretation tasks, powered by large network architectures and fueled by the emergence of large labeled datasets. The purpose of this study is to investigate the performance of a deep learning algorithm on the detection of pathologies in chest radiographs compared with practicing radiologists. METHODS AND FINDINGS: We developed CheXNeXt, a convolutional neural network to concurrently detect the presence of 14 different pathologies, including pneumonia, pleural effusion, pulmonary masses, and nodules in frontal-view chest radiographs. CheXNeXt was trained and internally validated on the ChestX-ray8 dataset, with a held-out validation set consisting of 420 images, sampled to contain at least 50 cases of each of the original pathology labels. On this validation set, the majority vote of a panel of 3 board-certified cardiothoracic specialist radiologists served as reference standard. We compared CheXNeXt's discriminative performance on the validation set to the performance of 9 radiologists using the area under the receiver operating characteristic curve (AUC). The radiologists included 6 board-certified radiologists (average experience 12 years, range 4-28 years) and 3 senior radiology residents, from 3 academic institutions. We found that CheXNeXt achieved radiologist-level performance on 11 pathologies and did not achieve radiologist-level performance on 3 pathologies. The radiologists achieved statistically significantly higher AUC performance on cardiomegaly, emphysema, and hiatal hernia, with AUCs of 0.888 (95% confidence interval [CI] 0.863-0.910), 0.911 (95% CI 0.866-0.947), and 0.985 (95% CI 0.974-0.991), respectively, whereas CheXNeXt's AUCs were 0.831 (95% CI 0.790-0.870), 0.704 (95% CI 0.567-0.833), and 0.851 (95% CI 0.785-0.909), respectively. CheXNeXt performed better than radiologists in detecting atelectasis, with an AUC of 0.862 (95% CI 0.825-0.895), statistically significantly higher than radiologists' AUC of 0.808 (95% CI 0.777-0.838); there were no statistically significant differences in AUCs for the other 10 pathologies. The average time to interpret the 420 images in the validation set was substantially longer for the radiologists (240 minutes) than for CheXNeXt (1.5 minutes). The main limitations of our study are that neither CheXNeXt nor the radiologists were permitted to use patient history or review prior examinations and that evaluation was limited to a dataset from a single institution. CONCLUSIONS: In this study, we developed and validated a deep learning algorithm that classified clinically important abnormalities in chest radiographs at a performance level comparable to practicing radiologists. Once tested prospectively in clinical settings, the algorithm could have the potential to expand patient access to chest radiograph diagnostics.", "publication_location": "Plos Medicine", "link": "http://dx.doi.org/10.1371/journal.pmed.1002686", "citations": "122", "readership": "368", "tweets": "183", "news_mentions": "6"},
{"title": "Deep learning feature extraction for target recognition and classification in underwater sonar images", "authors": "Zhu, P; Isaacs, J; Fu, B; Ferrari, S", "published_date": "January 18, 2018", "doi": "10.1109/CDC.2017.8264055", "abstract": "© 2017 IEEE. This paper presents an automatic target recognition (ATR) approach for sonar onboard unmanned underwater vehicles (UUVs). In this approach, target features are extracted by a convolutional neural network (CNN) operating on sonar images, and then classified by a support vector machine (SMV) that is trained based on manually labeled data. The proposed approach is tested on a set of sonar images obtained by a UUV equipped with side-scan sonar. Automatic target recognition is achieved through the use of matched filters, while target classification is achieved with the trained SVM classifier based on features extracted by the CNN. The results show that deep learning feature extraction provide better performance compared to using other feature extraction techniques such as histogram of oriented gradients (HOG) and local binary pattern (LBP). By processing images autonomously, the proposed approach can be combined with onboard planning and control systems to develop autonomous UUVs able to search for underwater targets without human intervention.", "publication_location": "2017 Ieee 56th Annual Conference on Decision and Control, Cdc 2017", "link": "http://dx.doi.org/10.1109/CDC.2017.8264055", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Spatial Organization and Molecular Correlation of Tumor-Infiltrating Lymphocytes Using Deep Learning on Pathology Images.", "authors": "Saltz, J; Gupta, R; Hou, L; Kurc, T; Singh, P; Nguyen, V; Samaras, D; Shroyer, KR; Zhao, T; Batiste, R; Van Arnam, J; Cancer Genome Atlas Research Network, ; Shmulevich, I; Rao, AUK; Lazar, AJ; Sharma, A; Thorsson, V", "published_date": "April 2018", "doi": "10.1016/j.celrep.2018.03.086", "abstract": "Beyond sample curation and basic pathologic characterization, the digitized H&E-stained images of TCGA samples remain underutilized. To highlight this resource, we present mappings of tumor-infiltrating lymphocytes (TILs) based on H&E images from 13 TCGA tumor types. These TIL maps are derived through computational staining using a convolutional neural network trained to classify patches of images. Affinity propagation revealed local spatial structure in TIL patterns and correlation with overall survival. TIL map structural patterns were grouped using standard histopathological parameters. These patterns are enriched in particular T cell subpopulations derived from molecular measures. TIL densities and spatial structure were differentially enriched among tumor types, immune subtypes, and tumor molecular subtypes, implying that spatial infiltrate state could reflect particular tumor cell aberration states. Obtaining spatial lymphocytic patterns linked to the rich genomic characterization of TCGA samples demonstrates one use for the TCGA image archives with insights into the tumor-immune microenvironment.", "publication_location": "Cell Reports", "link": "http://dx.doi.org/10.1016/j.celrep.2018.03.086", "citations": "144", "readership": "630", "tweets": "429", "news_mentions": "5"},
{"title": "Automatic estimation of knee joint space narrowing by deep learning segmentation algorithms", "authors": "Swiecicki, A; Said, N; O'Donnell, J; Buda, M; Li, N; Jiranek, WA; Mazurowski, MA", "published_date": "January 1, 2020", "doi": "10.1117/12.2551377", "abstract": "© 2020 SPIE. Evaluating the severity of knee osteoarthritis (OA) accounts for significant plain film workload and is a crucial component of knee radiograph interpretation, which informs surgical decision-making for costly and invasive procedures such as knee replacement. The Kellgren-Lawrence (KL) grading scale systematically and quantitatively assesses the severity of knee OA but is associated with notable inter-reader variability. In this study, we propose a deep learning method for the assessment of joint space narrowing (JSN) in the knee, which is an essential part of determining the KL grade. To determine the extent of JSN, we analyzed 99 knee radiographs to calculate the distance between the femur and tibia. Our algorithm's measurements of JSN and KL grade correlated well other radiologists' assessments. The average distance (in pixels) between the femur and tibia bones as measured by our algorithm was 9.60 for KL=0, 7.60 for KL=1, 6.89 for KL=2, 3.75 for KL=3, 1.25 for KL=4. Additionally, we used 100 manually annotated knee radiographs to train the algorithm to segment the femur and tibia bones. When evaluated on an independent set of 20 knee radiographs, the algorithm demonstrated a Dice coefficient of 96.59%. An algorithm for measurement of JSN and KL grades may play a significant role in automatically, reliably, and passively evaluating knee OA severity and influence and surgical decision-making and treatment pathways.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2551377", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Prediction of Occult Invasive Disease in Ductal Carcinoma in Situ Using Deep Learning Features.", "authors": "Shi, B; Grimm, LJ; Mazurowski, MA; Baker, JA; Marks, JR; King, LM; Maley, CC; Hwang, ES; Lo, JY", "published_date": "March 2018", "doi": "10.1016/j.jacr.2017.11.036", "abstract": "PURPOSE: The aim of this study was to determine whether deep features extracted from digital mammograms using a pretrained deep convolutional neural network are prognostic of occult invasive disease for patients with ductal carcinoma in situ (DCIS) on core needle biopsy. METHODS: In this retrospective study, digital mammographic magnification views were collected for 99 subjects with DCIS at biopsy, 25 of which were subsequently upstaged to invasive cancer. A deep convolutional neural network model that was pretrained on nonmedical images (eg, animals, plants, instruments) was used as the feature extractor. Through a statistical pooling strategy, deep features were extracted at different levels of convolutional layers from the lesion areas, without sacrificing the original resolution or distorting the underlying topology. A multivariate classifier was then trained to predict which tumors contain occult invasive disease. This was compared with the performance of traditional \"handcrafted\" computer vision (CV) features previously developed specifically to assess mammographic calcifications. The generalization performance was assessed using Monte Carlo cross-validation and receiver operating characteristic curve analysis. RESULTS: Deep features were able to distinguish DCIS with occult invasion from pure DCIS, with an area under the receiver operating characteristic curve of 0.70 (95% confidence interval, 0.68-0.73). This performance was comparable with the handcrafted CV features (area under the curve = 0.68; 95% confidence interval, 0.66-0.71) that were designed with prior domain knowledge. CONCLUSIONS: Despite being pretrained on only nonmedical images, the deep features extracted from digital mammograms demonstrated comparable performance with handcrafted CV features for the challenging task of predicting DCIS upstaging.", "publication_location": "Journal of the American College of Radiology : Jacr", "link": "http://dx.doi.org/10.1016/j.jacr.2017.11.036", "citations": "25", "readership": "50", "tweets": "3", "news_mentions": " "},
{"title": "SGD converges to global minimum in deep learning via star-convex path", "authors": "Zhou, Y; Yang, J; Zhang, H; Liang, Y; Tarokh, V", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved. Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper. In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum.", "publication_location": "7th International Conference on Learning Representations, Iclr 2019", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "SGD converges to global minimum in deep learning via star-convex path", "authors": "Zhou, Y; Yang, J; Zhang, H; Liang, Y; Tarokh, V", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved. Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can train these complex networks towards a global minimum. In this study, we establish the convergence of SGD to a global minimum for nonconvex optimization problems that are commonly encountered in neural network training. Our argument exploits the following two important properties: 1) the training loss can achieve zero value (approximately), which has been widely observed in deep learning; 2) SGD follows a star-convex path, which is verified by various experiments in this paper. In such a context, our analysis shows that SGD, although has long been considered as a randomized algorithm, converges in an intrinsically deterministic manner to a global minimum.", "publication_location": "7th International Conference on Learning Representations, Iclr 2019", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Predict In-Hospital Code Blue Events using Monitor Alarms through Deep Learning Approach.", "authors": "Xiao, R; King, J; Villaroman, A; Do, DH; Boyle, NG; Hu, X", "published_date": "July 2018", "doi": "10.1109/embc.2018.8513269", "abstract": "Bedside monitors in hospital intensive care units (ICUs) are known to produce excessive false alarms that could desensitize caregivers, resulting in delayed or even missed clinical interventions to life-threatening events. Our previous studies proposed a framework aggregating information in monitor alarm data by mining frequent alarm combinations (i.e., SuperAlarm) that are predictive to clinical endpoints, such as code blue events, in an effort to address this critical issue. In the present pilot study, we hypothesize that sequential deep learning models, specifically long-short term memory (LSTM), could capture time-depend features in continuous alarm sequences preceding code blue events and these features may be predictive of these endpoints. LSTM models are trained from continuous alarm sequences in various window lengths preceding code blue events, and the preliminary results showed the best performance reached an AUC of 0.8549. With the selection of optimal cutoff threshold, the 2-hour window model achieved 85.75% sensitivity and 72.61% specificity, respectively.", "publication_location": "Conference Proceedings : ... Annual International Conference of the Ieee Engineering in Medicine and Biology Society. Ieee Engineering in Medicine and Biology Society. Annual Conference", "link": "http://dx.doi.org/10.1109/embc.2018.8513269", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "2 Sepsis Watch: A Successful Deployment of a Deep Learning Sepsis Detection and Treatment Platform", "authors": "Theiling, BJ; Donohoe, R; Sendak, M; Bedoya, A; Gao, M; Ratliff, W; Denis, L; Balu, S; O'Brein, C", "published_date": "October 2019", "doi": "10.1016/j.annemergmed.2019.08.005", "abstract": " ", "publication_location": "Annals of Emergency Medicine", "link": "http://dx.doi.org/10.1016/j.annemergmed.2019.08.005", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "HyPar: Towards hybrid parallelism for deep learning accelerator array", "authors": " ", "published_date": "March 26, 2019", "doi": "10.1109/HPCA.2019.00027", "abstract": "© 2019 IEEE. With the rise of artificial intelligence in recent years, Deep Neural Networks (DNNs) have been widely used in many domains. To achieve high performance and energy efficiency, hardware acceleration (especially inference) of DNNs is intensively studied both in academia and industry. However, we still face two challenges: Large DNN models and datasets, which incur frequent off-chip memory accesses; and the training of DNNs, which is not well-explored in recent accelerator designs. To truly provide high throughput and energy efficient acceleration for the training of deep and large models, we inevitably need to use multiple accelerators to explore the coarse-grain parallelism, compared to the fine-grain parallelism inside a layer considered in most of the existing architectures. It poses the key research question to seek the best organization of computation and dataflow among accelerators. In this paper, we propose a solution HYPAR to determine layer-wise parallelism for deep neural network training with an array of DNN accelerators. HYPAR partitions the feature map tensors (input and output), the kernel tensors, the gradient tensors, and the error tensors for the DNN accelerators. A partition constitutes the choice of parallelism for weighted layers. The optimization target is to search a partition that minimizes the total communication during training a complete DNN. To solve this problem, we propose a communication model to explain the source and amount of communications. Then, we use a hierarchical layer-wise dynamic programming method to search for the partition for each layer. HYPAR is practical: The time complexity for the partition search in HYPAR is linear. We apply this method in an HMC-based DNN training architecture to minimize data movement. We evaluate HYPAR with ten DNN models from classic Lenet to large-size model VGGs, and the number of weighted layers of these models range from four to nineteen. Our evaluation finds that: The default Model Parallelism is indeed the worst; the default Data Parallelism is not the best; but hybrid parallelism can be better than either the default Data Parallelism or Model Parallelism in DNN training with an array of accelerators. Our evaluation shows that HYPAR achieves a performance gain of 3.39× and an energy efficiency gain of 1.51× compared to Data Parallelism on average, and HYPAR performs up to 2.40× better than \"one weird trick\".", "publication_location": "Proceedings   25th Ieee International Symposium on High Performance Computer Architecture, Hpca 2019", "link": "http://dx.doi.org/10.1109/HPCA.2019.00027", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Wideband Time-Domain Digital Backpropagation via Subband Processing and Deep Learning", "authors": "Häger, C; Pfister, HD", "published_date": "November 14, 2018", "doi": "10.1109/ECOC.2018.8535251", "abstract": "© 2018 IEEE. We propose a low-complexity sub-banded DSP architecture for digital backpropagation where the walk-off effect is compensated using simple delay elements. For a simulated 96-Gbaud signal and 2500 km optical link, our method achieves a 2.8 dB SNR improvement over linear equalization.", "publication_location": "European Conference on Optical Communication, Ecoc", "link": "http://dx.doi.org/10.1109/ECOC.2018.8535251", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A Deep-Learning Algorithm for Thyroid Malignancy Prediction From Whole Slide Cytopathology Images.", "authors": "Dov, D; Kovalsky, SZ; Cohen, J; Range, DE; Henao, R; Carin, L", "published_date": "2019", "doi": " ", "abstract": " ", "publication_location": "Corr", "link": "https://hdl.handle.net/10161/18529", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Respiratory Deformation Registration in 4D-CT/CBCT Using Deep Learning", "authors": "Teng, X; Chen, Y; Jiang, Z; Zhao, Y; Ren, L", "published_date": "June 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Medical Physics", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000471277701214&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "CBCT Image Quality Augmentation Using Deep Learning Models: A Comparison Study", "authors": "Zhao, Y; Jiang, Z; Teng, X; Ren, L", "published_date": "June 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Medical Physics", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000471277700105&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Detecting Cardiac Transplant Antibody Mediated Rejection by Artificial Intelligence: A Novel Deep Learning Approach", "authors": "Davis, R; Carney, J; Dibernardo, L; Pavlisko, E; Glass, M; Glass, C", "published_date": "March 1, 2020", "doi": " ", "abstract": " ", "publication_location": "Laboratory Investigation", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000518328800302&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Detecting Cardiac Transplant Antibody Mediated Rejection by Artificial Intelligence: A Novel Deep Learning Approach", "authors": "Davis, R; Carney, J; Dibernardo, L; Pavlisko, E; Glass, M; Glass, C", "published_date": "March 1, 2020", "doi": " ", "abstract": " ", "publication_location": "Modern Pathology : an Official Journal of the United States and Canadian Academy of Pathology, Inc", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000518328900302&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "“The human body is a black box”: Supporting clinical decision-making with deep learning", "authors": "Sendak, M; Elish, MC; Gao, M; Futoma, J; Ratliff, W; Nichols, M; Bedoya, A; Balu, S; O'Brien, C", "published_date": "January 27, 2020", "doi": "10.1145/3351095.3372827", "abstract": "© 2020 Copyright is held by the owner/author(s). ACM Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.", "publication_location": "Fat* 2020   Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency", "link": "http://dx.doi.org/10.1145/3351095.3372827", "citations": "2", "readership": " ", "tweets": "21", "news_mentions": " "},
{"title": "Combining deep learning methods and human knowledge to identify abnormalities in computed tomography (CT) reports", "authors": "Benitez, M; Tian, J; Kelly, M; Selvakumaran, V; Phelan, M; Mazurowski, M; Lo, JY; Rubin, GD; Henao, R", "published_date": "January 1, 2019", "doi": "10.1117/12.2512886", "abstract": "© 2019 SPIE. Many researchers in the field of machine learning have addressed the problem of detecting anomalies within Computed Tomography (CT) scans. Training these machine learning algorithms requires a dataset of CT scans with identified anomalies (labels), usually, in specific organs. This represents a problem, since it requires experts to review thousands of images in order to create labels for these data. We aim to decrease human burden at labeling CT scans by developing a model that identifies anomalies within plain-text-based reports that then could be further used as a method to create labels for models based on CT scans. This study contains more than 4800 CT reports from Duke Health System, for which we aim to identify organ specific abnormalities. We propose an iterative active learning approach that consists of building a machine learning model to classify CT reports by abnormalities in different organs and then improving it by actively adding reports sequentially. At each iteration, clinical experts review the report that provides the model with highest expected information gain. This process is done in real time by using a web interface. Then, this datum is used by the model to improve its performance. We evaluated the performance of our method for abnormalities in kidneys and lungs. When starting with a model trained on 99 reports, the results show the model achieves an Area Under the Curve (AUC) score of 0.93 on the test set after adding 130 actively labeled reports to the model from an unlabeled pool of 4,000. This suggests that a set of labeled CT scans can be obtained with significantly reduced human work by combining machine learning techniques and clinical experts' knowledge.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2512886", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning for segmentation of brain tumors: Impact of cross-institutional training and testing.", "authors": "AlBadawy, EA; Saha, A; Mazurowski, MA", "published_date": "March 2018", "doi": "10.1002/mp.12752", "abstract": "BACKGROUND AND PURPOSE: Convolutional neural networks (CNNs) are commonly used for segmentation of brain tumors. In this work, we assess the effect of cross-institutional training on the performance of CNNs. METHODS: We selected 44 glioblastoma (GBM) patients from two institutions in The Cancer Imaging Archive dataset. The images were manually annotated by outlining each tumor component to form ground truth. To automatically segment the tumors in each patient, we trained three CNNs: (a) one using data for patients from the same institution as the test data, (b) one using data for the patients from the other institution and (c) one using data for the patients from both of the institutions. The performance of the trained models was evaluated using Dice similarity coefficients as well as Average Hausdorff Distance between the ground truth and automatic segmentations. The 10-fold cross-validation scheme was used to compare the performance of different approaches. RESULTS: Performance of the model significantly decreased (P < 0.0001) when it was trained on data from a different institution (dice coefficients: 0.68 ± 0.19 and 0.59 ± 0.19) as compared to training with data from the same institution (dice coefficients: 0.72 ± 0.17 and 0.76 ± 0.12). This trend persisted for segmentation of the entire tumor as well as its individual components. CONCLUSIONS: There is a very strong effect of selecting data for training on performance of CNNs in a multi-institutional setting. Determination of the reasons behind this effect requires additional comprehensive investigation.", "publication_location": "Med Phys", "link": "http://dx.doi.org/10.1002/mp.12752", "citations": "28", "readership": "96", "tweets": "92", "news_mentions": " "},
{"title": "OLE: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning", "authors": "Lezama, J; Qiu, Q; Musé, P; Sapiro, G", "published_date": "December 14, 2018", "doi": "10.1109/CVPR.2018.00846", "abstract": "© 2018 IEEE. Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are ubiquitous tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, these carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this paper, we propose a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple and elegant geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OLÃ‰) does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss, being the first reported deep metric learning framework of its kind. Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative, and more robust. We demonstrate improved classification performance in general object recognition, plugging the proposed loss term into existing off-the-shelf architectures. In particular, we show the advantage of the proposed loss in the small data/model scenario, and we significantly advance the state-of-the-art on the Stanford STL-10 benchmark.", "publication_location": "Proceedings of the Ieee Computer Society Conference on Computer Vision and Pattern Recognition", "link": "http://dx.doi.org/10.1109/CVPR.2018.00846", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Prediction of occult invasive disease in ductal carcinoma in situ using deep learning features", "authors": "Lo, JY; Grimm, LJ; Mazurowski, MA; Baker, JA; Marks, JR; King, LM; Maley, CC; Hwang, E-SS; Shi, B", "published_date": "February 1, 2018", "doi": " ", "abstract": " ", "publication_location": "Cancer Research", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000425489400032&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions", "authors": "Li, O; Liu, H; Chen, C; Rudin, C", "published_date": "January 1, 2018", "doi": " ", "abstract": "Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability - they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as “black box” models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.", "publication_location": "32nd Aaai Conference on Artificial Intelligence, Aaai 2018", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep Learning-Based Risk Model for Best Management of Closed Groin Incisions After Vascular Surgery.", "authors": "Chang, B; Sun, Z; Peiris, P; Huang, ES; Benrashid, E; Dillavou, ED", "published_date": "March 17, 2020", "doi": "10.1016/j.jss.2020.02.012", "abstract": "BACKGROUND: Reduced surgical site infection (SSI) rates have been reported with use of closed incision negative pressure therapy (ciNPT) in high-risk patients. METHODS: A deep learning-based, risk-based prediction model was developed from a large national database of 72,435 patients who received infrainguinal vascular surgeries involving upper thigh/groin incisions. Patient demographics, histories, laboratory values, and other variables were inputs to the multilayered, adaptive model. The model was then retrospectively applied to a prospectively tracked single hospital data set of 370 similar patients undergoing vascular surgery, with ciNPT or control dressings applied over the closed incision at the surgeon's discretion. Objective predictive risk scores were generated for each patient and used to categorize patients as \"high\" or \"low\" predicted risk for SSI. RESULTS: Actual institutional cohort SSI rates were 10/148 (6.8%) and 28/134 (20.9%) for high-risk ciNPT versus control, respectively (P < 0.001), and 3/31 (9.7%) and 5/57 (8.8%) for low-risk ciNPT versus control, respectively (P = 0.99). Application of the model to the institutional cohort suggested that 205/370 (55.4%) patients were matched with their appropriate intervention over closed surgical incision (high risk with ciNPT or low risk with control), and 165/370 (44.6%) were inappropriately matched. With the model applied to the cohort, the predicted SSI rate with perfect utilization would be 27/370 (7.3%), versus 12.4% actual rate, with estimated cost savings of $231-$458 per patient. CONCLUSIONS: Compared with a subjective practice strategy, an objective risk-based strategy using prediction software may be associated with superior results in optimizing SSI rates and costs after vascular surgery.", "publication_location": "J Surg Res", "link": "http://dx.doi.org/10.1016/j.jss.2020.02.012", "citations": " ", "readership": " ", "tweets": "1", "news_mentions": " "},
{"title": "MRI-based treatment planning for proton radiotherapy: dosimetric validation of a deep learning-based liver synthetic CT generation method.", "authors": "Liu, Y; Lei, Y; Wang, Y; Wang, T; Ren, L; Lin, L; McDonald, M; Curran, WJ; Liu, T; Zhou, J; Yang, X", "published_date": "July 16, 2019", "doi": "10.1088/1361-6560/ab25bc", "abstract": "Magnetic resonance imaging (MRI) has been widely used in combination with computed tomography (CT) radiation therapy because MRI improves the accuracy and reliability of target delineation due to its superior soft tissue contrast over CT. The MRI-only treatment process is currently an active field of research since it could eliminate systematic MR-CT co-registration errors, reduce medical cost, avoid diagnostic radiation exposure, and simplify clinical workflow. The purpose of this work is to validate the application of a deep learning-based method for abdominal synthetic CT (sCT) generation by image evaluation and dosimetric assessment in a commercial proton pencil beam treatment planning system (TPS). This study proposes to integrate dense block into a 3D cycle-consistent generative adversarial networks (cycle GAN) framework in an effort to effectively learn the nonlinear mapping between MRI and CT pairs. A cohort of 21 patients with co-registered CT and MR pairs were used to test the deep learning-based sCT image quality by leave-one-out cross validation. The CT image quality, dosimetric accuracy and the distal range fidelity were rigorously checked, using side-by-side comparison against the corresponding original CT images. The average mean absolute error (MAE) was 72.87  ±  18.16 HU. The relative differences of the statistics of the PTV dose volume histogram (DVH) metrics between sCT and CT were generally less than 1%. Mean 3D gamma analysis passing rate of 1 mm/1%, 2 mm/2%, 3 mm/3% criteria with 10% dose threshold were 90.76%  ±  5.94%, 96.98%  ±  2.93% and 99.37%  ±  0.99%, respectively. The median, mean and standard deviation of absolute maximum range differences were 0.170 cm, 0.186 cm and 0.155 cm. The image similarity, dosimetric and distal range agreement between sCT and original CT suggests the feasibility of further development of an MRI-only workflow for liver proton radiotherapy.", "publication_location": "Phys Med Biol", "link": "http://dx.doi.org/10.1088/1361-6560/ab25bc", "citations": "9", "readership": "21", "tweets": "5", "news_mentions": " "},
{"title": "Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm.", "authors": "Buda, M; Saha, A; Mazurowski, MA", "published_date": "June 2019", "doi": "10.1016/j.compbiomed.2019.05.002", "abstract": "Recent analysis identified distinct genomic subtypes of lower-grade glioma tumors which are associated with shape features. In this study, we propose a fully automatic way to quantify tumor imaging characteristics using deep learning-based segmentation and test whether these characteristics are predictive of tumor genomic subtypes. We used preoperative imaging and genomic data of 110 patients from 5 institutions with lower-grade gliomas from The Cancer Genome Atlas. Based on automatic deep learning segmentations, we extracted three features which quantify two-dimensional and three-dimensional characteristics of the tumors. Genomic data for the analyzed cohort of patients consisted of previously identified genomic clusters based on IDH mutation and 1p/19q co-deletion, DNA methylation, gene expression, DNA copy number, and microRNA expression. To analyze the relationship between the imaging features and genomic clusters, we conducted the Fisher exact test for 10 hypotheses for each pair of imaging feature and genomic subtype. To account for multiple hypothesis testing, we applied a Bonferroni correction. P-values lower than 0.005 were considered statistically significant. We found the strongest association between RNASeq clusters and the bounding ellipsoid volume ratio (p < 0.0002) and between RNASeq clusters and margin fluctuation (p < 0.005). In addition, we identified associations between bounding ellipsoid volume ratio and all tested molecular subtypes (p < 0.02) as well as between angular standard deviation and RNASeq cluster (p < 0.02). In terms of automatic tumor segmentation that was used to generate the quantitative image characteristics, our deep learning algorithm achieved a mean Dice coefficient of 82% which is comparable to human performance.", "publication_location": "Comput Biol Med", "link": "http://dx.doi.org/10.1016/j.compbiomed.2019.05.002", "citations": "10", "readership": "55", "tweets": "17", "news_mentions": " "},
{"title": "Beyond Performance Metrics: Automatic Deep Learning Retinal OCT Analysis Reproduces Clinical Trial Outcome.", "authors": "Loo, J; Clemons, TE; Chew, EY; Friedlander, M; Jaffe, GJ; Farsiu, S", "published_date": "June 2020", "doi": "10.1016/j.ophtha.2019.12.015", "abstract": "PURPOSE:To validate the efficacy of a fully automatic, deep learning-based segmentation algorithm beyond conventional performance metrics by measuring the primary outcome of a clinical trial for macular telangiectasia type 2 (MacTel2). DESIGN:Evaluation of diagnostic test or technology. PARTICIPANTS:A total of 92 eyes from 62 participants with MacTel2 from a phase 2 clinical trial (NCT01949324) randomized to 1 of 2 treatment groups METHODS: The ellipsoid zone (EZ) defect areas were measured on spectral domain OCT images of each eye at 2 time points (baseline and month 24) by a fully automatic, deep learning-based segmentation algorithm. The change in EZ defect area from baseline to month 24 was calculated and analyzed according to the clinical trial protocol. MAIN OUTCOME MEASURE:Difference in the change in EZ defect area from baseline to month 24 between the 2 treatment groups. RESULTS:The difference in the change in EZ defect area from baseline to month 24 between the 2 treatment groups measured by the fully automatic segmentation algorithm was 0.072±0.035 mm2 (P = 0.021). This was comparable to the outcome of the clinical trial using semiautomatic measurements by expert readers, 0.065±0.033 mm2 (P = 0.025). CONCLUSIONS:The fully automatic segmentation algorithm was as accurate as semiautomatic expert segmentation to assess EZ defect areas and was able to reliably reproduce the statistically significant primary outcome measure of the clinical trial. This approach, to validate the performance of an automatic segmentation algorithm on the primary clinical trial end point, provides a robust gauge of its clinical applicability.", "publication_location": "Ophthalmology", "link": "http://dx.doi.org/10.1016/j.ophtha.2019.12.015", "citations": "2", "readership": "14", "tweets": "6", "news_mentions": " "},
{"title": "Human Versus Machine: Comparing a Deep Learning Algorithm to Human Gradings for Detecting Glaucoma on Fundus Photographs.", "authors": " ", "published_date": "March 2020", "doi": "10.1016/j.ajo.2019.11.006", "abstract": "PURPOSE: To compare the diagnostic performance of human gradings vs predictions provided by a machine-to-machine (M2M) deep learning (DL) algorithm trained to quantify retinal nerve fiber layer (RNFL) damage on fundus photographs. DESIGN: Evaluation of a machine learning algorithm. METHODS: An M2M DL algorithm trained with RNFL thickness parameters from spectral-domain optical coherence tomography was applied to a subset of 490 fundus photos of 490 eyes of 370 subjects graded by 2 glaucoma specialists for the probability of glaucomatous optical neuropathy (GON), and estimates of cup-to-disc (C/D) ratios. Spearman correlations with standard automated perimetry (SAP) global indices were compared between the human gradings vs the M2M DL-predicted RNFL thickness values. The area under the receiver operating characteristic curves (AUC) and partial AUC for the region of clinically meaningful specificity (85%-100%) were used to compare the ability of each output to discriminate eyes with repeatable glaucomatous SAP defects vs eyes with normal fields. RESULTS: The M2M DL-predicted RNFL thickness had a significantly stronger absolute correlation with SAP mean deviation (rho=0.54) than the probability of GON given by human graders (rho=0.48; P < .001). The partial AUC for the M2M DL algorithm was significantly higher than that for the probability of GON by human graders (partial AUC = 0.529 vs 0.411, respectively; P = .016). CONCLUSION: An M2M DL algorithm performed as well as, if not better than, human graders at detecting eyes with repeatable glaucomatous visual field loss. This DL algorithm could potentially replace human graders in population screening efforts for glaucoma.", "publication_location": "American Journal of Ophthalmology", "link": "http://dx.doi.org/10.1016/j.ajo.2019.11.006", "citations": "5", "readership": "30", "tweets": "5", "news_mentions": " "},
{"title": "From Machine to Machine: An OCT-Trained Deep Learning Algorithm for Objective Quantification of Glaucomatous Damage in Fundus Photographs.", "authors": "Medeiros, FA; Jammal, AA; Thompson, AC", "published_date": "April 2019", "doi": "10.1016/j.ophtha.2018.12.033", "abstract": "PURPOSE: Previous approaches using deep learning (DL) algorithms to classify glaucomatous damage on fundus photographs have been limited by the requirement for human labeling of a reference training set. We propose a new approach using quantitative spectral-domain (SD) OCT data to train a DL algorithm to quantify glaucomatous structural damage on optic disc photographs. DESIGN: Cross-sectional study. PARTICIPANTS: A total of 32 820 pairs of optic disc photographs and SD OCT retinal nerve fiber layer (RNFL) scans from 2312 eyes of 1198 participants. METHODS: The sample was divided randomly into validation plus training (80%) and test (20%) sets, with randomization performed at the patient level. A DL convolutional neural network was trained to assess optic disc photographs and predict SD OCT average RNFL thickness. MAIN OUTCOME MEASURES: The DL algorithm performance was evaluated in the test sample by evaluating correlation and agreement between the predictions and actual SD OCT measurements. We also assessed the ability to discriminate eyes with glaucomatous visual field loss from healthy eyes with area under the receiver operating characteristic (ROC) curves. RESULTS: The mean prediction of average RNFL thickness from all 6292 optic disc photographs in the test set was 83.3±14.5 μm, whereas the mean average RNFL thickness from all corresponding SD OCT scans was 82.5±16.8 μm (P = 0.164). There was a very strong correlation between predicted and observed RNFL thickness values (Pearson r = 0.832; R2 = 69.3%; P < 0.001), with mean absolute error of the predictions of 7.39 μm. The area under the ROC curves for discriminating glaucomatous from healthy eyes with the DL predictions and actual SD OCT average RNFL thickness measurements were 0.944 (95% confidence interval [CI], 0.912-0.966) and 0.940 (95% CI, 0.902-0.966), respectively (P = 0.724). CONCLUSIONS: We introduced a novel DL approach to assess fundus photographs and provide quantitative information about the amount of neural damage that can be used to diagnose and stage glaucoma. In addition, training neural networks to predict SD OCT data objectively represents a new approach that overcomes limitations of human labeling and could be useful in other areas of ophthalmology.", "publication_location": "Ophthalmology", "link": "http://dx.doi.org/10.1016/j.ophtha.2018.12.033", "citations": "25", "readership": "70", "tweets": "24", "news_mentions": " "},
{"title": "Image analysis with deep learning to predict breast cancer grade, ER status, histologic subtype, and intrinsic subtype.", "authors": "Couture, HD; Williams, LA; Geradts, J; Nyante, SJ; Butler, EN; Marron, JS; Perou, CM; Troester, MA; Niethammer, M", "published_date": "2018", "doi": "10.1038/s41523-018-0079-1", "abstract": "RNA-based, multi-gene molecular assays are available and widely used for patients with ER-positive/HER2-negative breast cancers. However, RNA-based genomic tests can be costly and are not available in many countries. Methods for inferring molecular subtype from histologic images may identify patients most likely to benefit from further genomic testing. To identify patients who could benefit from molecular testing based on H&E stained histologic images, we developed an image analysis approach using deep learning. A training set of 571 breast tumors was used to create image-based classifiers for tumor grade, ER status, PAM50 intrinsic subtype, histologic subtype, and risk of recurrence score (ROR-PT). The resulting classifiers were applied to an independent test set (n = 288), and accuracy, sensitivity, and specificity of each was assessed on the test set. Histologic image analysis with deep learning distinguished low-intermediate vs. high tumor grade (82% accuracy), ER status (84% accuracy), Basal-like vs. non-Basal-like (77% accuracy), Ductal vs. Lobular (94% accuracy), and high vs. low-medium ROR-PT score (75% accuracy). Sampling considerations in the training set minimized bias in the test set. Incorrect classification of ER status was significantly more common for Luminal B tumors. These data provide proof of principle that molecular marker status, including a critical clinical biomarker (i.e., ER status), can be predicted with accuracy >75% based on H&E features. Image-based methods could be promising for identifying patients with a greater need for further genomic testing, or in place of classically scored variables typically accomplished using human-based scoring.", "publication_location": "Npj Breast Cancer", "link": "http://dx.doi.org/10.1038/s41523-018-0079-1", "citations": "28", "readership": "108", "tweets": "12", "news_mentions": "12"},
{"title": "MRI-based treatment planning for liver stereotactic body radiotherapy: validation of a deep learning-based synthetic CT generation method.", "authors": "Liu, Y; Lei, Y; Wang, T; Kayode, O; Tian, S; Liu, T; Patel, P; Curran, WJ; Ren, L; Yang, X", "published_date": "August 2019", "doi": "10.1259/bjr.20190067", "abstract": "OBJECTIVE: The purpose of this work is to develop and validate a learning-based method to derive electron density from routine anatomical MRI for potential MRI-based SBRT treatment planning. METHODS: We proposed to integrate dense block into cycle generative adversarial network (GAN) to effectively capture the relationship between the CT and MRI for CT synthesis. A cohort of 21 patients with co-registered CT and MR pairs were used to evaluate our proposed method by the leave-one-out cross-validation. Mean absolute error, peak signal-to-noise ratio and normalized cross-correlation were used to quantify the imaging differences between the synthetic CT (sCT) and CT. The accuracy of Hounsfield unit (HU) values in sCT for dose calculation was evaluated by comparing the dose distribution in sCT-based and CT-based treatment planning. Clinically relevant dose-volume histogram metrics were then extracted from the sCT-based and CT-based plans for quantitative comparison. RESULTS: The mean absolute error, peak signal-to-noise ratio and normalized cross-correlation of the sCT were 72.87 ± 18.16 HU, 22.65 ± 3.63 dB and 0.92 ± 0.04, respectively. No significant differences were observed in the majority of the planning target volume and organ at risk dose-volume histogram metrics ( p  >  0.05). The average pass rate of γ analysis was over 99% with 1%/1 mm acceptance criteria on the coronal plane that intersects with isocenter. CONCLUSION: The image similarity and dosimetric agreement between sCT and original CT warrant further development of an MRI-only workflow for liver stereotactic body radiation therapy. ADVANCES IN KNOWLEDGE: This work is the first deep-learning-based approach to generating abdominal sCT through dense-cycle-GAN. This method can successfully generate the small bony structures such as the rib bones and is able to predict the HU values for dose calculation with comparable accuracy to reference CT images.", "publication_location": "The British Journal of Radiology", "link": "http://dx.doi.org/10.1259/bjr.20190067", "citations": "5", "readership": "28", "tweets": "1", "news_mentions": " "},
{"title": "Deep learning based spectral extrapolation for dual-source, dual-energy x-ray computed tomography.", "authors": "Clark, DP; Schwartz, FR; Marin, D; Ramirez-Giraldo, JC; Badea, CT", "published_date": "June 12, 2020", "doi": "10.1002/mp.14324", "abstract": "PURPOSE: Data completion is commonly employed in dual-source, dual-energy computed tomography (CT) when physical or hardware constraints limit the field of view (FoV) covered by one of two imaging chains. Practically, dual-energy data completion is accomplished by estimating missing projection data based on the imaging chain with the full FoV and then by appropriately truncating the analytical reconstruction of the data with the smaller FoV. While this approach works well in many clinical applications, there are applications which would benefit from spectral contrast estimates over the larger FoV (spectral extrapolation)-e.g. model-based iterative reconstruction, contrast-enhanced abdominal imaging of large patients, interior tomography, and combined temporal and spectral imaging. METHODS: To document the fidelity of spectral extrapolation and to prototype a deep learning algorithm to perform it, we assembled a data set of 50 dual-source, dual-energy abdominal x-ray CT scans (acquired at Duke University Medical Center with 5 Siemens Flash scanners; chain A: 50 cm FoV, 100 kV; chain B: 33 cm FoV, 140 kV + Sn; helical pitch: 0.8). Data sets were reconstructed using ReconCT (v14.1, Siemens Healthineers): 768 × 768 pixels per slice, 50 cm FoV, 0.75 mm slice thickness, \"Dual-Energy - WFBP\" reconstruction mode with dual-source data completion. A hybrid architecture consisting of a learned piecewise linear transfer function (PLTF) and a convolutional neural network (CNN) was trained using 40 scans (five scans reserved for validation, five for testing). The PLTF learned to map chain A spectral contrast to chain B spectral contrast voxel-wise, performing an image domain analog of dual-source data completion with approximate spectral reweighting. The CNN with its U-net structure then learned to improve the accuracy of chain B contrast estimates by copying chain A structural information, by encoding prior chain A, chain B contrast relationships, and by generalizing feature-contrast associations. Training was supervised, using data from within the 33-cm chain B FoV to optimize and assess network performance. RESULTS: Extrapolation performance on the testing data confirmed our network's robustness and ability to generalize to unseen data from different patients, yielding maximum extrapolation errors of 26 HU following the PLTF and 7.5 HU following the CNN (averaged per target organ). Degradation of network performance when applied to a geometrically simple phantom confirmed our method's reliance on feature-contrast relationships in correctly inferring spectral contrast. Integrating our image domain spectral extrapolation network into a standard dual-source, dual-energy processing pipeline for Siemens Flash scanner data yielded spectral CT data with adequate fidelity for the generation of both 50 keV monochromatic images and material decomposition images over a 30-cm FoV for chain B when only 20 cm of chain B data were available for spectral extrapolation. CONCLUSIONS: Even with a moderate amount of training data, deep learning methods are capable of robustly inferring spectral contrast from feature-contrast relationships in spectral CT data, leading to spectral extrapolation performance well beyond what may be expected at face value. Future work reconciling spectral extrapolation results with original projection data is expected to further improve results in outlying and pathological cases.", "publication_location": "Med Phys", "link": "http://dx.doi.org/10.1002/mp.14324", "citations": " ", "readership": " ", "tweets": "8", "news_mentions": " "},
{"title": "Automatic threat recognition of prohibited items at aviation checkpoint with x-ray imaging: A deep learning approach", "authors": "Liang, KJ; Heilmann, G; Gregory, C; Diallo, SO; Carlson, D; Spell, GP; Sigman, JB; Roe, K; Carin, L", "published_date": "January 1, 2018", "doi": "10.1117/12.2309484", "abstract": "© Copyright 2018 SPIE. The Transportation Security Administration safeguards all United States air travel. To do so, they employ human inspectors to screen x-ray images of carry-on baggage for threats and other prohibited items, which can be challenging. On the other hand, recent research applying deep learning techniques to computer-aided security screening to assist operators has yielded encouraging results. Deep learning is a subfield of machine learning based on learning abstractions from data, as opposed to engineering features by hand. These techniques have proven to be quite effective in many domains, including computer vision, natural language processing, speech recognition, self-driving cars, and geographical mapping technology. In this paper, we present initial results of a collaboration between Smiths Detection and Duke University funded by the Transportation Security Administration. Using convolutional object detection algorithms trained on annotated x-ray images, we show real-time detection of prohibited items in carry-on luggage. Results of the work so far indicate that this approach can detect selected prohibited items with high accuracy and minimal impact on operational false alarm rates.", "publication_location": "Smart Structures and Materials 2005: Active Materials: Behavior and Mechanics", "link": "http://dx.doi.org/10.1117/12.2309484", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning based detection of cone photoreceptors with multimodal adaptive optics scanning light ophthalmoscope images of achromatopsia.", "authors": "Cunefare, D; Langlo, CS; Patterson, EJ; Blau, S; Dubra, A; Carroll, J; Farsiu, S", "published_date": "August 2018", "doi": "10.1364/boe.9.003740", "abstract": "Fast and reliable quantification of cone photoreceptors is a bottleneck in the clinical utilization of adaptive optics scanning light ophthalmoscope (AOSLO) systems for the study, diagnosis, and prognosis of retinal diseases. To-date, manual grading has been the sole reliable source of AOSLO quantification, as no automatic method has been reliably utilized for cone detection in real-world low-quality images of diseased retina. We present a novel deep learning based approach that combines information from both the confocal and non-confocal split detector AOSLO modalities to detect cones in subjects with achromatopsia. Our dual-mode deep learning based approach outperforms the state-of-the-art automated techniques and is on a par with human grading.", "publication_location": "Biomedical Optics Express", "link": "http://dx.doi.org/10.1364/boe.9.003740", "citations": "13", "readership": "25", "tweets": "3", "news_mentions": " "},
{"title": "Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes.", "authors": "Ting, DSW; Cheung, CY-L; Lim, G; Tan, GSW; Quang, ND; Gan, A; Hamzah, H; Garcia-Franco, R; San Yeo, IY; Lee, SY; Wong, EYM; Sabanayagam, C; Baskaran, M; Ibrahim, F; Tan, NC; Finkelstein, EA; Lamoureux, EL; Wong, IY; Bressler, NM; Sivaprasad, S; Varma, R; Jonas, JB; He, MG; Cheng, C-Y; Cheung, GCM; Aung, T; Hsu, W; Lee, ML; Wong, TY", "published_date": "December 12, 2017", "doi": "10.1001/jama.2017.18152", "abstract": "Importance: A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases. Objective: To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes. Design, Setting, and Participants: Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes. Exposures: Use of a deep learning system. Main Outcomes and Measures: Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard. Results: In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6% men), the prevalence of referable diabetic retinopathy was 3.0%; vision-threatening diabetic retinopathy, 0.6%; possible glaucoma, 0.1%; and AMD, 2.5%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95% CI, 0.925-0.943), sensitivity was 90.5% (95% CI, 87.3%-93.0%), and specificity was 91.6% (95% CI, 91.0%-92.2%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95% CI, 0.956-0.961), sensitivity was 100% (95% CI, 94.1%-100.0%), and specificity was 91.1% (95% CI, 90.7%-91.4%). For possible glaucoma, AUC was 0.942 (95% CI, 0.929-0.954), sensitivity was 96.4% (95% CI, 81.7%-99.9%), and specificity was 87.2% (95% CI, 86.8%-87.5%). For AMD, AUC was 0.931 (95% CI, 0.928-0.935), sensitivity was 93.2% (95% CI, 91.1%-99.8%), and specificity was 88.7% (95% CI, 88.3%-89.0%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images). Conclusions and Relevance: In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.", "publication_location": "Jama", "link": "http://dx.doi.org/10.1001/jama.2017.18152", "citations": "389", "readership": "546", "tweets": "331", "news_mentions": "8"},
{"title": "Deep Learning-Based Segmentation of Nodules in Thyroid Ultrasound: Improving Performance by Utilizing Markers Present in the Images.", "authors": "Buda, M; Wildman-Tobriner, B; Castor, K; Hoang, JK; Mazurowski, MA", "published_date": "February 2020", "doi": "10.1016/j.ultrasmedbio.2019.10.003", "abstract": "Computer-aided segmentation of thyroid nodules in ultrasound imaging could assist in their accurate characterization. In this study, using data for 1278 nodules, we proposed and evaluated two methods for deep learning-based segmentation of thyroid nodules that utilize calipers present in the images. The first method used approximate nodule masks generated based on the calipers. The second method combined manual annotations with automatic guidance by the calipers. When only approximate nodule masks were used for training, the achieved Dice similarity coefficient (DSC) was 85.1%. The performance of a network trained using manual annotations was DSC = 90.4%. When the guidance by the calipers was added, the performance increased to DSC = 93.1%. An increase in the number of cases used for training resulted in increased performance for all methods. The proposed method utilizing the guidance by calipers matched the performance of the network that did not use it with a reduced number of manually annotated training cases.", "publication_location": "Ultrasound Med Biol", "link": "http://dx.doi.org/10.1016/j.ultrasmedbio.2019.10.003", "citations": " ", "readership": "11", "tweets": "4", "news_mentions": " "},
{"title": "Artificial intelligence using deep learning to screen for referable and vision-threatening diabetic retinopathy in Africa: a clinical validation study", "authors": "Bellemo, V; Lim, ZW; Lim, G; Nguyen, QD; Xie, Y; Yip, MYT; Hamzah, H; Ho, J; Lee, XQ; Hsu, W; Lee, ML; Musonda, L; Chandran, M; Chipalo-Mutati, G; Muma, M; Tan, GSW; Sivaprasad, S; Menon, G; Wong, TY; Ting, DSW", "published_date": "May 1, 2019", "doi": "10.1016/S2589-7500(19)30004-4", "abstract": "© 2019 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license Background: Radical measures are required to identify and reduce blindness due to diabetes to achieve the Sustainable Development Goals by 2030. Therefore, we evaluated the accuracy of an artificial intelligence (AI) model using deep learning in a population-based diabetic retinopathy screening programme in Zambia, a lower-middle-income country. Methods: We adopted an ensemble AI model consisting of a combination of two convolutional neural networks (an adapted VGGNet architecture and a residual neural network architecture) for classifying retinal colour fundus images. We trained our model on 76 370 retinal fundus images from 13 099 patients with diabetes who had participated in the Singapore Integrated Diabetic Retinopathy Program, between 2010 and 2013, which has been published previously. In this clinical validation study, we included all patients with a diagnosis of diabetes that attended a mobile screening unit in five urban centres in the Copperbelt province of Zambia from Feb 1 to June 31, 2012. In our model, referable diabetic retinopathy was defined as moderate non-proliferative diabetic retinopathy or worse, diabetic macular oedema, and ungradable images. Vision-threatening diabetic retinopathy comprised severe non-proliferative and proliferative diabetic retinopathy. We calculated the area under the curve (AUC), sensitivity, and specificity for referable diabetic retinopathy, and sensitivities of vision-threatening diabetic retinopathy and diabetic macular oedema compared with the grading by retinal specialists. We did a multivariate analysis for systemic risk factors and referable diabetic retinopathy between AI and human graders. Findings: A total of 4504 retinal fundus images from 3093 eyes of 1574 Zambians with diabetes were prospectively recruited. Referable diabetic retinopathy was found in 697 (22·5%) eyes, vision-threatening diabetic retinopathy in 171 (5·5%) eyes, and diabetic macular oedema in 249 (8·1%) eyes. The AUC of the AI system for referable diabetic retinopathy was 0·973 (95% CI 0·969–0·978), with corresponding sensitivity of 92·25% (90·10–94·12) and specificity of 89·04% (87·85–90·28). Vision-threatening diabetic retinopathy sensitivity was 99·42% (99·15–99·68) and diabetic macular oedema sensitivity was 97·19% (96·61–97·77). The AI model and human graders showed similar outcomes in referable diabetic retinopathy prevalence detection and systemic risk factors associations. Both the AI model and human graders identified longer duration of diabetes, higher level of glycated haemoglobin, and increased systolic blood pressure as risk factors associated with referable diabetic retinopathy. Interpretation: An AI system shows clinically acceptable performance in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, and diabetic macular oedema in population-based diabetic retinopathy screening. This shows the potential application and adoption of such AI technology in an under-resourced African population to reduce the incidence of preventable blindness, even when the model is trained in a different population. Funding: National Medical Research Council Health Service Research Grant, Large Collaborative Grant, Ministry of Health, Singapore; the SingHealth Foundation; and the Tanoto Foundation.", "publication_location": "The Lancet Digital Health", "link": "http://dx.doi.org/10.1016/S2589-7500(19)30004-4", "citations": "21", "readership": "108", "tweets": "22", "news_mentions": " "},
{"title": "Fast and robust active neuron segmentation in two-photon calcium imaging using spatiotemporal deep learning.", "authors": "Soltanian-Zadeh, S; Sahingur, K; Blau, S; Gong, Y; Farsiu, S", "published_date": "April 11, 2019", "doi": "10.1073/pnas.1812995116", "abstract": "Calcium imaging records large-scale neuronal activity with cellular resolution in vivo. Automated, fast, and reliable active neuron segmentation is a critical step in the analysis workflow of utilizing neuronal signals in real-time behavioral studies for discovery of neuronal coding properties. Here, to exploit the full spatiotemporal information in two-photon calcium imaging movies, we propose a 3D convolutional neural network to identify and segment active neurons. By utilizing a variety of two-photon microscopy datasets, we show that our method outperforms state-of-the-art techniques and is on a par with manual segmentation. Furthermore, we demonstrate that the network trained on data recorded at a specific cortical layer can be used to accurately segment active neurons from another layer with different neuron density. Finally, our work documents significant tabulation flaws in one of the most cited and active online scientific challenges in neuron segmentation. As our computationally fast method is an invaluable tool for a large spectrum of real-time optogenetic experiments, we have made our open-source software and carefully annotated dataset freely available online.", "publication_location": "Proceedings of the National Academy of Sciences of the United States of America", "link": "http://dx.doi.org/10.1073/pnas.1812995116", "citations": "14", "readership": "118", "tweets": "7", "news_mentions": "14"},
{"title": "A Novel Deformable Registration Method Using a Multiscale Framework with Joint Training of Unsupervised Deep Learning Models", "authors": "Jiang, Z; Yin, F; Ren, L", "published_date": "June 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Medical Physics", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000471277704013&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning analysis of breast MRIs for prediction of occult invasive disease in ductal carcinoma in situ.", "authors": "Zhu, Z; Harowicz, MR; Zhang, J; Saha, A; Grimm, LJ; Hwang, E-SS; Mazurowski, MA", "published_date": "2019", "doi": " ", "abstract": " ", "publication_location": "Comp. in Bio. and Med.", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Dose-Specific PET Image-Based Outcome Prediction: A Deep Learning Study for Oropharyngeal Cancer IMRT Application", "authors": "Liu, C; Wang, C; Lafata, K; Chang, Y; Cui, Y; Yin, F", "published_date": "June 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Medical Physics", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000471277704100&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "IP145. Deep Learning-Based Risk Model for Best Management of Closed Surgical Incisions Following Vascular Surgery", "authors": "Dillavou, ED; Sun, Z; Peiris, P; Huang, E; Benrashid, E; Chang, B", "published_date": "June 2019", "doi": "10.1016/j.jvs.2019.04.196", "abstract": " ", "publication_location": "Journal of Vascular Surgery", "link": "http://dx.doi.org/10.1016/j.jvs.2019.04.196", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Detecting Acute Cellular Rejection in Lung Transplant Biopsies by Artificial Intelligence: A Novel Deep Learning Approach", "authors": "Davis, H; Glass, C; Davis, RC; Glass, M; Pavlisko, EN", "published_date": "April 1, 2020", "doi": "10.1016/j.healun.2020.01.100", "abstract": "Copyright © 2020. Published by Elsevier Inc. PURPOSE: Acute cellular rejection (ACR), and its relation to chronic lung allograft dysfunction (CLAD), is an important cause of morbidity and mortality in lung transplant patients. ACR occurs in more than one third of lung transplant recipients in the first year after transplantation; diagnosis largely rests on histopathologic evaluation of lung biopsy. The reproducibility of the diagnosis of ACR is quite variable, even among experienced transplant pathologists with published kappa values of interobserver agreement at 0.183 and 0.24-0.62. With digital pathology and artificial intelligence (AI) technology, it may be possible to aid pathologists in the diagnosis of ACR. Herein we examine, for the first time, if an AI machine learning algorithm can reliably distinguish the vascular component of rejection in lung transplant biopsies from normal lung. METHODS: At a single high-volume lung transplant center, annotations were completed by board-certified lung transplant pathologists from hematoxylin and eosin stained slides scanned using a Leica Aperio AT2 digital whole slide scanner at 40X magnification. Annotations were imported into a Python 3.7 development environment using the open-source 'Openslide' library. Keras with a tensorflow backend was used for AI modeling and training. All training was performed on a Windows 10 based PC with an Intel Core I-9 processor, 64gb of ram, and Nvidia GPU. RESULTS: A total of 3,349 annotations (2580 regions of normal, 769 lesions of A1/A2 rejection) were completed. For the training set, 614 A1/A2 lesions and 2064 regions of normal were evaluated. For the validation set, 155 A1/A2 lesions and 156 regions of normal were used to test the AI algorithm's accuracy. The algorithm distinguished the vascular component of ACR from normal alveolated lung tissue with 95% validation accuracy. CONCLUSION: To our knowledge, our study is the first to provide evidence that an AI machine algorithm can reach acceptable, and possibly surpass, performance of current diagnostic standards of identifying ACR in lung transplant patients. Future studies should include multi-institutional validation testing.", "publication_location": "J Heart Lung Transplant", "link": "http://dx.doi.org/10.1016/j.healun.2020.01.100", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning segmentation of the mouse cornea from anterior segment OCT in the presence of non-corneal and non-physiologic structures", "authors": "Nallasamy, N; Liu, Y; Kuo, AN", "published_date": "July 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Investigative Ophthalmology & Visual Science", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000488628105081&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A multitask deep learning method in simultaneously predicting occult invasive disease in ductal carcinoma in-situ and segmenting microcalcifications in mammography", "authors": "Hou, R; Grimm, LJ; Mazurowski, MA; Marks, JR; King, LM; Maley, CC; Hwang, ES; Lo, JY", "published_date": "January 1, 2020", "doi": "10.1117/12.2549669", "abstract": "© 2020 SPIE. We proposed a two-branch multitask learning convolutional neural network to solve two different but related tasks at the same time. Our main task is to predict occult invasive disease in biopsy proven Ductal Carcinoma in-situ (DCIS), with an auxiliary task of segmenting microcalcifications (MCs). In this study, we collected digital mammography from 604 patients, 400 of which were DCIS. The model used patches with size of 512×512 extracted within a radiologist masked ROIs as input, with outputs including noisy MC segmentations obtained from our previous algorithms, and classification labels from final diagnosis at patients' definite surgery. We utilized a deep multitask model by combining both Unet segmentation networks and prediction classification networks, by sharing first several convolutional layers. The model achieved a patch-based ROC-AUC of 0.69, with a case-based ROC-AUC of 0.61. Segmentation results achieved a dice coefficient of 0.49.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2549669", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning-based features of breast MRI for prediction of occult invasive disease following a diagnosis of ductal carcinoma in situ: Preliminary data", "authors": "Zhu, Z; Harowicz, M; Zhang, J; Saha, A; Grimm, LJ; Hwang, S; Mazurowski, MA", "published_date": "January 1, 2018", "doi": "10.1117/12.2295470", "abstract": "© 2018 SPIE. Approximately 25% of patients with ductal carcinoma in situ (DCIS) diagnosed from core needle biopsy are subsequently upstaged to invasive cancer at surgical excision. Identifying patients with occult invasive disease is important as it changes treatment and precludes enrollment in active surveillance for DCIS. In this study, we investigated upstaging of DCIS to invasive disease using deep features. While deep neural networks require large amounts of training data, the available data to predict DCIS upstaging is sparse and thus directly training a neural network is unlikely to be successful. In this work, a pre-trained neural network is used as a feature extractor and a support vector machine (SVM) is trained on the extracted features. We used the dynamic contrast-enhanced (DCE) MRIs of patients at our institution from January 1, 2000, through March 23, 2014 who underwent MRI following a diagnosis of DCIS. Among the 131 DCIS patients, there were 35 patients who were upstaged to invasive cancer. Area under the ROC curve within the 10-fold cross-validation scheme was used for validation of our predictive model. The use of deep features was able to achieve an AUC of 0.68 (95% CI: 0.56-0.78) to predict occult invasive disease. This preliminary work demonstrates the promise of deep features to predict surgical upstaging following a diagnosis of DCIS.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2295470", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning-based automatic segmentation of ellipsoid zone defects in optical coherence tomography images of macular telangiectasia type 2", "authors": "Loo, J; Fang, L; Cunefare, D; Jaffe, GJ; Farsiu, S", "published_date": "July 1, 2018", "doi": " ", "abstract": " ", "publication_location": "Investigative Ophthalmology & Visual Science", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000442912503156&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Open-source, machine and deep learning-based automated algorithm for gestational age estimation through smartphone lens imaging.", "authors": "Desai, AD; Peng, C; Fang, L; Mukherjee, D; Yeung, A; Jaffe, SJ; Griffin, JB; Farsiu, S", "published_date": "December 2018", "doi": "10.1364/boe.9.006038", "abstract": "Gestational age estimation at time of birth is critical for determining the degree of prematurity of the infant and for administering appropriate postnatal treatment. We present a fully automated algorithm for estimating gestational age of premature infants through smartphone lens imaging of the anterior lens capsule vasculature (ALCV). Our algorithm uses a fully convolutional network and blind image quality analyzers to segment usable anterior capsule regions. Then, it extracts ALCV features using a residual neural network architecture and trains on these features using a support vector machine-based classifier. The classification algorithm is validated using leave-one-out cross-validation on videos captured from 124 neonates. The algorithm is expected to be an influential tool for remote and point-of-care gestational age estimation of premature neonates in low-income countries. To this end, we have made the software open source.", "publication_location": "Biomedical Optics Express", "link": "http://dx.doi.org/10.1364/boe.9.006038", "citations": "4", "readership": "24", "tweets": "1", "news_mentions": "4"},
{"title": "Deep learning-based single-shot prediction of differential effects of anti-VEGF treatment in patients with diabetic macular edema.", "authors": "Rasti, R; Allingham, MJ; Mettu, PS; Kavusi, S; Govind, K; Cousins, SW; Farsiu, S", "published_date": "February 1, 2020", "doi": "10.1364/BOE.379150", "abstract": "Anti-vascular endothelial growth factor (VEGF) agents are widely regarded as the first line of therapy for diabetic macular edema (DME) but are not universally effective. An automatic method that can predict whether a patient is likely to respond to anti-VEGF therapy can avoid unnecessary trial and error treatment strategies and promote the selection of more effective first-line therapies. The objective of this study is to automatically predict the efficacy of anti-VEGF treatment of DME in individual patients based on optical coherence tomography (OCT) images. We performed a retrospective study of 127 subjects treated for DME with three consecutive injections of anti-VEGF agents. Patients' retinas were imaged using spectral-domain OCT (SD-OCT) before and after anti-VEGF therapy, and the total retinal thicknesses before and after treatment were extracted from OCT B-scans. A novel deep convolutional neural network was designed and evaluated using pre-treatment OCT scans as input and differential retinal thickness as output, with 5-fold cross-validation. The group of patients responsive to anti-VEGF treatment was defined as those with at least a 10% reduction in retinal thickness following treatment. The predictive performance of the system was evaluated by calculating the precision, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). The algorithm achieved an average AUC of 0.866 in discriminating responsive from non-responsive patients, with an average precision, sensitivity, and specificity of 85.5%, 80.1%, and 85.0%, respectively. Classification precision was significantly higher when differentiating between very responsive and very unresponsive patients. The proposed automatic algorithm accurately predicts the response to anti-VEGF treatment in DME patients based on OCT images. This pilot study is a critical step toward using non-invasive imaging and automated analysis to select the most effective therapy for a patient's specific disease condition.", "publication_location": "Biomedical Optics Express", "link": "http://dx.doi.org/10.1364/BOE.379150", "citations": "3", "readership": " ", "tweets": "9", "news_mentions": "3"},
{"title": "Comprehensive Molecular and Pathologic Evaluation of Transitional Mesothelioma Assisted by Deep Learning Approach: A Multi-Institutional Study of the International Mesothelioma Panel from the MESOPATH Reference Center.", "authors": "Galateau Salle, F; Le Stang, N; Tirode, F; Courtiol, P; Nicholson, AG; Tsao, M-S; Tazelaar, HD; Churg, A; Dacic, S; Roggli, V; Pissaloux, D; Maussion, C; Moarii, M; Beasley, MB; Begueret, H; Chapel, DB; Copin, MC; Gibbs, AR; Klebe, S; Lantuejoul, S; Nabeshima, K; Vignaud, J-M; Attanoos, R; Brcic, L; Capron, F; Chirieac, LR; Damiola, F; Sequeiros, R; Cazes, A; Damotte, D; Foulet, A; Giusiano-Courcambeck, S; Hiroshima, K; Hofman, V; Husain, AN; Kerr, K; Marchevsky, A; Paindavoine, S; Picquenot, JM; Rouquette, I; Sagan, C; Sauter, J; Thivolet, F; Brevet, M; Rouvier, P; Travis, WD; Planchard, G; Weynand, B; Clozel, T; Wainrib, G; Fernandez-Cuesta, L; Pairon, J-C; Rusch, V; Girard, N", "published_date": "June 2020", "doi": "10.1016/j.jtho.2020.01.025", "abstract": "INTRODUCTION: Histologic subtypes of malignant pleural mesothelioma are a major prognostic indicator and decision denominator for all therapeutic strategies. In an ambiguous case, a rare transitional mesothelioma (TM) pattern may be diagnosed by pathologists either as epithelioid mesothelioma (EM), biphasic mesothelioma (BM), or sarcomatoid mesothelioma (SM). This study aimed to better characterize the TM subtype from a histological, immunohistochemical, and molecular standpoint. Deep learning of pathologic slides was applied to this cohort. METHODS: A random selection of 49 representative digitalized sections from surgical biopsies of TM was reviewed by 16 panelists. We evaluated BAP1 expression and CDKN2A (p16) homozygous deletion. We conducted a comprehensive, integrated, transcriptomic analysis. An unsupervised deep learning algorithm was trained to classify tumors. RESULTS: The 16 panelists recorded 784 diagnoses on the 49 cases. Even though a Kappa value of 0.42 is moderate, the presence of a TM component was diagnosed in 51%. In 49% of the histological evaluation, the reviewers classified the lesion as EM in 53%, SM in 33%, or BM in 14%. Median survival was 6.7 months. Loss of BAP1 observed in 44% was less frequent in TM than in EM and BM. p16 homozygous deletion was higher in TM (73%), followed by BM (63%) and SM (46%). RNA sequencing unsupervised clustering analysis revealed that TM grouped together and were closer to SM than to EM. Deep learning analysis achieved 94% accuracy for TM identification. CONCLUSION: These results revealed that the TM pattern should be classified as non-EM or at minimum as a subgroup of the SM type.", "publication_location": "J Thorac Oncol", "link": "http://dx.doi.org/10.1016/j.jtho.2020.01.025", "citations": "3", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep learning of 3D computed tomography (CT) images for organ segmentation using 2D multi-channel SegNet model", "authors": "Liu, Y; Fu, W; Selvakumaran, V; Phelan, M; Segars, WP; Samei, E; Mazurowski, M; Lo, JY; Rubin, GD; Henao, R", "published_date": "January 1, 2019", "doi": "10.1117/12.2512887", "abstract": "© 2019 SPIE. Purpose To accurately segment organs from 3D CT image volumes using a 2D, multi-channel SegNet model consisting of a deep Convolutional Neural Network (CNN) encoder-decoder architecture. Method We trained a SegNet model on the extended cardiac-Torso (XCAT) dataset, which was previously constructed based on patient Chest-Abdomen-Pelvis (CAP) Computed Tomography (CT) studies from 50 Duke patients. Each study consists of one low-resolution (5-mm section thickness) 3D CT image volume and its corresponding 3D, manually labeled volume. To improve modeling on such small sample size regime, we performed median frequency class balancing weighting in the loss function of the SegNet, data normalization adjusting for intensity coverage of CT volumes, data transformation to harmonize voxel resolution, CT section extrapolation to virtually increase the number of transverse sections available as inputs to the 2D multi-channel model, and data augmentation to simulate mildly rotated volumes. To assess model performance, we calculated Dice coefficients on a held-out test set, as well as qualitative evaluation of segmentation on high-resolution CTs. Further, we incorporated 50 patients high-resolution CTs with manually-labeled kidney segmentation masks for the purpose of quantitatively evaluating the performance of our XCAT trained segmentation model. The entire study was conducted from raw, identifiable data within the Duke Protected Analytics Computing Environment (PACE). Result We achieved median Dice coefficients over 0.8 for most organs and structures on XCAT test instances and observed good performance on additional images without manual segmentation labels, qualitatively evaluated by Duke Radiology experts. Moreover, we achieved 0.89 median Dice Coefficients for kidneys on high-resolution CTs. Conclusion 2D, multi-channel models like SegNet are effective for organ segmentations of 3D CT image volumes, achieving high segmentation accuracies.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2512887", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Efficient process-in-memory architecture design for unsupervised GAN-based deep learning using ReRAM", "authors": " ", "published_date": "May 13, 2019", "doi": "10.1145/3299874.3319482", "abstract": "© 2019 ACM. The ending of Moore's Law makes domain-specific architecture as the future of computing. The most representative is the emergence of various deep learning accelerators. Among the proposed solutions, resistive random access memory (ReRAM) based process-in-memory (PIM) architecture is anticipated as a promising candidate because ReRAM has the capability of both data storage and in-situ computation. However, we found that existing solutions are unable to efficiently support the computational needs required by the training of unsupervised generative adversarial networks (GANs), due to the lack of the following two features: 1) Computation efficiency: GAN utilizes a new operator, called transposed convolution. It inserts massive zeros in its input before a convolution operation, resulting in significant resource under-utilization; 2) Data traffic: The data intensive training process of GANs often incurs structural heavy data traffic as well as frequent massive data swaps. Our research follows the PIM strategy by leveraging the energy-efficiency of ReRAM arrays for vector-matrix multiplication to enhance the performance and energy efficiency. Specifically, we propose a novel computation deformation technique that can skip zero-insertions in transposed convolution for computation efficiency improvement. Moreover, we explore an efficient pipelined training procedure to reduce on-chip memory access. The implementation of related circuits and architecture is also discussed. At the end, we present our perspective on the future trend and opportunities of deep learning accelerators.", "publication_location": "Proceedings of the Acm Great Lakes Symposium on Vlsi, Glsvlsi", "link": "http://dx.doi.org/10.1145/3299874.3319482", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic IMRT planning via static field fluence prediction (AIP-SFFP): a deep learning algorithm for real-time prostate treatment planning.", "authors": "Li, X; Zhang, J; Sheng, Y; Chang, Y; Yin, F-F; Ge, Y; Wu, QJ; Wang, C", "published_date": "July 14, 2020", "doi": "10.1088/1361-6560/aba5eb", "abstract": "PURPOSE: To develop a Deep Learning (DL) based algorithm, Automatic IMRT Planning via Static Field Fluence Prediction (AIP-SFFP), for automated prostate IMRT planning with real-time planning efficiency. METHODS: AIP-SFFP generates a prostate IMRT plan through predictions of fluence maps using the patient anatomy. No inverse planning is required. AIP-SFFP centralizes a custom-build deep learning neuro network for fluence map prediction. Predictions are imported to a commercial treatment planning system for dose calculation and plan generation. AIP-SFFP was demonstrated for prostate IMRT simultaneously-integrated-boost (SIB) planning (58.8Gy/70Gy to PTV58.8Gy/PTV70Gyin 25 fx). Training data was generated from 106 patients using a knowledge-based planning (KBP) plan generator. Two types of 2D projection images were designed to represent structures' sizes and locations, and a total of 8 projections were utilized to describe targets and organs-at-risk. Projections at 9 template beam angles were stacked as inputs for AI training. 14 patients were used as independent tests. The generated test plans were compared with the plans from the KBP training plan generator and clinic practice. RESULTS: After normalization (PTV70GyV70Gy=95%), all 14 AI plans met institutional criteria. The coverage of PTV58.8Gyin AI plans was comparable to KBP and Clinic plans without statistical significance. BODY D1ccand rectum D0.1ccof AI plans were slightly higher (<1Gy) compared to KBP and Clinic plans; in contrast, bladder D1ccand other rectum and bladder low dose in AI plans were slightly improved without clinical relevance. The overall isodose distribution in AI plans was comparable with KBP plans and clinical plans. AIP-SFFP generated each test plan within 20 seconds including prediction and dose calculation. CONCLUSIONS: AIP-SFFP was successfully developed for prostate IMRT planning. AIP-SFFP demonstrated good overall plan qualities and real-time efficiency. Holding great promises, AIP-SFFP will be investigated for immediate clinical application.", "publication_location": "Phys Med Biol", "link": "http://dx.doi.org/10.1088/1361-6560/aba5eb", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Student Beats the Teacher: Deep Learning Using a 3D Convolutional Neural Network (CNN) for Augmentation of CBCT Reconstructed From Under-Sampled Projections", "authors": "Jiang, Z; Yin, F; Ren, L", "published_date": "June 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Medical Physics", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000471277701122&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic Deep Learning OCT Analysis Algorithm Reliably Reproduces Expert-Evaluated Outcome of a Randomized Clinical Trial for Macular Telangiectasia Type 2 Treatment", "authors": "Loo, J; Clemons, TE; Chew, EY; Friedlander, M; Jaffe, GJ; Farsiu, S", "published_date": "July 1, 2019", "doi": " ", "abstract": " ", "publication_location": "Investigative Ophthalmology & Visual Science", "link": "http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcApp=PARTNER_APP&SrcAuth=LinksAMR&KeyUT=WOS:000488628103223&DestLinkType=FullRecord&DestApp=ALL_WOS&UsrCustomerID=47d3190e77e5a3a53558812f597b0b92", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic segmentation of nine retinal layer boundaries in OCT images of non-exudative AMD patients using deep learning and graph search.", "authors": "Fang, L; Cunefare, D; Wang, C; Guymer, RH; Li, S; Farsiu, S", "published_date": "May 2017", "doi": "10.1364/boe.8.002732", "abstract": "We present a novel framework combining convolutional neural networks (CNN) and graph search methods (termed as CNN-GS) for the automatic segmentation of nine layer boundaries on retinal optical coherence tomography (OCT) images. CNN-GS first utilizes a CNN to extract features of specific retinal layer boundaries and train a corresponding classifier to delineate a pilot estimate of the eight layers. Next, a graph search method uses the probability maps created from the CNN to find the final boundaries. We validated our proposed method on 60 volumes (2915 B-scans) from 20 human eyes with non-exudative age-related macular degeneration (AMD), which attested to effectiveness of our proposed technique.", "publication_location": "Biomedical Optics Express", "link": "http://dx.doi.org/10.1364/boe.8.002732", "citations": "179", "readership": "176", "tweets": "1", "news_mentions": " "},
{"title": "ASIC Implementation of Time-Domain Digital Backpropagation with Deep-Learned Chromatic Dispersion Filters", "authors": "Fougstedt, C; Häger, C; Svensson, L; Pfister, HD; Larsson-Edefors, P", "published_date": "November 14, 2018", "doi": "10.1109/ECOC.2018.8535430", "abstract": "© 2018 IEEE. We consider time-domain digital backpropagation with chromatic dispersion filters jointly optimized and quantized using machine-learning techniques. Compared to the baseline implementations, we show improved BER performance and >40% power dissipation reductions in 28-nm CMOS.", "publication_location": "European Conference on Optical Communication, Ecoc", "link": "http://dx.doi.org/10.1109/ECOC.2018.8535430", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep multiscale model learning", "authors": "Wang, Y; Cheung, SW; Chung, ET; Efendiev, Y; Wang, M", "published_date": "April 1, 2020", "doi": "10.1016/j.jcp.2019.109071", "abstract": "© 2019 The objective of this paper is to design novel multi-layer neural networks for multiscale simulations of flows taking into account the observed fine data and physical modeling concepts. Our approaches use deep learning techniques combined with local multiscale model reduction methodologies to predict flow dynamics. Using reduced-order model concepts is important for constructing robust deep learning architectures since the reduced-order models provide fewer degrees of freedom. We consider flow dynamics in porous media as multi-layer networks in this work. More precisely, the solution (e.g., pressures and saturation) at the time instant n+1 depends on the solution at the time instant n and input parameters, such as permeability fields, forcing terms, and initial conditions. One can regard the solution as a multi-layer network, where each layer, in general, is a nonlinear forward map and the number of layers relates to the internal time steps. We will rely on rigorous model reduction concepts to define unknowns and connections between layers. It is critical to use reduced-order models for this purpose, which will identify the regions of influence and the appropriate number of variables. Furthermore, due to the lack of available observed fine data, the reduced-order model can provide us sufficient inexpensive data as needed. The designed deep neural network will be trained using both coarse simulation data which is obtained from the reduced-order model and observed fine data. We will present the main ingredients of our approach and numerical examples. Numerical results show that using deep learning with data generated from multiscale models as well as available observed fine data, we can obtain an improved forward map which can better approximate the fine scale model.", "publication_location": "Journal of Computational Physics", "link": "http://dx.doi.org/10.1016/j.jcp.2019.109071", "citations": "4", "readership": "45", "tweets": "1", "news_mentions": " "},
{"title": "Geometry-aware deep transform", "authors": "Huang, J; Qiu, Q; Calderbank, R; Sapiro, G", "published_date": "February 17, 2015", "doi": "10.1109/ICCV.2015.471", "abstract": "© 2015 IEEE. Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled training samples to learn a huge number of parameters in a network, therefore, understanding the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is the case for many applications. In this paper, we propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria. We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for both synthetic and real-world data. We further support the proposed framework with a formal (K)-robustness analysis.", "publication_location": "Proceedings of the Ieee International Conference on Computer Vision", "link": "http://dx.doi.org/10.1109/ICCV.2015.471", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Comparative effectiveness of convolutional neural network (CNN) and recurrent neural network (RNN) architectures for radiology text report classification.", "authors": "Banerjee, I; Ling, Y; Chen, MC; Hasan, SA; Langlotz, CP; Moradzadeh, N; Chapman, B; Amrhein, T; Mong, D; Rubin, DL; Farri, O; Lungren, MP", "published_date": "June 2019", "doi": "10.1016/j.artmed.2018.11.004", "abstract": "This paper explores cutting-edge deep learning methods for information extraction from medical imaging free text reports at a multi-institutional scale and compares them to the state-of-the-art domain-specific rule-based system - PEFinder and traditional machine learning methods - SVM and Adaboost. We proposed two distinct deep learning models - (i) CNN Word - Glove, and (ii) Domain phrase attention-based hierarchical recurrent neural network (DPA-HNN), for synthesizing information on pulmonary emboli (PE) from over 7370 clinical thoracic computed tomography (CT) free-text radiology reports collected from four major healthcare centers. Our proposed DPA-HNN model encodes domain-dependent phrases into an attention mechanism and represents a radiology report through a hierarchical RNN structure composed of word-level, sentence-level and document-level representations. Experimental results suggest that the performance of the deep learning models that are trained on a single institutional dataset, are better than rule-based PEFinder on our multi-institutional test sets. The best F1 score for the presence of PE in an adult patient population was 0.99 (DPA-HNN) and for a pediatrics population was 0.99 (HNN) which shows that the deep learning models being trained on adult data, demonstrated generalizability to pediatrics population with comparable accuracy. Our work suggests feasibility of broader usage of neural network models in automated classification of multi-institutional imaging text reports for a variety of applications including evaluation of imaging utilization, imaging yield, clinical decision support tools, and as part of automated classification of large corpus for medical imaging deep learning work.", "publication_location": "Artif Intell Med", "link": "http://dx.doi.org/10.1016/j.artmed.2018.11.004", "citations": "25", "readership": "75", "tweets": "15", "news_mentions": " "},
{"title": "Artificial intelligence for teleophthalmology-based diabetic retinopathy screening in a national programme: an economic analysis modelling study", "authors": "Xie, Y; Nguyen, QD; Hamzah, H; Lim, G; Bellemo, V; Gunasekeran, DV; Yip, MYT; Qi Lee, X; Hsu, W; Li Lee, M; Tan, CS; Tym Wong, H; Lamoureux, EL; Tan, GSW; Wong, TY; Finkelstein, EA; Ting, DSW", "published_date": "May 1, 2020", "doi": "10.1016/S2589-7500(20)30060-1", "abstract": "© 2020 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license Background: Deep learning is a novel machine learning technique that has been shown to be as effective as human graders in detecting diabetic retinopathy from fundus photographs. We used a cost-minimisation analysis to evaluate the potential savings of two deep learning approaches as compared with the current human assessment: a semi-automated deep learning model as a triage filter before secondary human assessment; and a fully automated deep learning model without human assessment. Methods: In this economic analysis modelling study, using 39 006 consecutive patients with diabetes in a national diabetic retinopathy screening programme in Singapore in 2015, we used a decision tree model and TreeAge Pro to compare the actual cost of screening this cohort with human graders against the simulated cost for semi-automated and fully automated screening models. Model parameters included diabetic retinopathy prevalence rates, diabetic retinopathy screening costs under each screening model, cost of medical consultation, and diagnostic performance (ie, sensitivity and specificity). The primary outcome was total cost for each screening model. Deterministic sensitivity analyses were done to gauge the sensitivity of the results to key model assumptions. Findings: From the health system perspective, the semi-automated screening model was the least expensive of the three models, at US$62 per patient per year. The fully automated model was $66 per patient per year, and the human assessment model was $77 per patient per year. The savings to the Singapore health system associated with switching to the semi-automated model are estimated to be $489 000, which is roughly 20% of the current annual screening cost. By 2050, Singapore is projected to have 1 million people with diabetes; at this time, the estimated annual savings would be $15 million. Interpretation: This study provides a strong economic rationale for using deep learning systems as an assistive tool to screen for diabetic retinopathy. Funding: Ministry of Health, Singapore.", "publication_location": "The Lancet Digital Health", "link": "http://dx.doi.org/10.1016/S2589-7500(20)30060-1", "citations": " ", "readership": " ", "tweets": "22", "news_mentions": "3"},
{"title": "Not afraid of the dark: NIR-VIS face recognition via cross-spectral hallucination and low-rank embedding", "authors": "Lezama, J; Qiu, Q; Sapiro, G", "published_date": "November 6, 2017", "doi": "10.1109/CVPR.2017.720", "abstract": "© 2017 IEEE. Surveillance cameras today often capture NIR (near infrared) images in low-light environments. However, most face datasets accessible for training and verification are only collected in the VIS (visible light) spectrum. It remains a challenging problem to match NIR to VIS face images due to the different light spectrum. Recently, breakthroughs have been made for VIS face recognition by applying deep learning on a huge amount of labeled VIS face samples. The same deep learning approach cannot be simply applied to NIR face recognition for two main reasons: First, much limited NIR face images are available for training compared to the VIS spectrum. Second, face galleries to be matched are mostly available only in the VIS spectrum. In this paper, we propose an approach to extend the deep learning breakthrough for VIS face recognition to the NIR spectrum, without retraining the underlying deep models that see only VIS faces. Our approach consists of two core components, cross-spectral hallucination and low-rank embedding, to optimize respectively input and output of a VIS deep model for cross-spectral face recognition. Cross-spectral hallucination produces VIS faces from NIR images through a deep learning approach. Low-rank embedding restores a low-rank structure for faces deep features across both NIR and VIS spectrum. We observe that it is often equally effective to perform hallucination to input NIR images or low-rank embedding to output deep features for a VIS deep model for cross-spectral recognition. When hallucination and low-rank embedding are deployed together, we observe significant further improvement; we obtain state-of-the-art accuracy on the CASIA NIR-VIS v2.0 benchmark, without the need at all to re-train the recognition system.", "publication_location": "Proceedings   30th Ieee Conference on Computer Vision and Pattern Recognition, Cvpr 2017", "link": "http://dx.doi.org/10.1109/CVPR.2017.720", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep longitudinal transfer learning-based automatic segmentation of photoreceptor ellipsoid zone defects on optical coherence tomography images of macular telangiectasia type 2.", "authors": "Loo, J; Fang, L; Cunefare, D; Jaffe, GJ; Farsiu, S", "published_date": "June 1, 2018", "doi": "10.1364/BOE.9.002681", "abstract": "Photoreceptor ellipsoid zone (EZ) defects visible on optical coherence tomography (OCT) are important imaging biomarkers for the onset and progression of macular diseases. As such, accurate quantification of EZ defects is paramount to monitor disease progression and treatment efficacy over time. We developed and trained a novel deep learning-based method called Deep OCT Atrophy Detection (DOCTAD) to automatically segment EZ defect areas by classifying 3-dimensional A-scan clusters as normal or defective. Furthermore, we introduce a longitudinal transfer learning paradigm in which the algorithm learns from segmentation errors on images obtained at one time point to segment subsequent images with higher accuracy. We evaluated the performance of this method on 134 eyes of 67 subjects enrolled in a clinical trial of a novel macular telangiectasia type 2 (MacTel2) therapeutic agent. Our method compared favorably to other deep learning-based and non-deep learning-based methods in matching expert manual segmentations. To the best of our knowledge, this is the first automatic segmentation method developed for EZ defects on OCT images of MacTel2.", "publication_location": "Biomedical Optics Express", "link": "http://dx.doi.org/10.1364/BOE.9.002681", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Machine learning for early detection of sepsis: an internal and temporal validation study", "authors": " ", "published_date": " ", "doi": "10.1093/jamiaopen/ooaa006", "abstract": "Abstract                                 Objective                  Determine if deep learning detects sepsis earlier and more accurately than other models. To evaluate model performance using implementation-oriented metrics that simulate clinical practice.                                                Materials and Methods                  We trained internally and temporally validated a deep learning model (multi-output Gaussian process and recurrent neural network [MGP–RNN]) to detect sepsis using encounters from adult hospitalized patients at a large tertiary academic center. Sepsis was defined as the presence of 2 or more systemic inflammatory response syndrome (SIRS) criteria, a blood culture order, and at least one element of end-organ failure. The training dataset included demographics, comorbidities, vital signs, medication administrations, and labs from October 1, 2014 to December 1, 2015, while the temporal validation dataset was from March 1, 2018 to August 31, 2018. Comparisons were made to 3 machine learning methods, random forest (RF), Cox regression (CR), and penalized logistic regression (PLR), and 3 clinical scores used to detect sepsis, SIRS, quick Sequential Organ Failure Assessment (qSOFA), and National Early Warning Score (NEWS). Traditional discrimination statistics such as the C-statistic as well as metrics aligned with operational implementation were assessed.                                                Results                  The training set and internal validation included 42 979 encounters, while the temporal validation set included 39 786 encounters. The C-statistic for predicting sepsis within 4 h of onset was 0.88 for the MGP–RNN compared to 0.836 for RF, 0.849 for CR, 0.822 for PLR, 0.756 for SIRS, 0.619 for NEWS, and 0.481 for qSOFA. MGP–RNN detected sepsis a median of 5 h in advance. Temporal validation assessment continued to show the MGP–RNN outperform all 7 clinical risk score and machine learning comparisons.                                                Conclusions                  We developed and validated a novel deep learning model to detect sepsis. Using our data elements and feature set, our modeling approach outperformed other machine learning methods and clinical scores.", "publication_location": "Jamia Open", "link": "http://dx.doi.org/10.1093/jamiaopen/ooaa006", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "DeepConf: Automating data center network topologies management with machine learning", "authors": " ", "published_date": "August 7, 2018", "doi": "10.1145/3229543.3229554", "abstract": "© 2018 Association for Computing Machinery. In recent years, many techniques have been developed to improve the performance and efficiency of data center networks. While these techniques provide high accuracy, they are often designed using heuristics that leverage domain-specific properties of the workload or hardware. In this vision paper, we argue that many data center networking techniques, e.g., routing, topology augmentation, energy savings, with diverse goals share design and architectural similarities. We present a framework for developing general intermediate representations of network topologies using deep learning that is amenable to solving a large class of data center problems. We develop a framework, DeepConf, that simplifies the process of configuring and training deep learning agents by using our intermediate representation to learn different tasks. To illustrate the strength of our approach, we implemented and evaluated a DeepConf-agent that tackles the data center topology augmentation problem. Our initial results are promising - DeepConf performs comparably to the optimal solution.", "publication_location": "Netai 2018   Proceedings of the 2018 Workshop on Network Meets Ai and Ml, Part of Sigcomm 2018", "link": "http://dx.doi.org/10.1145/3229543.3229554", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Convolutional neural networks on surfaces via seamless toric covers", "authors": "Maron, H; Galun, M; Aigerman, N; Trope, M; Dym, N; Yumer, E; Kim, VG; Lipman, Y", "published_date": "January 1, 2017", "doi": "10.1145/3072959.3073616", "abstract": "© 2017 ACM. The recent success of convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to achieve similar success for geometric tasks. One of the main challenges in applying CNNs to surfaces is defining a natural convolution operator on surfaces. In this paper we present a method for applying deep learning to sphere-type shapes using a global seamless parameterization to a planar flat-torus, for which the convolution operator is well defined. As a result, the standard deep learning framework can be readily applied for learning semantic, highlevel properties of the shape. An indication of our success in bridging the gap between images and surfaces is the fact that our algorithm succeeds in learning semantic information from an input of raw low-dimensional feature vectors. We demonstrate the usefulness of our approach by presenting two applications: human body segmentation, and automatic landmark detection on anatomical surfaces. We show that our algorithm compares favorably with competing geometric deep-learning algorithms for segmentation tasks, and is able to produce meaningful correspondences on anatomical surfaces where hand-crafted features are bound to fail.", "publication_location": "Acm Transactions on Graphics", "link": "http://dx.doi.org/10.1145/3072959.3073616", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Convolutional regularization methods for 4D, x-ray CT reconstruction", "authors": "Clark, DP; Badea, CT", "published_date": "January 1, 2019", "doi": "10.1117/12.2512816", "abstract": "© SPIE. Downloading of the abstract is permitted for personal use only. Deep learning methods have shown great promise in tackling challenging medical imaging tasks. Within the field of x-ray CT, deep learning for image denoising is of interest because of the fundamental link between ionizing radiation dose and diagnostic image quality, the limited availability of clinical projection data, and the computational expense of iterative reconstruction methods. Here, we work with 3D, temporal CT data (4D, cardiac CT), where redundancies in spatial sampling necessitate careful control of imaging dose. Specifically, using custom extensions to the Tensorflow and Keras machine learning packages, we construct and train a 4D, convolutional neural network (CNN) to denoise helical, cardiac CT data acquired in a mouse model of atherosclerosis. With the objective of accelerating iterative reconstruction, we train the CNN to map undersampled algebraic reconstructions of the 4D data to fully-sampled and regularized iterative reconstructions under mean-squared-error, perceptual loss, and low rank cost terms. Using phantom data for quantitative validation, we verify that the CNN robustly denoises static potions of the image without compromising temporal fidelity and that the CNN performs similarly to regularized, iterative reconstruction with the split Bregman method (CNN temporal RMSE: 142 HU; iterative temporal RMSE: 136 HU). Using in vivo validation and testing data excluded from CNN training, we verify that the CNN generalizes well, approximately reproducing the noise power spectrum of the iteratively reconstructed data (noise std. in water vial near heart, CNN: 62-73 HU, depending on cardiac phase; iterative: 94-100 HU), without degradation of spatial resolution (axial MTF, 10% cutoff, CNN: 2.69 lp/mm; iterative: 2.63 lp/mm). Overall, the results presented in this work represent a positive step toward realizing the promises of deep learning methods in medical imaging.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2512816", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Glaucoma detection based on deep convolutional neural network.", "authors": "Xiangyu Chen, ; Yanwu Xu, ; Damon Wing Kee Wong, ; Tien Yin Wong, ; Jiang Liu,", "published_date": "August 2015", "doi": "10.1109/EMBC.2015.7318462", "abstract": "Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection.", "publication_location": "Conference Proceedings : ... Annual International Conference of the Ieee Engineering in Medicine and Biology Society. Ieee Engineering in Medicine and Biology Society. Annual Conference", "link": "http://dx.doi.org/10.1109/EMBC.2015.7318462", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Generative adversarial network-based image completion to identify abnormal locations in digital breast tomosynthesis images", "authors": "Swiecicki, A; Buda, M; Saha, A; Li, N; Ghate, SV; Walsh, R; Mazurowski, MA", "published_date": "January 1, 2020", "doi": "10.1117/12.2551379", "abstract": "© 2020 SPIE. Deep learning has achieved great success in image analysis and decision making in radiology. However, a large amount of annotated imaging data is needed to construct well-performing deep learning models. A particular challenge in the context of breast cancer is the number of available cases that contain cancer, given the very low prevalence of the disease in the screening population. The question arises whether normal cases, which in the context of breast cancer screening are available in abundance, can be used to train a deep learning model that identifies locations that are abnormal. In this study, we propose to achieve this goal through the generative adversarial network (GAN)-based image completion. Our hypothesis is that if a generative network has a difficulty to correctly complete a part of an image at a certain location, then such a location is likely to represent an abnormality. We test this hypothesis using a dataset of 4348 patients with digital breast tomosynthesis (DBT) imaging from our institution. We trained our model on normal only images, to be able to fill in parts of images that were artificially removed. Then, using an independent test set, at different locations in the images, we measured how difficult it was for the network to reconstruct an artificially removed patch of the image. The difficulty was measured by mean squared error (MSE) between the original removed patch and the reconstructed patch. On average, the MSE was 2.11 times higher (with standard deviation equal to 1.01) at the locations containing expert-annotated cancerous lesions than that at the locations outside those abnormal locations. Our generative approach demonstrates a great potential for using this model to aid breast cancer detection.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2551379", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Human-machine partnership with artificial intelligence for chest radiograph diagnosis.", "authors": "Patel, BN; Rosenberg, L; Willcox, G; Baltaxe, D; Lyons, M; Irvin, J; Rajpurkar, P; Amrhein, T; Gupta, R; Halabi, S; Langlotz, C; Lo, E; Mammarappallil, J; Mariano, AJ; Riley, G; Seekins, J; Shen, L; Zucker, E; Lungren, M", "published_date": "2019", "doi": "10.1038/s41746-019-0189-7", "abstract": "Human-in-the-loop (HITL) AI may enable an ideal symbiosis of human experts and AI models, harnessing the advantages of both while at the same time overcoming their respective limitations. The purpose of this study was to investigate a novel collective intelligence technology designed to amplify the diagnostic accuracy of networked human groups by forming real-time systems modeled on biological swarms. Using small groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs and compared against human experts alone, as well as two state-of-the-art deep learning AI models. Our work demonstrates that both the swarm-based technology and deep-learning technology achieved superior diagnostic accuracy than the human experts alone. Our work further demonstrates that when used in combination, the swarm-based technology and deep-learning technology outperformed either method alone. The superior diagnostic accuracy of the combined HITL AI solution compared to radiologists and AI alone has broad implications for the surging clinical AI deployment and implementation strategies in future practice.", "publication_location": "Npj Digital Medicine", "link": "http://dx.doi.org/10.1038/s41746-019-0189-7", "citations": "3", "readership": "55", "tweets": "201", "news_mentions": "8"},
{"title": "Deep mining heterogeneous networks of biomedical linked data to predict novel drug-target associations.", "authors": "Zong, N; Kim, H; Ngo, V; Harismendy, O", "published_date": "August 2017", "doi": "10.1093/bioinformatics/btx160", "abstract": "Motivation:A heterogeneous network topology possessing abundant interactions between biomedical entities has yet to be utilized in similarity-based methods for predicting drug-target associations based on the array of varying features of drugs and their targets. Deep learning reveals features of vertices of a large network that can be adapted in accommodating the similarity-based solutions to provide a flexible method of drug-target prediction. Results:We propose a similarity-based drug-target prediction method that enhances existing association discovery methods by using a topology-based similarity measure. DeepWalk, a deep learning method, is adopted in this study to calculate the similarities within Linked Tripartite Network (LTN), a heterogeneous network generated from biomedical linked datasets. This proposed method shows promising results for drug-target association prediction: 98.96% AUC ROC score with a 10-fold cross-validation and 99.25% AUC ROC score with a Monte Carlo cross-validation with LTN. By utilizing DeepWalk, we demonstrate that: (i) this method outperforms other existing topology-based similarity computation methods, (ii) the performance is better for tripartite than with bipartite networks and (iii) the measure of similarity using network topology outperforms the ones derived from chemical structure (drugs) or genomic sequence (targets). Our proposed methodology proves to be capable of providing a promising solution for drug-target prediction based on topological similarity with a heterogeneous network, and may be readily re-purposed and adapted in the existing of similarity-based methodologies. Availability and Implementation:The proposed method has been developed in JAVA and it is available, along with the data at the following URL: https://github.com/zongnansu1982/drug-target-prediction . Contact:nazong@ucsd.edu. Supplementary information:Supplementary data are available at Bioinformatics online.", "publication_location": "Bioinformatics (Oxford, England)", "link": "http://dx.doi.org/10.1093/bioinformatics/btx160", "citations": "58", "readership": "76", "tweets": "2", "news_mentions": " "},
{"title": "Breast cancer molecular subtype classification using deep features: Preliminary results", "authors": "Zhu, Z; Albadawy, E; Saha, A; Zhang, J; Harowicz, MR; Mazurowski, MA", "published_date": "January 1, 2018", "doi": "10.1117/12.2295471", "abstract": "© 2018 SPIE. Radiogenomics is a field of investigation that attempts to examine the relationship between imaging characteris-tics of cancerous lesions and their genomic composition. This could offer a noninvasive alternative to establishing genomic characteristics of tumors and aid cancer treatment planning. While deep learning has shown its supe-riority in many detection and classification tasks, breast cancer radiogenomic data suffers from a very limited number of training examples, which renders the training of the neural network for this problem directly and with no pretraining a very difficult task. In this study, we investigated an alternative deep learning approach referred to as deep features or off-the-shelf network approach to classify breast cancer molecular subtypes using breast dynamic contrast enhanced MRIs. We used the feature maps of different convolution layers and fully connected layers as features and trained support vector machines using these features for prediction. For the feature maps that have multiple layers, max-pooling was performed along each channel. We focused on distinguishing the Luminal A subtype from other subtypes. To evaluate the models, 10 fold cross-validation was performed and the final AUC was obtained by averaging the performance of all the folds. The highest average AUC obtained was 0.64 (0.95 CI: 0.57-0.71), using the feature maps of the last fully connected layer. This indicates the promise of using this approach to predict the breast cancer molecular subtypes. Since the best performance appears in the last fully connected layer, it also implies that breast cancer molecular subtypes may relate to high level image features.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2295471", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Lightweight Learning-based Automatic Segmentation of Subretinal Blebs on Microscope-Integrated Optical Coherence Tomography Images.", "authors": "Song, Z; Xu, L; Wang, J; Rasti, R; Sastry, A; Li, JD; Raynor, W; Izatt, JA; Toth, CA; Vajzovic, L; Deng, B; Farsiu, S", "published_date": "July 21, 2020", "doi": "10.1016/j.ajo.2020.07.020", "abstract": "PURPOSE:Subretinal injections of therapeutics are commonly used to treat ocular diseases. Accurate dosing of therapeutics at target locations is crucial but difficult to achieve using subretinal injections due to leakage, and there is no method available to measure the volume of therapeutics successfully administered to the subretinal location during surgery. Here we introduce the first automatic method for quantifying the volume of subretinal blebs, using porcine eyes injected with Ringer's lactate solution as samples. DESIGN:Experimental study. METHODS:Microscope-integrated optical coherence tomography was utilized to obtain 3D visualization of subretinal blebs in porcine eyes at Duke Eye Center. Two different injection phases were imaged and analyzed in 15 eyes (30 volumes), selected from a total of 37 eyes. The inclusion/exclusion criteria were set independently from the algorithm-development and testing team. A novel lightweight, deep learning-based algorithm was designed to segment subretinal blebs boundaries. A cross-validation method was used to avoid selection bias. An ensemble-classifier strategy was applied to generate final results for the test dataset. RESULTS:The algorithm performs significantly better than four other state-of-the-art deep learning-based segmentation methods, achieving an F1 score of 93.86 ± 1.17% and 96.90 ± 0.59% on the independent test data for entry and full blebs, respectively. CONCLUSION:The proposed algorithm accurately segmented the volumetric boundaries of Ringer's lactate solution delivered into the subretinal space of porcine eyes with robust performance and real-time speed. This is the first step for future applications in computer-guided delivery of therapeutics into the subretinal space in human subjects.", "publication_location": "American Journal of Ophthalmology", "link": "http://dx.doi.org/10.1016/j.ajo.2020.07.020", "citations": " ", "readership": " ", "tweets": "2", "news_mentions": " "},
{"title": "Automatic phantom test pattern classification through transfer learning with deep neural networks", "authors": "Fricks, RB; Solomon, J; Samei, E", "published_date": "January 1, 2020", "doi": "10.1117/12.2549366", "abstract": "© 2020 SPIE. Imaging phantoms are test patterns used to measure image quality in computer tomography (CT) systems. A new phantom platform (Mercury Phantom, Gammex) provides test patterns for estimating the task transfer function (TTF) or noise power spectrum (NPF) and simulates different patient sizes. Determining which image slices are suitable for analysis currently requires manual annotation of these patterns by an expert, as subtle defects may make an image unsuitable for measurement. We propose a method of automatically classifying these test patterns in a series of phantom images using deep learning techniques. By adapting a convolutional neural network based on the VGG19 architecture with weights trained on ImageNet, we use transfer learning to produce a classifier for this domain. The classifier is trained and evaluated with over 3,500 phantom images acquired at a university medical center. Input channels for color images are successfully adapted to convey contextual information for phantom images. A series of ablation studies are employed to verify design aspects of the classifier and evaluate its performance under varying training conditions. Our solution makes extensive use of image augmentation to produce a classifier that accurately classifies typical phantom images with 98% accuracy, while maintaining as much as 86% accuracy when the phantom is improperly imaged.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2549366", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "High-Order stochastic gradient thermostats for Bayesian learning of deep models", "authors": "Li, C; Chen, C; Fan, K; Carin, L", "published_date": "January 1, 2016", "doi": " ", "abstract": "© Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Learning in deep models using Bayesian methods has generated significant attention recently. This is largely because of the feasibility of modern Bayesian methods to yield scalable learning and inference, while maintaining a measure of uncertainty in the model parameters. Stochastic gradient MCMC algorithms (SG-MCMC) are a family of diffusion-based sampling methods for large-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradient thermostats (mSGNHT) augment each parameter of interest, with a momentum and a thermostat variable to maintain stationary distributions as target posterior distributions. As the number of variables in a continuous-time diffusion increases, its numerical approximation error becomes a practical bottleneck, so better use of a numerical integrator is desirable. To this end, we propose use of an efficient symmetric splitting integrator in mSGNHT, instead of the traditional Euler integrator. We demonstrate that the proposed scheme is more accurate, robust, and converges faster. These properties are demonstrated to be desirable in Bayesian deep learning. Extensive experiments on two canonical models and their deep extensions demonstrate that the proposed scheme improves general Bayesian posterior sampling, particularly for deep models.", "publication_location": "30th Aaai Conference on Artificial Intelligence, Aaai 2016", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Spectral learning on matrices and tensors", "authors": "Janzamin, M; Ge, R; Kossaifi, J; Anandkumar, A", "published_date": "January 1, 2019", "doi": "10.1561/2200000057", "abstract": "© Majid Janzamin, Rong Ge, Jean Kossaifi and Anima Anandkumar (2019 Spectral methods have been the mainstay in several domains such as machine learning, applied mathematics and scientific computing. They involve finding a certain kind of spectral decomposition to obtain basis functions that can capture important structures or directions for the problem at hand. The most common spectral method is the principal component analysis (PCA). It utilizes the principal components or the top eigenvectors of the data covariance matrix to carry out dimensionality reduction as one of its applications. This data pre-processing step is often effective in separating signal from noise. PCA and other spectral techniques applied to matrices have several limitations. By limiting to only pairwise moments, they are effectively making a Gaussian approximation on the underlying data. Hence, they fail on data with hidden variables which lead to non-Gaussianity. However, in almost any data set, there are latent effects that cannot be directly observed, e.g., topics in a document corpus, or underlying causes of a disease. By extending the spectral decomposition methods to higher order moments, we demonstrate the ability to learn a wide range of latent variable models efficiently. Higher-order moments can be represented by tensors, and intuitively, they can encode more information than just pairwise moment matrices. More crucially, tensor decomposition can pick up latent effects that are missed by matrix methods. For instance, tensor decomposition can uniquely identify non-orthogonal components. Exploiting these aspects turns out to be fruitful for provable unsupervised learning of a wide range of latent variable models. We also outline the computational techniques to design efficient tensor decomposition methods. They are embarrassingly parallel and thus scalable to large data sets. Whilst there exist many optimized linear algebra software packages, efficient tensor algebra packages are also beginning to be developed. We introduce Tensorly, which has a simple python interface for expressing tensor operations. It has a flexible back-end system supporting NumPy, PyTorch, TensorFlow and MXNet amongst others. This allows it to carry out multi-GPU and CPU operations, and can also be seamlessly integrated with deep-learning functionalities.", "publication_location": "Foundations and Trends in Machine Learning", "link": "http://dx.doi.org/10.1561/2200000057", "citations": " ", "readership": "39", "tweets": "73", "news_mentions": " "},
{"title": "AI for medical imaging goes deep.", "authors": "Ting, DSW; Liu, Y; Burlina, P; Xu, X; Bressler, NM; Wong, TY", "published_date": "May 2018", "doi": "10.1038/s41591-018-0029-3", "abstract": " ", "publication_location": "Nat Med", "link": "http://dx.doi.org/10.1038/s41591-018-0029-3", "citations": "42", "readership": "130", "tweets": "77", "news_mentions": "2"},
{"title": "Fast spectral x-ray CT reconstruction with data-adaptive, convolutional regularization", "authors": "Clark, DP; Badea, CT", "published_date": "January 1, 2020", "doi": "10.1117/12.2549615", "abstract": "© 2020 SPIE Advancements in deep learning and GPU computing have exponentially driven the application of neural networks to classic medical imaging problems: denoising, segmentation, artifact removal, etc. Deep learning solutions are particularly attractive for processing multi-channel, volumetric image data, where processing and reconstruction methods are often computationally expensive. Convolutional neural networks (CNNs) are commonly applied to multi-channel image data by matching the number of network input channels to the number of data channels, learning explicit relationships between channels. This provides a high degree of specificity to a particular problem, but may fail to generalize to a broader class of closely related problems. We propose a solution to this generalization problem in the context of spectral x-ray CT, where the scanning kVps (energy bins) and contrast are often variable. Specifically, we propose a novel CNN architecture which handles variable numbers of input channels, variable noise levels between channels, and variable modes of spectral contrast. We demonstrate our architecture in the application of preclinical, photon-counting, micro-CT, effectively replacing 1-2 hours of iterative reconstruction, with <10 minutes of analytical reconstruction and CNN regularization. Experimental validation shows the effectiveness of our approach when applied to both in vivo photon-counting validation data (4 energy thresholds) and to simulated, dual-energy CT data virtually acquired with an energy integrating detector. In both cases, the results output by the CNN provide greater spectral accuracy than analytical reconstruction alone, but suffer from some degradation of spatial resolution. We conclude by proposing several extensions of our work to better preserve spatial resolution.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2549615", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adverse drug reaction discovery from electronic health records with deep neural networks", "authors": "Zhang, W; Kuang, Z; Peissig, P; Page, D", "published_date": "February 4, 2020", "doi": "10.1145/3368555.3384459", "abstract": "© 2020 ACM. Adverse drug reactions (ADRs) are detrimental and unexpected clinical incidents caused by drug intake. The increasing availability of massive quantities of longitudinal event data such as electronic health records (EHRs) has redefined ADR discovery as a big data analytics problem, where data-hungry deep neural networks are especially suitable because of the abundance of the data. To this end, we introduce neural self-controlled case series (NSCCS), a deep learning framework for ADR discovery from EHRs. NSCCS rigorously follows a self-controlled case series design to adjust implicitly and efficiently for individual heterogeneity. In this way, NSCCS is robust to time-invariant confounding issues and thus more capable of identifying associations that reflect the underlying mechanism between various types of drugs and adverse conditions. We apply NSCCS to a large-scale real-world EHR dataset and empirically demonstrate its superior performance with comprehensive experiments on a benchmark ADR discovery task.", "publication_location": "Acm Chil 2020   Proceedings of the 2020 Acm Conference on Health, Inference, and Learning", "link": "http://dx.doi.org/10.1145/3368555.3384459", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep video deblurring for hand-held cameras", "authors": "Su, S; Delbracio, M; Wang, J; Sapiro, G; Heidrich, W; Wang, O", "published_date": "November 6, 2017", "doi": "10.1109/CVPR.2017.33", "abstract": "© 2017 IEEE. Motion blur from camera shake is a major problem in videos captured by hand-held devices. Unlike single-image deblurring, video-based approaches can take advantage of the abundant information that exists across neighboring frames. As a result the best performing methods rely on the alignment of nearby frames. However, aligning images is a computationally expensive and fragile procedure, and methods that aggregate information must therefore be able to identify which regions have been accurately aligned and which have not, a task that requires high level scene understanding. In this work, we introduce a deep learning solution to video deblurring, where a CNN is trained end-toend to learn how to accumulate information across frames. To train this network, we collected a dataset of real videos recorded with a high frame rate camera, which we use to generate synthetic motion blur for supervision. We show that the features learned from this dataset extend to deblurring motion blur that arises due to camera shake in a wide range of videos, and compare the quality of results to a number of other baselines.", "publication_location": "Proceedings   30th Ieee Conference on Computer Vision and Pattern Recognition, Cvpr 2017", "link": "http://dx.doi.org/10.1109/CVPR.2017.33", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Computer vision algorithms and hardware implementations: A survey", "authors": "Feng, X; Jiang, Y; Yang, X; Du, M; Li, X", "published_date": "November 1, 2019", "doi": "10.1016/j.vlsi.2019.07.005", "abstract": "© 2019 The Authors The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.", "publication_location": "Integration, the Vlsi Journal", "link": "http://dx.doi.org/10.1016/j.vlsi.2019.07.005", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Drones and convolutional neural networks facilitate automated and accurate cetacean species identification and photogrammetry", "authors": "Gray, PC; Bierlich, KC; Mantell, SA; Friedlaender, AS; Goldbogen, JA; Johnston, DW", "published_date": "September 1, 2019", "doi": "10.1111/2041-210X.13246", "abstract": "© 2019 The Authors. Methods in Ecology and Evolution © 2019 British Ecological Society The flourishing application of drones within marine science provides more opportunity to conduct photogrammetric studies on large and varied populations of many different species. While these new platforms are increasing the size and availability of imagery datasets, established photogrammetry methods require considerable manual input, allowing individual bias in techniques to influence measurements, increasing error and magnifying the time required to apply these techniques. Here, we introduce the next generation of photogrammetry methods utilizing a convolutional neural network to demonstrate the potential of a deep learning-based photogrammetry system for automatic species identification and measurement. We then present the same data analysed using conventional techniques to validate our automatic methods. Our results compare favorably across both techniques, correctly predicting whale species with 98% accuracy (57/58) for humpback whales, minke whales, and blue whales. Ninety percent of automated length measurements were within 5% of manual measurements, providing sufficient resolution to inform morphometric studies and establish size classes of whales automatically. The results of this study indicate that deep learning techniques applied to survey programs that collect large archives of imagery may help researchers and managers move quickly past analytical bottlenecks and provide more time for abundance estimation, distributional research, and ecological assessments.", "publication_location": "Methods in Ecology and Evolution", "link": "http://dx.doi.org/10.1111/2041-210X.13246", "citations": "6", "readership": "57", "tweets": "127", "news_mentions": " "},
{"title": "Gradient-Based Inverse Estimation for a Rainfall-Runoff Model", "authors": "Krapu, C; Borsuk, M; Kumar, M", "published_date": "August 1, 2019", "doi": "10.1029/2018WR024461", "abstract": "©2019. American Geophysical Union. All Rights Reserved. Recent advances in deep learning for neural networks with large numbers of parameters have been enabled by automatic differentiation, an algorithmic technique for calculating gradients of measures of model fit with respect to model parameters. Estimation of high-dimensional parameter sets is an important problem within the hydrological sciences. Here, we demonstrate the effectiveness of gradient-based estimation techniques for high-dimensional inverse estimation problems using a conceptual rainfall-runoff model. In particular, we compare the effectiveness of Hamiltonian Monte Carlo and automatic differentiation variational inference against two nongradient-dependent methods, random walk Metropolis and differential evolution Metropolis. We show that the former two techniques exhibit superior performance for inverse estimation of daily rainfall values and are much more computationally efficient on larger data sets in an experiment with synthetic data. We also present a case study evaluating the effectiveness of automatic differentiation variational inference for inverse estimation over 25 years of daily precipitation conditional on streamflow observations at three catchments and show that it is scalable to very high dimensional parameter spaces. The presented results highlight the power of combining hydrological process-based models with optimization techniques from deep learning for high-dimensional estimation problems.", "publication_location": "Water Resources Research", "link": "http://dx.doi.org/10.1029/2018WR024461", "citations": "1", "readership": "24", "tweets": "2", "news_mentions": " "},
{"title": "Comprehensive functional genomic resource and integrative model for the human brain.", "authors": "Wang, D; Liu, S; Warrell, J; Won, H; Shi, X; Navarro, FCP; Clarke, D; Gu, M; Emani, P; Yang, YT; Xu, M; Gandal, MJ; Lou, S; Zhang, J; Park, JJ; Yan, C; Rhie, SK; Manakongtreecheep, K; Zhou, H; Nathan, A; Peters, M; Mattei, E; Fitzgerald, D; Brunetti, T; Moore, J; Jiang, Y; Girdhar, K; Hoffman, GE; Kalayci, S; Gümüş, ZH; Crawford, GE; PsychENCODE Consortium, ; Roussos, P; Akbarian, S; Jaffe, AE; White, KP; Weng, Z; Sestan, N; Geschwind, DH; Knowles, JA; Gerstein, MB", "published_date": "December 14, 2018", "doi": "10.1126/science.aat8464", "abstract": "Despite progress in defining genetic risk for psychiatric disorders, their molecular mechanisms remain elusive. Addressing this, the PsychENCODE Consortium has generated a comprehensive online resource for the adult brain across 1866 individuals. The PsychENCODE resource contains ~79,000 brain-active enhancers, sets of Hi-C linkages, and topologically associating domains; single-cell expression profiles for many cell types; expression quantitative-trait loci (QTLs); and further QTLs associated with chromatin, splicing, and cell-type proportions. Integration shows that varying cell-type proportions largely account for the cross-population variation in expression (with >88% reconstruction accuracy). It also allows building of a gene regulatory network, linking genome-wide association study variants to genes (e.g., 321 for schizophrenia). We embed this network into an interpretable deep-learning model, which improves disease prediction by ~6-fold versus polygenic risk scores and identifies key genes and pathways in psychiatric disorders.", "publication_location": "Science", "link": "http://dx.doi.org/10.1126/science.aat8464", "citations": "183", "readership": "534", "tweets": "171", "news_mentions": "12"},
{"title": "A convolutional neural network to filter artifacts in spectroscopic MRI.", "authors": "Gurbani, SS; Schreibmann, E; Maudsley, AA; Cordova, JS; Soher, BJ; Poptani, H; Verma, G; Barker, PB; Shim, H; Cooper, LAD", "published_date": "November 2018", "doi": "10.1002/mrm.27166", "abstract": "PURPOSE: Proton MRSI is a noninvasive modality capable of generating volumetric maps of in vivo tissue metabolism without the need for ionizing radiation or injected contrast agent. Magnetic resonance spectroscopic imaging has been shown to be a viable imaging modality for studying several neuropathologies. However, a key hurdle in the routine clinical adoption of MRSI is the presence of spectral artifacts that can arise from a number of sources, possibly leading to false information. METHODS: A deep learning model was developed that was capable of identifying and filtering out poor quality spectra. The core of the model used a tiled convolutional neural network that analyzed frequency-domain spectra to detect artifacts. RESULTS: When compared with a panel of MRS experts, our convolutional neural network achieved high sensitivity and specificity with an area under the curve of 0.95. A visualization scheme was implemented to better understand how the convolutional neural network made its judgement on single-voxel or multivoxel MRSI, and the convolutional neural network was embedded into a pipeline capable of producing whole-brain spectroscopic MRI volumes in real time. CONCLUSION: The fully automated method for assessment of spectral quality provides a valuable tool to support clinical MRSI or spectroscopic MRI studies for use in fields such as adaptive radiation therapy planning.", "publication_location": "Magn Reson Med", "link": "http://dx.doi.org/10.1002/mrm.27166", "citations": "18", "readership": "58", "tweets": "5", "news_mentions": " "},
{"title": "MRI image harmonization using cycle-consistent generative adversarial network", "authors": "Modanwal, G; Vellal, A; Buda, M; Mazurowski, MA", "published_date": "January 1, 2020", "doi": "10.1117/12.2551301", "abstract": "© 2020 SPIE. Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a valuable modality for evaluating breast abnormalities found in mammography and performing early disease detection in high-risk patients. However, images produced by various MRI scanners (e.g., GE Healthcare & Siemens) differ in terms of intensity and other image characteristics such as noise distribution. This is a challenge both for the evaluation of images by radiologists and for the computational analysis of images using radiomics or deep learning. For example, an algorithm trained on a set of images acquired by one MRI scanner may perform poorly on a dataset produced by a different scanner. Therefore, there is an urgent need for image harmonization. Traditional image to image translation algorithms can be used to solve this problem, but they require paired data (i.e. the same object imaged using different scanners). In this study, we utilize a deep learning algorithm that uses unpaired data to solve this problem through a bi-directional translation between MRI images. The proposed method is based on a cycle-consistent adversarial network (CycleGAN) that uses two generator-discriminator pairs. The original CycleGAN struggles in preserving the structure (i.e. breast tissue characteristics and shape) during the translation. To overcome this, we modified the discriminator architecture and forced the penalization based on the structure at the scale of smaller patches. This allows the network to focus more on features pertaining to breast tissue. The results demonstrate that the transformed images are visually realistic, preserve the structure and harmonize intensity across images from different scanners.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2551301", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "MimickNet, Mimicking Clinical Image Post- Processing Under Black-Box Constraints.", "authors": "Huang, O; Long, W; Bottenus, N; Lerendegui, M; Trahey, GE; Farsiu, S; Palmeri, ML", "published_date": "June 2020", "doi": "10.1109/tmi.2020.2970867", "abstract": "Image post-processing is used in clinical-grade ultrasound scanners to improve image quality (e.g., reduce speckle noise and enhance contrast). These post-processing techniques vary across manufacturers and are generally kept proprietary, which presents a challenge for researchers looking to match current clinical-grade workflows. We introduce a deep learning framework, MimickNet, that transforms conventional delay-and-summed (DAS) beams into the approximate Dynamic Tissue Contrast Enhanced (DTCE™) post-processed images found on Siemens clinical-grade scanners. Training MimickNet only requires post-processed image samples from a scanner of interest without the need for explicit pairing to DAS data. This flexibility allows MimickNet to hypothetically approximate any manufacturer's post-processing without access to the pre-processed data. MimickNet post-processing achieves a 0.940 ± 0.018 structural similarity index measurement (SSIM) compared to clinical-grade post-processing on a 400 cine-loop test set, 0.937 ± 0.025 SSIM on a prospectively acquired dataset, and 0.928 ± 0.003 SSIM on an out-of-distribution cardiac cine-loop after gain adjustment. To our knowledge, this is the first work to establish deep learning models that closely approximate ultrasound post-processing found in current medical practice. MimickNet serves as a clinical post-processing baseline for future works in ultrasound image formation to compare against. Additionally, it can be used as a pretrained model for fine-tuning towards different post-processing techniques. To this end, we have made the MimickNet software, phantom data, and permitted in vivo data open-source at https://github.com/ouwen/MimickNet.", "publication_location": "Ieee Transactions on Medical Imaging", "link": "http://dx.doi.org/10.1109/tmi.2020.2970867", "citations": "1", "readership": "6", "tweets": "1", "news_mentions": " "},
{"title": "An Inner-loop Free Solution to Inverse Problems using Deep Neural Networks", "authors": "Fai, K; Wei, Q; Carin, L; Heller, K", "published_date": "January 1, 2017", "doi": " ", "abstract": "© 2017 Neural information processing systems foundation. All rights reserved. We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "What can machine learning teach us about communications?", "authors": "Lian, M; Häger, C; Pfister, HD", "published_date": "January 15, 2019", "doi": "10.1109/ITW.2018.8613331", "abstract": "© 2018 IEEE Information Theory Workshop, ITW 2018. All rights reserved. Rapid improvements in machine learning over the past decade are beginning to have far-reaching effects. For communications, engineers with limited domain expertise can now use off-the-shelf learning packages to design high-performance systems based on simulations. Prior to the current revolution in machine learning, the majority of communication engineers were quite aware that system parameters (such as filter coefficients) could be learned using stochastic gradient descent. It was not at all clear, however, that more complicated parts of the system architecture could be learned as well. In this paper, we discuss the application of machine-learning techniques to two communications problems and focus on what can be learned from the resulting systems. We were pleasantly surprised that the observed gains in one example have a simple explanation that only became clear in hindsight. In essence, deep learning discovered a simple and effective strategy that had not been considered earlier.", "publication_location": "2018 Ieee Information Theory Workshop, Itw 2018", "link": "http://dx.doi.org/10.1109/ITW.2018.8613331", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Inferring Functional Neural Connectivity with Deep Residual Convolutional Networks", "authors": "Dunn, TW; Koo, PK", "published_date": " ", "doi": "10.1101/141010", "abstract": "Measuring synaptic connectivity in large neuronal populations remains a major goal of modern neuroscience. While this connectivity is traditionally revealed by anatomical methods such as electron microscopy, an efficient alternative is to computationally infer functional connectivity from recordings of neural activity. However, these statistical techniques still require further refinement before they can be reliably applied to real data. Here, we report significant improvements to a deep learning method for functional connectomics, as assayed on synthetic ChaLearn Connectomics data. The method, which integrates recent advances in convolutional neural network architecture and model-free partial correlation coefficients, outperforms published methods on competition data and can achieve over 90% precision at 1% recall on validation datasets. This suggests that future application of the model to in vivo whole-brain imaging data in larval zebrafish could reliably recover on the order of 106 synaptic connections with a 10% false discovery rate. The model also generalizes to networks with different underlying connection probabilities and should scale well when parallelized across multiple GPUs. The method offers real potential as a statistical complement to existing experiments and circuit hypotheses in neuroscience.", "publication_location": " ", "link": "http://dx.doi.org/10.1101/141010", "citations": " ", "readership": "25", "tweets": "10", "news_mentions": " "},
{"title": "Learned sensing: jointly optimized microscope hardware for accurate image classification.", "authors": "Muthumbi, A; Chaware, A; Kim, K; Zhou, KC; Konda, PC; Chen, R; Judkewitz, B; Erdmann, A; Kappes, B; Horstmeyer, R", "published_date": "December 2019", "doi": "10.1364/BOE.10.006351", "abstract": "Since its invention, the microscope has been optimized for interpretation by a human observer. With the recent development of deep learning algorithms for automated image analysis, there is now a clear need to re-design the microscope's hardware for specific interpretation tasks. To increase the speed and accuracy of automated image classification, this work presents a method to co-optimize how a sample is illuminated in a microscope, along with a pipeline to automatically classify the resulting image, using a deep neural network. By adding a \"physical layer\" to a deep classification network, we are able to jointly optimize for specific illumination patterns that highlight the most important sample features for the particular learning task at hand, which may not be obvious under standard illumination. We demonstrate how our learned sensing approach for illumination design can automatically identify malaria-infected cells with up to 5-10% greater accuracy than standard and alternative microscope lighting designs. We show that this joint hardware-software design procedure generalizes to offer accurate diagnoses for two different blood smear types, and experimentally show how our new procedure can translate across different experimental setups while maintaining high accuracy.", "publication_location": "Biomedical Optics Express", "link": "http://dx.doi.org/10.1364/BOE.10.006351", "citations": "5", "readership": "33", "tweets": "11", "news_mentions": "11"},
{"title": "Upstream fusion of multiple sensing modalities using machine learning and topological analysis: An initial exploration", "authors": "Garagić, D; Peskoe, J; Liu, F; Claffey, MS; Bendich, P; Hineman, J; Borggren, N; Harer, J; Zulch, P; Rhodes, BJ", "published_date": "June 25, 2018", "doi": "10.1109/AERO.2018.8396737", "abstract": "© 2018 IEEE. This paper presents a processing pipeline for fusing 'raw' and / or feature-level multi-sensor data - upstream fusion - and initial results from this pipeline using imagery, radar, and radio frequency (RF) signals data to determine which tracked object, among several, hosts an emitter of interest. Correctly making this determination requires fusing data across these modalities. Our approach performs better than standard fusion approaches that make detection / characterization decisions for each modality individually and then try to fuse those decisions - downstream (or post-decision) fusion. Our approach (1) fully exploits the inter-modality dependencies and phenomenologies inherent in different sensing modes, (2) automatically discovers compressive hierarchical representations that integrate structural and statistical characteristics to enhance target / event discriminability, and (3) completely obviates the need to specify features, manifolds, or model scope a priori. This approach comprises a unique synthesis of Deep Learning (DL), topological analysis over probability measure (TAPM), and hierarchical Bayesian non-parametric (HBNP) recognition models. Deep Generative Networks (DGNs - a deep generative statistical form of DL) create probability measures that provide a basis for calculating homologies (topological summaries over the probability measures). The statistics of the resulting persistence diagrams are inputs to HBNP methods that learn to discriminate between target types and distinguish emitting targets from non-emitting targets, for example. HBNP learning obviates batch-mode off-line learning. This approach overcomes the inadequacy of pre-defined features as a means for creating efficient, discriminating, low-dimensional representations from high-dimensional multi-modality sensor data collected under difficult, dynamic sensing conditions. The invariant properties in the resulting compact representations afford multiple compressive sensing benefits, including concise information sharing and enhanced performance. Machine learning makes adaptivity a central feature of our approach. Adaptivity is critical because it enables flexible processing that automatically accommodates a broad range of challenges that non-adaptive, standard fusion approaches would typically require manual intervention to begin to address. These include (a) interest in unknown or unanticipated targets, (b) desire to be rapidly able to fuse between different combinations of sensor modalities, and (c) potential need to transfer information between platforms that host different sensors. This paper presents results that demonstrate our approach enables accurate, real-time target detection, tracking, and recognition of known and unknown moving or stationary targets or events and their activities evolving over space and time.", "publication_location": "Ieee Aerospace Conference Proceedings", "link": "http://dx.doi.org/10.1109/AERO.2018.8396737", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Generalization error of deep neural networks: Role of classification margin and data structure", "authors": "Sokolić, J; Giryes, R; Sapiro, G; Rodrigues, MRD", "published_date": "September 1, 2017", "doi": "10.1109/SAMPTA.2017.8024476", "abstract": "© 2017 IEEE. Understanding the generalization properties of deep learning models is critical for their successful usage in many applications, especially in the regimes where the number of training samples is limited. We study the generalization properties of deep neural networks (DNNs) via the Jacobian matrix of the network. Our analysis is general to arbitrary network structures, types of non-linearities and pooling operations. We show that bounding the spectral norm of the Jacobian matrix in the network reduces the generalization error. In addition, we tie this error to the invariance in the data and the network. Experiments on the MNIST and ImageNet datasets support these findings. This short paper summarizes our generalization error theorems for DNNs and for general invariant classifiers [1], [2].", "publication_location": "2017 12th International Conference on Sampling Theory and Applications, Sampta 2017", "link": "http://dx.doi.org/10.1109/SAMPTA.2017.8024476", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Estimating Rates of Progression and Predicting Future Visual Fields in Glaucoma Using a Deep Variational Autoencoder.", "authors": " ", "published_date": "December 2, 2019", "doi": "10.1038/s41598-019-54653-6", "abstract": "In this manuscript we develop a deep learning algorithm to improve estimation of rates of progression and prediction of future patterns of visual field loss in glaucoma. A generalized variational auto-encoder (VAE) was trained to learn a low-dimensional representation of standard automated perimetry (SAP) visual fields using 29,161 fields from 3,832 patients. The VAE was trained on a 90% sample of the data, with randomization at the patient level. Using the remaining 10%, rates of progression and predictions were generated, with comparisons to SAP mean deviation (MD) rates and point-wise (PW) regression predictions, respectively. The longitudinal rate of change through the VAE latent space (e.g., with eight dimensions) detected a significantly higher proportion of progression than MD at two (25% vs. 9%) and four (35% vs 15%) years from baseline. Early on, VAE improved prediction over PW, with significantly smaller mean absolute error in predicting the 4th, 6th and 8th visits from the first three (e.g., visit eight: VAE8: 5.14 dB vs. PW: 8.07 dB; P < 0.001). A deep VAE can be used for assessing both rates and trajectories of progression in glaucoma, with the additional benefit of being a generative technique capable of predicting future patterns of visual field damage.", "publication_location": "Scientific Reports", "link": "http://dx.doi.org/10.1038/s41598-019-54653-6", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Weakly supervised 3D classification of chest CT using aggregated multi-resolution deep segmentation features", "authors": "Saha, A; Tushar, FI; Faryna, K; D'Anniballe, VM; Hou, R; Mazurowski, MA; Rubin, GD; Lo, JY", "published_date": "January 1, 2020", "doi": "10.1117/12.2550857", "abstract": "© 2020 SPIE. Weakly supervised disease classification of CT imaging suffers from poor localization owing to case-level annotations, where even a positive scan can hold hundreds to thousands of negative slices along multiple planes. Furthermore, although deep learning segmentation and classification models extract distinctly unique combinations of anatomical features from the same target class(es), they are typically seen as two independent processes in a computer-aided diagnosis (CAD) pipeline, with little to no feature reuse. In this research, we propose a medical classifier that leverages the semantic structural concepts learned via multi-resolution segmentation feature maps, to guide weakly supervised 3D classification of chest CT volumes. Additionally, a comparative analysis is drawn across two different types of feature aggregation to explore the vast possibilities surrounding feature fusion. Using a dataset of 1593 scans labeled on a case-level basis via rule-based model, we train a dual-stage convolutional neural network (CNN) to perform organ segmentation and binary classification of four representative diseases (emphysema, pneumonia/atelectasis, mass and nodules) in lungs. The baseline model, with separate stages for segmentation and classification, results in AUC of 0.791. Using identical hyperparameters, the connected architecture using static and dynamic feature aggregation improves performance to AUC of 0.832 and 0.851, respectively. This study advances the field in two key ways. First, case-level report data is used to weakly supervise a 3D CT classifier of multiple, simultaneous diseases for an organ. Second, segmentation and classification models are connected with two different feature aggregation strategies to enhance the classification performance.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2550857", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Toward Generalized Change Detection on Planetary Surfaces with Convolutional Autoencoders and Transfer Learning", "authors": "Kerner, HR; Wagstaff, KL; Bue, BD; Gray, PC; Iii, JFB; Amor, HB", "published_date": "October 1, 2019", "doi": "10.1109/JSTARS.2019.2936771", "abstract": "© 2008-2012 IEEE. Ongoing planetary exploration missions are returning large volumes of image data. Identifying surface changes in these images, e.g., new impact craters, is critical for investigating many scientific hypotheses. Traditional approaches to change detection rely on image differencing and manual feature engineering. These methods can be sensitive to irrelevant variations in illumination or image quality and typically require before and after images to be coregistered, which itself is a major challenge. Additionally, most prior change detection studies have been limited to remote sensing images of earth. We propose a new deep learning approach for binary patch-level change detection involving transfer learning and nonlinear dimensionality reduction using convolutional autoencoders. Our experiments on diverse remote sensing datasets of Mars, the moon, and earth show that our methods can detect meaningful changes with high accuracy using a relatively small training dataset despite significant differences in illumination, image quality, imaging sensors, coregistration, and surface properties. We show that the latent representations learned by a convolutional autoencoder yield the most general representations for detecting change across surface feature types, scales, sensors, and planetary bodies.", "publication_location": "Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "link": "http://dx.doi.org/10.1109/JSTARS.2019.2936771", "citations": "1", "readership": "13", "tweets": "8", "news_mentions": " "},
{"title": "Machine Learning Algorithm Predicts Cardiac Resynchronization Therapy Outcomes: Lessons From the COMPANION Trial.", "authors": "Kalscheur, MM; Kipp, RT; Tattersall, MC; Mei, C; Buhr, KA; DeMets, DL; Field, ME; Eckhardt, LL; Page, CD", "published_date": "January 2018", "doi": "10.1161/CIRCEP.117.005499", "abstract": "BACKGROUND: Cardiac resynchronization therapy (CRT) reduces morbidity and mortality in heart failure patients with reduced left ventricular function and intraventricular conduction delay. However, individual outcomes vary significantly. This study sought to use a machine learning algorithm to develop a model to predict outcomes after CRT. METHODS AND RESULTS: Models were developed with machine learning algorithms to predict all-cause mortality or heart failure hospitalization at 12 months post-CRT in the COMPANION trial (Comparison of Medical Therapy, Pacing, and Defibrillation in Heart Failure). The best performing model was developed with the random forest algorithm. The ability of this model to predict all-cause mortality or heart failure hospitalization and all-cause mortality alone was compared with discrimination obtained using a combination of bundle branch block morphology and QRS duration. In the 595 patients with CRT-defibrillator in the COMPANION trial, 105 deaths occurred (median follow-up, 15.7 months). The survival difference across subgroups differentiated by bundle branch block morphology and QRS duration did not reach significance (P=0.08). The random forest model produced quartiles of patients with an 8-fold difference in survival between those with the highest and lowest predicted probability for events (hazard ratio, 7.96; P<0.0001). The model also discriminated the risk of the composite end point of all-cause mortality or heart failure hospitalization better than subgroups based on bundle branch block morphology and QRS duration. CONCLUSIONS: In the COMPANION trial, a machine learning algorithm produced a model that predicted clinical outcomes after CRT. Applied before device implant, this model may better differentiate outcomes over current clinical discriminators and improve shared decision-making with patients.", "publication_location": "Circ Arrhythm Electrophysiol", "link": "http://dx.doi.org/10.1161/CIRCEP.117.005499", "citations": "20", "readership": "55", "tweets": "9", "news_mentions": " "},
{"title": "Abnormal events detection using deep neural networks: Application to extreme sea surface temperature detection in the Red Sea", "authors": " ", "published_date": "March 1, 2019", "doi": "10.1117/1.JEI.28.2.021012", "abstract": "© 2019 SPIE and IS & T. We present a method based on deep learning for detecting and localizing abnormal/extreme events in sea surface temperature (SST) of the Red Sea images using training samples of normal events only. The method operates in two stages; the first one involves features extraction from each patch of the SST input image using the first two convolutional layers extracted from a pretrained convolutional neural network. In the second stage, two methods are used for training the model from the normal training data. The first method uses one-class support vector machine (1-SVM) classifier that allows a fast and robust abnormal detection in the presence of outliers in the training dataset. In the second method, a Gaussian model is defined on the Mahalanobis distances between all normal training data. Experimental tests are conducted on satellite-derived SST data of the Red Sea spanning for a period of 31 years (1985-2015). Our results suggest that the Gaussian model of Mahalanobis distances outperformed 1-SVM by providing better performance in terms of sensitivity and specificity.", "publication_location": "Journal of Electronic Imaging", "link": "http://dx.doi.org/10.1117/1.JEI.28.2.021012", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images.", "authors": "Lu, D; Popuri, K; Ding, GW; Balachandar, R; Beg, MF; Alzheimer’s Disease Neuroimaging Initiative,", "published_date": "April 9, 2018", "doi": "10.1038/s41598-018-22871-z", "abstract": "Alzheimer's Disease (AD) is a progressive neurodegenerative disease where biomarkers for disease based on pathophysiology may be able to provide objective measures for disease diagnosis and staging. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo measurements of structure and function (glucose metabolism) in a living brain. It is hypothesized that combining multiple different image modalities providing complementary information could help improve early diagnosis of AD. In this paper, we propose a novel deep-learning-based framework to discriminate individuals with AD utilizing a multimodal and multiscale deep neural network. Our method delivers 82.4% accuracy in identifying the individuals with mild cognitive impairment (MCI) who will convert to AD at 3 years prior to conversion (86.4% combined accuracy for conversion within 1-3 years), a 94.23% sensitivity in classifying individuals with clinical diagnosis of probable AD, and a 86.3% specificity in classifying non-demented controls improving upon results in published literature.", "publication_location": "Scientific Reports", "link": "http://dx.doi.org/10.1038/s41598-018-22871-z", "citations": "62", "readership": "212", "tweets": "13", "news_mentions": " "},
{"title": "How to escape saddle points efficiently", "authors": "Jin, C; Ge, R; Netrapalli, P; Kakade, SM; Jordan, MI", "published_date": "January 1, 2017", "doi": " ", "abstract": "© Copyright 2017 by the author(s). This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost \"dimension-free\"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.", "publication_location": "34th International Conference on Machine Learning, Icml 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Random forests can hash", "authors": "Qiu, Q; Sapiro, G; Bronstein, A", "published_date": "January 1, 2015", "doi": " ", "abstract": "© 2015 International Conference on Learning Representations, ICLR. All rights reserved. Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.", "publication_location": "3rd International Conference on Learning Representations, Iclr 2015   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Random forests can hash", "authors": "Qiu, Q; Sapiro, G; Bronstein, A", "published_date": "January 1, 2015", "doi": " ", "abstract": "© 2015 International Conference on Learning Representations, ICLR. All rights reserved. Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks.", "publication_location": "3rd International Conference on Learning Representations, Iclr 2015   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Stochastic spectral descent for restricted boltzmann machines", "authors": "Carlson, D; Cevher, V; Carin, L", "published_date": "January 1, 2015", "doi": " ", "abstract": "Copyright 2015 by the authors. Restricted Boltzmann Machines (RBMs) are widely used as building blocks for deep learning models. Learning typically proceeds by using stochastic gradient descent, and the gradients are estimated with sampling methods. However, the gradient estimation is a computational bottleneck, so better use of the gradients will speed up the descent algorithm. To this end, we first derive upper bounds on the RBM cost function, then show that descent methods can have natural advantages by operating in the ℓ∞ and Shatten-∞ norm. We introduce a new method called \"Stochastic Spectral Descent\" that updates parameters in the normed space. Empirical results show dramatic improvements over stochastic gradient descent, and have only have a fractional increase on the per-iteration cost.", "publication_location": "Journal of Machine Learning Research", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Feature space perturbations yield more transferable adversarial examples", "authors": " ", "published_date": "June 1, 2019", "doi": "10.1109/CVPR.2019.00723", "abstract": "© 2019 IEEE. Many recent works have shown that deep learning models are vulnerable to quasi-imperceptible input perturbations, yet practitioners cannot fully explain this behavior. This work describes a transfer-based blackbox targeted adversarial attack of deep feature space representations that also provides insights into cross-model class representations of deep CNNs. The attack is explicitly designed for transferability and drives feature space representation of a source image at layer L towards the representation of a target image at L. The attack yields highly transferable targeted examples, which outperform competition winning methods by over 30% in targeted attack metrics. We also show the choice of L to generate examples from is important, transferability characteristics are blackbox model agnostic, and indicate that well trained deep models have similar highly-abstract representations.", "publication_location": "Proceedings of the Ieee Computer Society Conference on Computer Vision and Pattern Recognition", "link": "http://dx.doi.org/10.1109/CVPR.2019.00723", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Provable bounds for learning some deep representations", "authors": "Arora, S; Bhaskara, A; Ge, R; Ma, T", "published_date": "January 1, 2014", "doi": " ", "abstract": "2014 We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer network that has degree at most nγ for some γ < 1 and each edge has a random edge weight in [-1,1]. Our algorithm learns almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural nets with random edge weights.", "publication_location": "31st International Conference on Machine Learning, Icml 2014", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning structured sparsity in deep neural networks", "authors": "Wen, W; Wu, C; Wang, Y; Chen, Y; Li, H", "published_date": "January 1, 2016", "doi": " ", "abstract": "© 2016 NIPS Foundation - All Rights Reserved. High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN's evaluation. Experimental results show that SSL achieves on average 5.1× and 3.1× speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth reduces a 20-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25% to 92.60%, which is still higher than that of original ResNet with 32 layers. For AlexNet, SSL reduces the error by ∼ 1%.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A generative model for deep convolutional learning", "authors": "Pu, Y; Yuan, X; Carin, L", "published_date": "January 1, 2015", "doi": " ", "abstract": "© 2015 International Conference on Learning Representations, ICLR. All rights reserved. A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.", "publication_location": "3rd International Conference on Learning Representations, Iclr 2015   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A generative model for deep convolutional learning", "authors": "Pu, Y; Yuan, X; Carin, L", "published_date": "January 1, 2015", "doi": " ", "abstract": "© 2015 International Conference on Learning Representations, ICLR. All rights reserved. A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.", "publication_location": "3rd International Conference on Learning Representations, Iclr 2015   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep metric learning with data summarization", "authors": "Wang, W; Chen, C; Chen, W; Rai, P; Carin, L", "published_date": "January 1, 2016", "doi": "10.1007/978-3-319-46128-1_49", "abstract": "© Springer International Publishing AG 2016. We present Deep Stochastic Neighbor Compression (DSNC), a framework to compress training data for instance-based methods (such as k-nearest neighbors). We accomplish this by inferring a smaller set of pseudo-inputs in a new feature space learned by a deep neural network. Our framework can equivalently be seen as jointly learning a nonlinear distance metric (induced by the deep feature space) and learning a compressed version of the training data. In particular, compressing the data in a deep feature space makes DSNC robust against label noise and issues such as within-class multi-modal distributions. This leads to DSNC yielding better accuracies and faster predictions at test time, as compared to other competing methods. We conduct comprehensive empirical evaluations, on both quantitative and qualitative tasks, and on several benchmark datasets, to show its effectiveness as compared to several baselines.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-319-46128-1_49", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A systematic study of the class imbalance problem in convolutional neural networks.", "authors": "Buda, M; Maki, A; Mazurowski, MA", "published_date": "October 2018", "doi": "10.1016/j.neunet.2018.07.011", "abstract": "In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.", "publication_location": "Neural Netw", "link": "http://dx.doi.org/10.1016/j.neunet.2018.07.011", "citations": "262", "readership": "1038", "tweets": "218", "news_mentions": " "},
{"title": "DeepMellow: Removing the need for a target network in deep q-learning", "authors": "Kim, S; Asadi, K; Littman, M; Konidaris, G", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 2019 International Joint Conferences on Artificial Intelligence. All rights reserved. Deep Q-Network (DQN) is an algorithm that achieves human-level performance in complex domains like Atari games. One of the important elements of DQN is its use of a target network, which is necessary to stabilize learning. We argue that using a target network is incompatible with online reinforcement learning, and it is possible to achieve faster and more stable learning without a target network when we use Mellowmax, an alternative softmax operator. We derive novel properties of Mellowmax, and empirically show that the combination of DQN and Mellowmax, but without a target network, outperforms DQN with a target network.", "publication_location": "Ijcai International Joint Conference on Artificial Intelligence", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Background Adaptive Faster R-CNN for semi-supervised convolutional object detection of threats in X-ray images", "authors": "Sigman, JB; Spell, GP; Liang, KJ; Carin, L", "published_date": "January 1, 2020", "doi": "10.1117/12.2558542", "abstract": "© 2020 SPIE. Recently, progress has been made in the supervised training of Convolutional Object Detectors (e.g. Faster R-CNN) for threat recognition in carry-on luggage using X-ray images. This is part of the Transportation Security Administration's (TSA's) mission to ensure safety for air travelers in the United States. Collecting more data reliably improves performance for this class of deep algorithm, but requires time and money to produce training data with threats staged in realistic contexts. In contrast to these hand-collected data containing threats, data from the real-world, known as the Stream-of-Commerce (SOC), can be collected quickly with minimal cost; while technically unlabeled, in this work we make a practical assumption that these are without threat objects. Because of these data constraints, we will use both labeled and unlabeled sources of data for the automatic threat recognition problem. In this paper, we present a semi-supervised approach for this problem which we call Background Adaptive Faster R-CNN. This approach is a training method for two-stage object detectors which uses Domain Adaptation methods from the field of deep learning. The data sources described earlier are considered two “domains”: one a hand-collected data domain of images with threats, and the other a real-world domain of images assumed without threats. Two domain discriminators, one for discriminating object proposals and one for image features, are adversarially trained to prevent encoding domain-specific information. Penalizing this encoding is important because otherwise the Convolutional Neural Network (CNN) can learn to distinguish images from the two sources based on superficial characteristics, and minimize a purely supervised loss function without improving its ability to recognize objects. For the hand-collected data, only object proposals and image features completely outside of areas corresponding to ground truth object bounding boxes (background) are used. The losses for these domain-adaptive discriminators are added to the Faster R-CNN losses of images from both domains. This technique enables threat recognition based on examples from the labeled data, and can reduce false alarm rates by matching the statistics of extracted features on the hand-collected backgrounds to that of the real world data. Performance improvements are demonstrated on two independently-collected datasets of labeled threats.", "publication_location": "Smart Structures and Materials 2005: Active Materials: Behavior and Mechanics", "link": "http://dx.doi.org/10.1117/12.2558542", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Targeting EEG/LFP synchrony with neural nets", "authors": "Li, Y; Murias, M; Major, S; Dawson, G; Dzirasa, K; Carin, L; Carlson, DE", "published_date": "January 1, 2017", "doi": " ", "abstract": "© 2017 Neural information processing systems foundation. All rights reserved. We consider the analysis of Electroencephalography (EEG) and Local Field Potential (LFP) datasets, which are \"big\" in terms of the size of recorded data but rarely have sufficient labels required to train complex models (e.g., conventional deep learning methods). Furthermore, in many scientific applications, the goal is to be able to understand the underlying features related to the classification, which prohibits the blind application of deep networks. This motivates the development of a new model based on parameterized convolutional filters guided by previous neuroscience research; the filters learn relevant frequency bands while targeting synchrony, which are frequency-specific power and phase correlations between electrodes. This results in a highly expressive convolutional neural network with only a few hundred parameters, applicable to smaller datasets. The proposed approach is demonstrated to yield competitive (often state-of-the-art) predictive performance during our empirical tests while yielding interpretable features. Furthermore, a Gaussian process adapter is developed to combine analysis over distinct electrode layouts, allowing the joint processing of multiple datasets to address overfitting and improve generalizability. Finally, it is demonstrated that the proposed framework effectively tracks neural dynamics on children in a clinical trial on Autism Spectrum Disorder.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Breast mass detection in mammography and tomosynthesis via fully convolutional network-based heatmap regression", "authors": "Zhang, J; Cain, EH; Saha, A; Zhu, Z; Mazurowski, MA", "published_date": "January 1, 2018", "doi": "10.1117/12.2295443", "abstract": "© 2018 SPIE. Breast mass detection in mammography and digital breast tomosynthesis (DBT) is an essential step in computerized breast cancer analysis. Deep learning-based methods incorporate feature extraction and model learning into a unified framework and have achieved impressive performance in various medical applications (e.g., disease diagnosis, tumor detection, and landmark detection). However, these methods require large-scale accurately annotated data. Unfortunately, it is challenging to get precise annotations of breast masses. To address this issue, we propose a fully convolutional network (FCN) based heatmap regression method for breast mass detection, using only weakly annotated mass regions in mammography images. Specifically, we first generate heat maps of masses based on human-annotated rough regions for breast masses. We then develop an FCN model for end-to-end heatmap regression with an F-score loss function, where the mammography images are regarded as the input and heatmaps for breast masses are used as the output. Finally, the probability map of mass locations can be estimated with the trained model. Experimental results on a mammography dataset with 439 subjects demonstrate the effectiveness of our method. Furthermore, we evaluate whether we can use mammography data to improve detection models for DBT, since mammography shares similar structure with tomosynthesis. We propose a transfer learning strategy by fine-tuning the learned FCN model from mammography images. We test this approach on a small tomosynthesis dataset with only 40 subjects, and we show an improvement in the detection performance as compared to training the model from scratch.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2295443", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "On target shift in adversarial domain adaptation", "authors": "Li, Y; Murias, M; Major, S; Dawson, G; Carlson, DE", "published_date": "January 1, 2020", "doi": " ", "abstract": "© 2019 by the author(s). Discrepancy between training and testing domains is a fundamental problem in the generalization of machine learning techniques. Recently, several approaches have been proposed to learn domain invariant feature representations through adversarial deep learning. However, label shift, where the percentage of data in each class is different between domains, has received less attention. Label shift naturally arises in many contexts, especially in behavioral studies where the behaviors are freely chosen. In this work, we propose a method called Domain Adversarial nets for Target Shift (DATS) to address label shift while learning a domain invariant representation. This is accomplished by using distribution matching to estimate label proportions in a blind test set. We extend this framework to handle multiple domains by developing a scheme to upweight source domains most similar to the target domain. Empirical results show that this framework performs well under large label shift in synthetic and real experiments, demonstrating the practical importance.", "publication_location": "Aistats 2019   22nd International Conference on Artificial Intelligence and Statistics", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Incorporating side-channel information into convolutional neural networks for robotic tasks", "authors": "Zhou, Y; Hauser, K", "published_date": "July 21, 2017", "doi": "10.1109/ICRA.2017.7989251", "abstract": "© 2017 IEEE. Convolutional neural networks (CNN) are a deep learning technique that has achieved state-of-the-art prediction performance in computer vision and robotics, but assume the input data can be formatted as an image or video (e.g. predicting a robot grasping location given RGB-D image input). This paper considers the problem of augmenting a traditional CNN for handling image-like input (called main-channel input) with additional, highly predictive, non-image-like input (called side-channel input). An example of such a task would be to predict whether a robot path is collision-free given an occupancy grid of the environment and the path's start and goal configurations; the occupancy grid is the main-channel and the start and goal are the side-channel. This paper presents several candidate network architectures for doing so. Empirical tests on robot collision prediction and control problems compare the proposed architectures in terms of learning speed, memory usage, learning capacity, and susceptibility to overfitting.", "publication_location": "Proceedings   Ieee International Conference on Robotics and Automation", "link": "http://dx.doi.org/10.1109/ICRA.2017.7989251", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "MAT: A multi-strength adversarial training method to mitigate adversarial attacks", "authors": "Song, C; Cheng, HP; Yang, H; Li, S; Wu, C; Wu, Q; Chen, Y; Li, H", "published_date": "August 7, 2018", "doi": "10.1109/ISVLSI.2018.00092", "abstract": "© 2018 IEEE. Some recent work revealed that deep neural networks (DNNs) are vulnerable to so-called adversarial attacks where input examples are intentionally perturbed to fool DNNs. In this work, we revisit the DNN training process that includes adversarial examples into the training dataset so as to improve DNN's resilience to adversarial attacks, namely, adversarial training. Our experiments show that different adversarial strengths, i.e., perturbation levels of adversarial examples, have different working ranges to resist the attacks. Based on the observation, we propose a multi-strength adversarial training method (MAT) that combines the adversarial training examples with different adversarial strengths to defend adversarial attacks. Two training structures-mixed MAT and parallel MAT-are developed to facilitate the tradeoffs between training time and hardware cost. Our results show that MAT can substantially minimize the accuracy degradation of deep learning systems to adversarial attacks on MNIST, CIFAR-10, CIFAR-100, and SVHN. The tradeoffs between training time, robustness, and hardware cost are also well discussed on a FPGA platform.", "publication_location": "Proceedings of Ieee Computer Society Annual Symposium on Vlsi, Isvlsi", "link": "http://dx.doi.org/10.1109/ISVLSI.2018.00092", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "QuickNAT: A fully convolutional network for quick and accurate segmentation of neuroanatomy.", "authors": "Guha Roy, A; Conjeti, S; Navab, N; Wachinger, C; Alzheimer's Disease Neuroimaging Initiative,", "published_date": "February 1, 2019", "doi": "10.1016/j.neuroimage.2018.11.042", "abstract": "Whole brain segmentation from structural magnetic resonance imaging (MRI) is a prerequisite for most morphological analyses, but is computationally intense and can therefore delay the availability of image markers after scan acquisition. We introduce QuickNAT, a fully convolutional, densely connected neural network that segments a MRI brain scan in 20 s. To enable training of the complex network with millions of learnable parameters using limited annotated data, we propose to first pre-train on auxiliary labels created from existing segmentation software. Subsequently, the pre-trained model is fine-tuned on manual labels to rectify errors in auxiliary labels. With this learning strategy, we are able to use large neuroimaging repositories without manual annotations for training. In an extensive set of evaluations on eight datasets that cover a wide age range, pathology, and different scanners, we demonstrate that QuickNAT achieves superior segmentation accuracy and reliability in comparison to state-of-the-art methods, while being orders of magnitude faster. The speed up facilitates processing of large data repositories and supports translation of imaging biomarkers by making them available within seconds for fast clinical decision making.", "publication_location": "Neuroimage", "link": "http://dx.doi.org/10.1016/j.neuroimage.2018.11.042", "citations": "33", "readership": "116", "tweets": "21", "news_mentions": " "},
{"title": "Breast tumor segmentation in DCE-MRI using fully convolutional networks with an application in radiogenomics", "authors": "Zhang, J; Saha, A; Zhu, Z; Mazurowski, MA", "published_date": "January 1, 2018", "doi": "10.1117/12.2295436", "abstract": "© 2018 SPIE. Breast tumor segmentation based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) remains an active as well as a challenging problem. Previous studies often rely on manual annotation for tumor regions, which is not only time-consuming but also error-prone. Recent studies have shown high promise of deep learning-based methods in various segmentation problems. However, these methods are usually faced with the challenge of limited number (e.g., tens or hundreds) of medical images for training, leading to sub-optimal segmentation performance. Also, previous methods cannot efficiently deal with prevalent class-imbalance problems in tumor segmentation, where the number of voxels in tumor regions is much lower than that in the background area. To address these issues, in this study, we propose a mask-guided hierarchical learning (MHL) framework for breast tumor segmentation via fully convolutional networks (FCN). Our strategy is first decomposing the original difficult problem into several sub-problems and then solving these relatively simpler sub-problems in a hierarchical manner. To precisely identify locations of tumors that underwent a biopsy, we further propose an FCN model to detect two landmarks defined on nipples. Finally, based on both segmentation probability maps and our identified landmarks, we proposed to select biopsied tumors from all detected tumors via a tumor selection strategy using the pathology location. We validate our MHL method using data for 272 patients, and achieve a mean Dice similarity coefficient (DSC) of 0.72 in breast tumor segmentation. Finally, in a radiogenomic analysis, we show that a previously developed image features show a comparable performance for identifying luminal A subtype when applied to the automatic segmentation and a semi-manual segmentation demonstrating a high promise for fully automated radiogenomic analysis in breast cancer.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2295436", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Tradeoffs between convergence speed and reconstruction accuracy in inverse problems", "authors": "Giryes, R; Eldar, YC; Bronstein, AM; Sapiro, G", "published_date": "April 1, 2018", "doi": "10.1109/TSP.2018.2791945", "abstract": "© 2018 IEEE. Solving inverse problems with iterative algorithms is popular, especially for large data. Due to time constraints, the number of possible iterations is usually limited, potentially affecting the achievable accuracy. Given an error one is willing to tolerate, an important question is whether it is possible to modify the original iterations to obtain faster convergence to a minimizer achieving the allowed error without increasing the computational cost of each iteration considerably. Relying on recent recovery techniques developed for settings in which the desired signal belongs to some low-dimensional set, we show that using a coarse estimate of this set may lead to faster convergence at the cost of an additional reconstruction error related to the accuracy of the set approximation. Our theory ties to recent advances in sparse recovery, compressed sensing, and deep learning. Particularly, it may provide a possible explanation to the successful approximation of the 1 -minimization solution by neural networks with layers representing iterations, as practiced in the learned iterative shrinkage-thresholding algorithm.", "publication_location": "Ieee Transactions on Signal Processing", "link": "http://dx.doi.org/10.1109/TSP.2018.2791945", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms", "authors": "Shen, D; Wang, G; Wang, W; Min, MR; Su, Q; Zhang, Y; Li, C; Henao, R; Carin, L", "published_date": "January 1, 2018", "doi": "10.18653/v1/p18-1041", "abstract": "© 2018 Association for Computational Linguistics Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring a substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging.", "publication_location": "Acl 2018   56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)", "link": "http://dx.doi.org/10.18653/v1/p18-1041", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Multimodality Imaging of Ductal Carcinoma In Situ", "authors": " ", "published_date": "March 1, 2020", "doi": "10.1007/s12609-019-00349-9", "abstract": "© 2020, Springer Science+Business Media, LLC, part of Springer Nature. Purpose of Review: We aim to review the appearance of ductal carcinoma in situ (DCIS) across the spectrum of imaging modalities used in common clinical practice. Recent Findings: Changes in technology and clinical breast cancer screening patterns have impacted the imaging evaluation of DCIS. DCIS classically presents as asymptomatic calcifications in women undergoing screening mammography. The replacement of traditional 2D mammography with digital breast tomosynthesis has changed the typical appearance of screen-detected DCIS. Ultrasound is traditionally utilized to detect DCIS in women with clinical symptoms, but efforts to increase screening ultrasound rates for women with dense breasts makes it more important to identify the appearance of DCIS in asymptomatic women. Improvements in MRI technology have made MRI the most sensitive imaging modality to detect DCIS and define the extent of disease, which is increasingly important given greater utilization of MRI for high-risk screening and determination of extent of known disease. Finally, the emergence of active surveillance, or non-surgical management, for DCIS has increased the focus on presurgical identification of associated invasive cancer, with early results demonstrating promise via computer vision and deep learning approaches for this task. Summary: DCIS has a highly variable imaging appearance which is subject to changes in imaging technology and clinical management.", "publication_location": "Current Breast Cancer Reports", "link": "http://dx.doi.org/10.1007/s12609-019-00349-9", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Artificial Intelligence and Echocardiography: A Primer for Cardiac Sonographers.", "authors": " ", "published_date": "June 11, 2020", "doi": "10.1016/j.echo.2020.04.025", "abstract": "Artificial intelligence (AI) is emerging as a key component in diagnostic medical imaging, including echocardiography. AI with deep learning has already been used with automated view labeling, measurements, and interpretation. As the development and use of AI in echocardiography increase, potential concerns may be raised by cardiac sonographers and the profession. This report, from a sonographer's perspective, focuses on defining AI, the basics of the technology, identifying some current applications of AI, and how the use of AI may improve patient care in the future.", "publication_location": "Journal of the American Society of Echocardiography", "link": "http://dx.doi.org/10.1016/j.echo.2020.04.025", "citations": " ", "readership": " ", "tweets": "36", "news_mentions": " "},
{"title": "A comparison of models for predicting early hospital readmissions.", "authors": " ", "published_date": "August 2015", "doi": "10.1016/j.jbi.2015.05.016", "abstract": "Risk sharing arrangements between hospitals and payers together with penalties imposed by the Centers for Medicare and Medicaid (CMS) are driving an interest in decreasing early readmissions. There are a number of published risk models predicting 30day readmissions for particular patient populations, however they often exhibit poor predictive performance and would be unsuitable for use in a clinical setting. In this work we describe and compare several predictive models, some of which have never been applied to this task and which outperform the regression methods that are typically applied in the healthcare literature. In addition, we apply methods from deep learning to the five conditions CMS is using to penalize hospitals, and offer a simple framework for determining which conditions are most cost effective to target.", "publication_location": "Journal of Biomedical Informatics", "link": "http://dx.doi.org/10.1016/j.jbi.2015.05.016", "citations": "106", "readership": "319", "tweets": "18", "news_mentions": "57"},
{"title": "Decoding Movements from Cortical Ensemble Activity Using a Long Short-Term Memory Recurrent Network.", "authors": "Tseng, P-H; Urpi, NA; Lebedev, M; Nicolelis, M", "published_date": "June 2019", "doi": "10.1162/neco_a_01189", "abstract": "Although many real-time neural decoding algorithms have been proposed for brain-machine interface (BMI) applications over the years, an optimal, consensual approach remains elusive. Recent advances in deep learning algorithms provide new opportunities for improving the design of BMI decoders, including the use of recurrent artificial neural networks to decode neuronal ensemble activity in real time. Here, we developed a long-short term memory (LSTM) decoder for extracting movement kinematics from the activity of large (N = 134-402) populations of neurons, sampled simultaneously from multiple cortical areas, in rhesus monkeys performing motor tasks. Recorded regions included primary motor, dorsal premotor, supplementary motor, and primary somatosensory cortical areas. The LSTM's capacity to retain information for extended periods of time enabled accurate decoding for tasks that required both movements and periods of immobility. Our LSTM algorithm significantly outperformed the state-of-the-art unscented Kalman filter when applied to three tasks: center-out arm reaching, bimanual reaching, and bipedal walking on a treadmill. Notably, LSTM units exhibited a variety of well-known physiological features of cortical neuronal activity, such as directional tuning and neuronal dynamics across task epochs. LSTM modeled several key physiological attributes of cortical circuits involved in motor tasks. These findings suggest that LSTM-based approaches could yield a better algorithm strategy for neuroprostheses that employ BMIs to restore movement in severely disabled patients.", "publication_location": "Neural Comput", "link": "http://dx.doi.org/10.1162/neco_a_01189", "citations": "4", "readership": "20", "tweets": "6", "news_mentions": "2"},
{"title": "Evaluation of an AI-Based Detection Software for Acute Findings in Abdominal Computed Tomography Scans: Toward an Automated Work List Prioritization of Routine CT Examinations.", "authors": " ", "published_date": "January 2019", "doi": "10.1097/RLI.0000000000000509", "abstract": "OBJECTIVE: The aim of this study was to test the diagnostic performance of a deep learning-based triage system for the detection of acute findings in abdominal computed tomography (CT) examinations. MATERIALS AND METHODS: Using a RIS/PACS (Radiology Information System/Picture Archiving and Communication System) search engine, we obtained 100 consecutive abdominal CTs with at least one of the following findings: free-gas, free-fluid, or fat-stranding and 100 control cases with absence of these findings. The CT data were analyzed using a convolutional neural network algorithm previously trained for detection of these findings on an independent sample. The validation of the results was performed on a Web-based feedback system by a radiologist with 1 year of experience in abdominal imaging without prior knowledge of image findings through both visual confirmation and comparison with the clinically approved, written report as the standard of reference. All cases were included in the final analysis, except those in which the whole dataset could not be processed by the detection software. Measures of diagnostic accuracy were then calculated. RESULTS: A total of 194 cases were included in the analysis, 6 excluded because of technical problems during the extraction of the DICOM datasets from the local PACS. Overall, the algorithm achieved a 93% sensitivity (91/98, 7 false-negative) and 97% specificity (93/96, 3 false-positive) in the detection of acute abdominal findings. Intra-abdominal free gas was detected with a 92% sensitivity (54/59) and 93% specificity (39/42), free fluid with a 85% sensitivity (68/80) and 95% specificity (20/21), and fat stranding with a 81% sensitivity (42/50) and 98% specificity (48/49). False-positive results were due to streak artifacts, partial volume effects, and a misidentification of a diverticulum (each n = 1). CONCLUSIONS: The algorithm's autonomous detection of acute pathological abdominal findings demonstrated a high diagnostic performance, enabling guidance of the radiology workflow toward prioritization of abdominal CT examinations with acute conditions.", "publication_location": "Investigative Radiology", "link": "http://dx.doi.org/10.1097/RLI.0000000000000509", "citations": "11", "readership": "27", "tweets": "4", "news_mentions": " "},
{"title": "Imaging biomarkers of adiposity and sarcopenia as potential predictors for overall survival among patients with endometrial cancer treated with bevacizumab", "authors": "Gillen, J; Mills, KA; Dvorak, J; Zheng, B; Thai, T; Salani, R; Cosgrove, CM; Davidson, B; Thaker, PH; Moore, KN", "published_date": "November 1, 2019", "doi": "10.1016/j.gore.2019.100502", "abstract": "© 2019 The Authors Objective: To examine associations of body mass index (BMI), subcutaneous fat area (SFA) and density (SFD), visceral fat area (VFA) and density (VFD) and total psoas area (TPA) to outcomes among patients receiving chemotherapy with or without bevacizumab for advanced or recurrent endometrial cancer (EC). Methods: This was a multi-institutional, retrospective study of patients with EC treated with and without bevacizumab as part of front-line, platinum based chemotherapy. Demographics and clinical characteristics were collected. SFA, VFA, SFD, VFD, and TPA were determined from pre-treatment CT scans using a deep learning algorithm. Data was compared with overall survival (OS) and progression free survival (PFS). Results: Seventy-eight patients were analyzed. The majority were Caucasian (87.2%) with a mean BMI of 34.7 kg/m2. PFS and OS did not differ between patients with BMI, SFA, VFA, SFD, VFD, or TPA ≥ the 50th percentile compared to <50th percentile (p = 0.91, 0.45, 0.71, 0.74, 0.60, and 0.74 respectively) and (p = 0.99, 0.59, 0.14, 0.77, and 0.85 respectively). When adjusting for prognostic factors, elevated VFA trended towards shorter OS (25.1 vs 59.5 months, HR = 1.68 [0.92–3.05]). Patients receiving bevacizumab had similar OS compared to those who did not (37.6 vs 44.5 months, p = 0.409). When stratified by adiposity markers, no subset demonstrated benefit from bevacizumab. Conclusion: Obesity has been associated with increased levels of vascular endothelial growth factor (VEGF), the main target for bevacizumab therapy. Imaging measurements of VFA may provide prognostic information for patients with EC but no adiposity marker was predictive of improved response to bevacizumab.", "publication_location": "Gynecologic Oncology Reports", "link": "http://dx.doi.org/10.1016/j.gore.2019.100502", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "QBiC-Pred: quantitative predictions of transcription factor binding changes due to sequence variants.", "authors": "Martin, V; Zhao, J; Afek, A; Mielko, Z; Gordân, R", "published_date": "July 2, 2019", "doi": "10.1093/nar/gkz363", "abstract": "Non-coding genetic variants/mutations can play functional roles in the cell by disrupting regulatory interactions between transcription factors (TFs) and their genomic target sites. For most human TFs, a myriad of DNA-binding models are available and could be used to predict the effects of DNA mutations on TF binding. However, information on the quality of these models is scarce, making it hard to evaluate the statistical significance of predicted binding changes. Here, we present QBiC-Pred, a web server for predicting quantitative TF binding changes due to nucleotide variants. QBiC-Pred uses regression models of TF binding specificity trained on high-throughput in vitro data. The training is done using ordinary least squares (OLS), and we leverage distributional results associated with OLS estimation to compute, for each predicted change in TF binding, a P-value reflecting our confidence in the predicted effect. We show that OLS models are accurate in predicting the effects of mutations on TF binding in vitro and in vivo, outperforming widely-used PWM models as well as recently developed deep learning models of specificity. QBiC-Pred takes as input mutation datasets in several formats, and it allows post-processing of the results through a user-friendly web interface. QBiC-Pred is freely available at http://qbic.genome.duke.edu.", "publication_location": "Nucleic Acids Res", "link": "http://dx.doi.org/10.1093/nar/gkz363", "citations": "3", "readership": "26", "tweets": "14", "news_mentions": " "},
{"title": "Smart inverse design of graphene-based photonic metamaterials by an adaptive artificial neural network.", "authors": "Chen, Y; Zhu, J; Xie, Y; Feng, N; Liu, QH", "published_date": "May 2019", "doi": "10.1039/c9nr01315f", "abstract": "The burgeoning research of graphene and other 2D materials enables many unprecedented metamaterials and metadevices for applications on nanophotonics. The design of on-demand graphene-based metamaterials often calls for the solution of a complex inverse problem within a small sampling space, which highly depends on the rich experiences from researchers of nanophotonics. Conventional optimization algorithms could be used for this inverse design, but they converge to local optimal solutions and take significant computational costs with increased nanostructure parameters. Here, we establish a deep learning method based on an adaptive batch-normalized neural network, aiming to implement smart and rapid inverse design for graphene-based metamaterials with on-demand optical responses. This method allows a quick converging speed with high precision and low computational consumption. As typical complex proof-of-concept examples, the optical metamaterials consisting of graphene/dielectric alternating multilayers are chosen to demonstrate the validity of our design paradigm. Our method demonstrates a high prediction accuracy of over 95% after very few training epochs. A universal programming package is developed to achieve the design goals of graphene-based metamaterials with low absorption and near unity absorption, respectively. Our work may find important design applications in the field of nanoscale photonics based on graphene and other 2D materials.", "publication_location": "Nanoscale", "link": "http://dx.doi.org/10.1039/c9nr01315f", "citations": "9", "readership": "22", "tweets": "1", "news_mentions": " "},
{"title": "Digital envirotyping: quantifying environmental determinants of health and behavior.", "authors": "Engelhard, MM; Oliver, JA; McClernon, FJ", "published_date": "2020", "doi": "10.1038/s41746-020-0245-3", "abstract": "Digital phenotyping efforts have used wearable devices to connect a rich array of physiologic data to health outcomes or behaviors of interest. The environmental context surrounding these phenomena has received less attention, yet is critically needed to understand their antecedents and deliver context-appropriate interventions. The coupling of improved smart eyewear with deep learning represents a technological turning point, one that calls for more comprehensive, ambitious study of environments and health.", "publication_location": "Npj Digital Medicine", "link": "http://dx.doi.org/10.1038/s41746-020-0245-3", "citations": " ", "readership": " ", "tweets": "3", "news_mentions": " "},
{"title": "Moving Frailty Toward Clinical Practice: NIA Intramural Frailty Science Symposium Summary.", "authors": "Walston, J; Bandeen-Roche, K; Buta, B; Bergman, H; Gill, TM; Morley, JE; Fried, LP; Robinson, TN; Afilalo, J; Newman, AB; López-Otín, C; De Cabo, R; Theou, O; Studenski, S; Cohen, HJ; Ferrucci, L", "published_date": "August 2019", "doi": "10.1111/jgs.15928", "abstract": "Frailty has long been an important concept in the practice of geriatric medicine and in gerontological research, but integration and implementation of frailty concepts into clinical practice in the United States has been slow. The National Institute on Aging (NIA) Intramural Research Program and the Johns Hopkins Older Americans Independence Center sponsored a symposium to identify potential barriers that impede the movement of frailty into clinical practice and to highlight opportunities to facilitate the further integration of frailty into clinical practice. Primary and subspecialty care providers, and investigators working to integrate and translate new biological aging knowledge into more specific preventive and treatment strategies for frailty provided the meeting content. Recommendations included a call for more specific language that clarifies conceptual differences between frailty definitions and measurement tools; the development of randomized controlled trials to test whether specific intervention strategies for a variety of conditions differently affect frail and non-frail individuals; development of implementation studies and therapeutic trials aimed at tailoring care as a function of pragmatic frailty markers; the use of deep learning and dynamic systems approaches to improve the translatability of findings from epidemiological studies; and the incorporation of advances in aging biology, especially focused on mitochondria, stem cells, and senescent cells, toward the further development of biologically targeted intervention and prevention strategies that can be used to treat or prevent frailty. J Am Geriatr Soc 67:1559-1564, 2019.", "publication_location": "Journal of the American Geriatrics Society", "link": "http://dx.doi.org/10.1111/jgs.15928", "citations": "24", "readership": "50", "tweets": "66", "news_mentions": " "},
{"title": "Classification of chest CT using case-level weak supervision", "authors": "Tang, R; Tushar, FI; Han, S; Hou, R; Rubin, GD; Lo, JY", "published_date": "January 1, 2019", "doi": "10.1117/12.2513576", "abstract": "© 2019 SPIE. Our goal is to investigate using only case-level labels extracted automatically from radiology reports to construct a multi-disease classifier for CT scans with deep learning method. We chose four lung diseases as a start: atelectasis, pulmonary edema, nodule and pneumonia. From a dataset of approximately 5,000 chest CT cases from our institution, we used a rule-based model to analyze those radiologist reports, labeling disease by text mining to identify cases with those diseases. From those results, we randomly selected the following mix of cases: 275 normal, 170 atelectasis, 175 nodule, 195 pulmonary edema, and 208 pneumonia. As a key feature of this study, each chest CT scan was represented by only 10 axial slices (taken at regular intervals through the lungs), and furthermore all slices shared the same label based on the radiology report. So the label was weak, because often disease will not appear in all slices. We used ResNet-50 as our classification model, with 4-fold cross-validation. Each slice was analyzed separately to yield a slice-level performance. For each case, we chose the 5 slices with highest probability and used their mean probability as the final patient-level probability. Performance was evaluated using the receiver operating characteristic (ROC) area under the curve (AUC). For the 4 diseases separately, the slice-based AUCs were 0.71 for nodule, 0.79 for atelectasis, 0.96 for edema, and 0.90 for pneumonia. The patient-based AUC were 0.74 for nodule, 0.83 for atelectasis, 0.97 for edema, and 0.91 for pneumonia. We backprojected the activations of last convolution layer and the weights from prediction layer to synthesize a heat map. This heat map could be an approximate disease detector, also could tell us feature patterns which ResNet-50 focus on.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2513576", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Multi-modal MRI segmentation of sarcoma tumors using convolutional neural networks", "authors": "Holbrook, M; Blocker, SJ; Mowery, YM; Badea, CT", "published_date": "January 1, 2019", "doi": "10.1117/12.2512822", "abstract": "© SPIE. Downloading of the abstract is permitted for personal use only. Small animal imaging is essential in building a bridge from basic science to the clinic by providing the confidence necessary to move new cancer therapies to patients. However, there is considerable variability in preclinical imaging, including tumor volume estimations based on tumor segmentation procedures which can be clearly user-biased. Our group is engaged in developing quantitative imaging methods which will be applied in the preclinical arm of a co-clinical trial studying synergy between anti-PD-1 treatment and radiotherapy using a genetically engineered mouse model of soft tissue sarcoma. This study focuses on a convolutional neural network (CNN)-based method for automatic tumor segmentation based on multimodal MRI images, i.e. T1 weighted, T2 weighted and T1 weighted with contrast agent. Our images were acquired on a 7.0 T Bruker Biospec small animal MRI scanner. Preliminary results show that our U-net structure and 3D patch-wise approach using both Dice and cross entropy loss functions delivers strong segmentation results. We have also compared single performance using only T2 weighted versus multimodal MR images for CNN segmentation. Our results showthat Dice similarity coefficient were higher when using multimodal versus single T2 weighted data (0.84 ± 0.05 and 0.81 ± 0.03). In conclusion, we successfully established a segmentation method for preclinical MR sarcoma data based on deep learning. This approach has the advantage of reducing user bias in tumor segmentation and improving the accuracy and precision of tumor volume estimations for co-clinical cancer trials.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2512822", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Routability-Driven Macro Placement with Embedded CNN-Based Prediction Model", "authors": "Huang, YH; Xie, Z; Fang, GQ; Yu, TC; Ren, H; Fang, SY; Chen, Y; Hu, J", "published_date": "May 14, 2019", "doi": "10.23919/DATE.2019.8715126", "abstract": "© 2019 EDAA. With the dramatic shrink of feature size and the advance of semiconductor technology nodes, numerous and complicated design rules need to be followed, and a chip design can only be taped-out after passing design rule check (DRC). The high design complexity seriously deteriorates design routability, which can be measured by the number of DRC violations after the detailed routing stage. In addition, a modern large-scaled design typically consists of many huge macros due to the wide use of intellectual properties (IPs). Empirically, the placement of these macros greatly determines routability, while there exists no effective cost metric to directly evaluate a macro placement because of the extremely high complexity and unpredictability of cell placement and routing. In this paper, we propose the first work of routability-driven macro placement with deep learning. A convolutional neural network (CNN)-based routability prediction model is proposed and embedded into a macro placer such that a good macro placement with minimized DRC violations can be derived through a simulated annealing (SA) optimization process. Experimental results show the accuracy of the predictor and the effectiveness of the macro placer.", "publication_location": "Proceedings of the 2019 Design, Automation and Test in Europe Conference and Exhibition, Date 2019", "link": "http://dx.doi.org/10.23919/DATE.2019.8715126", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Neuromorphic computing's yesterday, today, and tomorrow – an evolutional view", "authors": "Chen, Y; Li, H; Wu, C; Song, C; Li, S; Min, C; Cheng, HP; Wen, W; Liu, X", "published_date": "March 1, 2018", "doi": "10.1016/j.vlsi.2017.11.001", "abstract": "© 2017 Elsevier B.V. Neuromorphic computing was originally referred to as the hardware that mimics neuro-biological architectures to implement models of neural systems. The concept was then extended to the computing systems that can run bio-inspired computing models, e.g., neural networks and deep learning networks. In recent years, the rapid growth of cognitive applications and the limited processing capability of conventional von Neumann architecture on these applications motivated worldwide research on neuromorphic computing systems. In this paper, we review the evolution of neuromorphic computing technique in both computing model and hardware implementation from a historical perspective. Various implementation methods and practices are also discussed. Finally, we present some emerging technologies that may potentially change the landscape of neuromorphic computing in the future, e.g., new devices and interdisciplinary computing architectures.", "publication_location": "Integration, the Vlsi Journal", "link": "http://dx.doi.org/10.1016/j.vlsi.2017.11.001", "citations": "11", "readership": "79", "tweets": "3", "news_mentions": " "},
{"title": "Convolutional encoder-decoder for breast mass segmentation in digital breast tomosynthesis", "authors": "Zhang, J; Ghate, SV; Grimm, LJ; Saha, A; Cain, EH; Zhu, Z; Mazurowski, MA", "published_date": "January 1, 2018", "doi": "10.1117/12.2295437", "abstract": "© 2018 SPIE. Digital breast tomosynthesis (DBT) is a relatively new modality for breast imaging that can provide detailed assessment of dense tissue within the breast. In the domains of cancer diagnosis, radiogenomics, and resident education, it is important to accurately segment breast masses. However, breast mass segmentation is a very challenging task, since mass regions have low contrast difference between their neighboring tissues. Notably, the task might become more difficult in cases that were assigned BI-RADS 0 category since this category includes many lesions that are of low conspicuity and locations that were deemed to be overlapping normal tissue upon further imaging and were not sent to biopsy. Segmentation of such lesions is of particular importance in the domain of reader performance analysis and education. In this paper, we propose a novel deep learning-based method for segmentation of BI-RADS 0 lesions in DBT. The key components of our framework are an encoding path for local-to-global feature extraction, and a decoding patch to expand the images. To address the issue of limited training data, in the training stage, we propose to sample patches not only in mass regions but also in non-mass regions. We utilize a Dice-like loss function in the proposed network to alleviate the class-imbalance problem. The preliminary results on 40 subjects show promise of our method. In addition to quantitative evaluation of the method, we present a visualization of the results that demonstrate both the performance of the algorithm as well as the difficulty of the task at hand.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2295437", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Breast Cancer Radiogenomics: Current Status and Future Directions.", "authors": "Grimm, LJ; Mazurowski, MA", "published_date": "January 2020", "doi": "10.1016/j.acra.2019.09.012", "abstract": "Radiogenomics is an area of research that aims to identify associations between imaging phenotypes (\"radio-\") and tumor genome (\"-genomics\"). Breast cancer radiogenomics research in particular has been an especially prolific area of investigation in recent years as evidenced by the wide number and variety of publications and conferences presentations. To date, research has primarily been focused on dynamic contrast enhanced pre-operative breast MRI and breast cancer molecular subtypes, but investigations have extended to all breast imaging modalities as well as multiple additional genetic markers including those that are commercially available. Furthermore, both human and computer-extracted features as well as deep learning techniques have been explored. This review will summarize the specific imaging modalities used in radiogenomics analysis, describe the methods of extracting imaging features, and present the types of genomics, molecular, and related information used for analysis. Finally, the limitations and future directions of breast cancer radiogenomics research will be discussed.", "publication_location": "Acad Radiol", "link": "http://dx.doi.org/10.1016/j.acra.2019.09.012", "citations": "5", "readership": "21", "tweets": "6", "news_mentions": " "},
{"title": "An Automated Grading System for Detection of Vision-Threatening Referable Diabetic Retinopathy on the Basis of Color Fundus Photographs.", "authors": "Li, Z; Keel, S; Liu, C; He, Y; Meng, W; Scheetz, J; Lee, PY; Shaw, J; Ting, D; Wong, TY; Taylor, H; Chang, R; He, M", "published_date": "December 2018", "doi": "10.2337/dc18-0147", "abstract": "OBJECTIVE: The goal of this study was to describe the development and validation of an artificial intelligence-based, deep learning algorithm (DLA) for the detection of referable diabetic retinopathy (DR). RESEARCH DESIGN AND METHODS: A DLA using a convolutional neural network was developed for automated detection of vision-threatening referable DR (preproliferative DR or worse, diabetic macular edema, or both). The DLA was tested by using a set of 106,244 nonstereoscopic retinal images. A panel of ophthalmologists graded DR severity in retinal photographs included in the development and internal validation data sets (n = 71,043); a reference standard grading was assigned once three graders achieved consistent grading outcomes. For external validation, we tested our DLA using 35,201 images of 14,520 eyes (904 eyes with any DR; 401 eyes with vision-threatening referable DR) from population-based cohorts of Malays, Caucasian Australians, and Indigenous Australians. RESULTS: Among the 71,043 retinal images in the training and validation data sets, 12,329 showed vision-threatening referable DR. In the internal validation data set, the area under the curve (AUC), sensitivity, and specificity of the DLA for vision-threatening referable DR were 0.989, 97.0%, and 91.4%, respectively. Testing against the independent, multiethnic data set achieved an AUC, sensitivity, and specificity of 0.955, 92.5%, and 98.5%, respectively. Among false-positive cases, 85.6% were due to a misclassification of mild or moderate DR. Undetected intraretinal microvascular abnormalities accounted for 77.3% of all false-negative cases. CONCLUSIONS: This artificial intelligence-based DLA can be used with high accuracy in the detection of vision-threatening referable DR in retinal images. This technology offers potential to increase the efficiency and accessibility of DR screening programs.", "publication_location": "Diabetes Care", "link": "http://dx.doi.org/10.2337/dc18-0147", "citations": "39", "readership": "53", "tweets": "4", "news_mentions": " "},
{"title": "Computer-Automated Malaria Diagnosis and Quantitation Using Convolutional Neural Networks", "authors": "Mehanian, C; Jaiswal, M; Delahunt, C; Thompson, C; Horning, M; Hu, L; McGuire, S; Ostbye, T; Mehanian, M; Wilson, B; Champlin, C; Long, E; Proux, S; Gamboa, D; Chiodini, P; Carter, J; Dhorda, M; Isaboke, D; Ogutu, B; Oyibo, W; Villasis, E; Tun, KM; Bachman, C; Bell, D", "published_date": "July 1, 2017", "doi": "10.1109/ICCVW.2017.22", "abstract": "© 2017 IEEE. The optical microscope remains a widely-used tool for diagnosis and quantitation of malaria. An automated system that can match the performance of well-trained technicians is motivated by a shortage of trained microscopists. We have developed a computer vision system that leverages deep learning to identify malaria parasites in micrographs of standard, field-prepared thick blood films. The prototype application diagnoses P. falciparum with sufficient accuracy to achieve competency level 1 in the World Health Organization external competency assessment, and quantitates with sufficient accuracy for use in drug resistance studies. A suite of new computer vision techniques-global white balance, adaptive nonlinear grayscale, and a novel augmentation scheme-underpin the system's state-of-the-art performance. We outline a rich, global training set; describe the algorithm in detail; argue for patient-level performance metrics for the evaluation of automated diagnosis methods; and provide results for P. falciparum.", "publication_location": "Proceedings   2017 Ieee International Conference on Computer Vision Workshops, Iccvw 2017", "link": "http://dx.doi.org/10.1109/ICCVW.2017.22", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A multi-scale framework with unsupervised joint training of convolutional neural networks for pulmonary deformable image registration.", "authors": "Jiang, Z; Yin, F-F; Ge, Y; Ren, L", "published_date": "January 13, 2020", "doi": "10.1088/1361-6560/ab5da0", "abstract": "To achieve accurate and fast deformable image registration (DIR) for pulmonary CT, we proposed a Multi-scale DIR framework with unsupervised Joint training of Convolutional Neural Network (MJ-CNN). MJ-CNN contains three models at multi-scale levels for a coarse-to-fine DIR to avoid being trapped in a local minimum. It is trained based on image similarity and deformation vector field (DVF) smoothness, requiring no supervision of ground-truth DVF. The three models are first trained sequentially and separately for their own registration tasks, and then are trained jointly for an end-to-end optimization under the multi-scale framework. In this study, MJ-CNN was trained using public SPARE 4D-CT data. The trained MJ-CNN was then evaluated on public DIR-LAB 4D-CT dataset as well as clinical CT-to-CBCT and CBCT-to-CBCT registration. For 4D-CT inter-phase registration, MJ-CNN achieved comparable accuracy to conventional iteration optimization-based methods, and showed the smallest registration errors compared to recently published deep learning-based DIR methods, demonstrating the efficacy of the proposed multi-scale joint training scheme. Besides, MJ-CNN trained using one dataset (SPARE) could generalize to a different dataset (DIR-LAB) acquired by different scanners and imaging protocols. Furthermore, MJ-CNN trained on 4D-CTs also performed well on CT-to-CBCT and CBCT-to-CBCT registration without any re-training or fine-tuning, demonstrating MJ-CNN's robustness against applications and imaging techniques. MJ-CNN took about 1.4 s for DVF estimation and required no manual-tuning of parameters during the evaluation. MJ-CNN is able to perform accurate DIR for pulmonary CT with nearly real-time speed, making it very applicable for clinical tasks.", "publication_location": "Phys Med Biol", "link": "http://dx.doi.org/10.1088/1361-6560/ab5da0", "citations": "1", "readership": "14", "tweets": "2", "news_mentions": " "},
{"title": "Will AI Improve Tumor Delineation Accuracy for Radiation Therapy?", "authors": "Chang, Z", "published_date": "June 2019", "doi": "10.1148/radiol.2019190385", "abstract": " ", "publication_location": "Radiology", "link": "http://dx.doi.org/10.1148/radiol.2019190385", "citations": "2", "readership": "11", "tweets": "2", "news_mentions": "2"},
{"title": "Artificial Intelligence in Radiotherapy Treatment Planning: Present and Future.", "authors": "Wang, C; Zhu, X; Hong, JC; Zheng, D", "published_date": "January 1, 2019", "doi": "10.1177/1533033819873922", "abstract": "Treatment planning is an essential step of the radiotherapy workflow. It has become more sophisticated over the past couple of decades with the help of computer science, enabling planners to design highly complex radiotherapy plans to minimize the normal tissue damage while persevering sufficient tumor control. As a result, treatment planning has become more labor intensive, requiring hours or even days of planner effort to optimize an individual patient case in a trial-and-error fashion. More recently, artificial intelligence has been utilized to automate and improve various aspects of medical science. For radiotherapy treatment planning, many algorithms have been developed to better support planners. These algorithms focus on automating the planning process and/or optimizing dosimetric trade-offs, and they have already made great impact on improving treatment planning efficiency and plan quality consistency. In this review, the smart planning tools in current clinical use are summarized in 3 main categories: automated rule implementation and reasoning, modeling of prior knowledge in clinical practice, and multicriteria optimization. Novel artificial intelligence-based treatment planning applications, such as deep learning-based algorithms and emerging research directions, are also reviewed. Finally, the challenges of artificial intelligence-based treatment planning are discussed for future works.", "publication_location": "Technology in Cancer Research & Treatment", "link": "http://dx.doi.org/10.1177/1533033819873922", "citations": "3", "readership": "46", "tweets": "10", "news_mentions": " "},
{"title": "Video generation from text", "authors": "Li, Y; Min, MR; Shen, D; Carlson, D; Carin, L", "published_date": "January 1, 2018", "doi": " ", "abstract": "Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Generating videos from text has proven to be a significant challenge for existing generative models. We tackle this problem by training a conditional generative model to extract both static and dynamic information from text. This is manifested in a hybrid framework, employing a Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN). The static features, called “gist,” are used to sketch text-conditioned background color and object layout structure. Dynamic features are considered by transforming input text into an image filter. To obtain a large amount of data for training the deep-learning model, we develop a method to automatically create a matched text-video corpus from publicly available online videos. Experimental results show that the proposed framework generates plausible and diverse short-duration smooth videos, while accurately reflecting the input text information. It significantly outperforms baseline models that directly adapt text-to-image generation procedures to produce videos. Performance is evaluated both visually and by adapting the inception score used to evaluate image generation in GANs.", "publication_location": "32nd Aaai Conference on Artificial Intelligence, Aaai 2018", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "DRASIC: Distributed recurrent autoencoder for scalable image compression", "authors": "Diao, E; Ding, J; Tarokh, V", "published_date": "March 1, 2020", "doi": "10.1109/DCC47342.2020.00008", "abstract": "© 2020 IEEE. We propose a new architecture for distributed image compression from a group of distributed data sources. The work is motivated by practical needs of data-driven codec design, low power consumption, robustness, and data privacy. The proposed architecture, which we refer to as Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC), is able to train distributed encoders and one joint decoder on correlated data sources. Its compression capability is much better than the method of training codecs separately. Meanwhile, the performance of our distributed system with 10 distributed sources is only within 2 dB peak signal-to-noise ratio (PSNR) of the performance of a single codec trained with all data sources. We experiment distributed sources with different correlations and show how our data-driven methodology well matches the Slepian-Wolf Theorem in Distributed Source Coding (DSC). To the best of our knowledge, this is the first data-driven DSC framework for general distributed code design with deep learning.", "publication_location": "Data Compression Conference Proceedings", "link": "http://dx.doi.org/10.1109/DCC47342.2020.00008", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "MimickNet, Matching Clinical Post-Processing under Realistic Black-Box Constraints", "authors": "Huang, O; Long, W; Bottenus, N; Trahey, GE; Farsiu, S; Palmeri, ML", "published_date": "October 1, 2019", "doi": "10.1109/ULTSYM.2019.8925597", "abstract": "© 2019 IEEE. Image post-processing is used in clinical-grade ultrasound scanners to improve image quality (e.g., reduce speckle noise and enhance contrast). These post-processing techniques vary across manufacturers and are generally kept proprietary, which presents a challenge for researchers looking to match current clinical-grade workflows. We introduce a deep learning framework, MimickNet, that transforms conventional delay-and-summed (DAS) beamformed images into the approximate post-processed images found on clinical-grade scanners. Training MimickNet only requires post-processed image samples from a scanner of interest without the need for explicit pairing to DAS data. Unpaired image flexibility allows MimckNet to hypothetically approximate any manufacturer's post-processing without hacking into commercial machines for pre-processed data. MimickNet generates images with an average similarity index measurement (SSIM) of 0.930±0.0892 on a 300 cineloop test set, and it generalizes to cardiac cineloops achieving an SSIM of 0.967±0.002 despite using no cardiac data in the training process. To our knowledge, this is the first work to approximate current clinical-grade ultrasound post-processing under realistic black-box constraints where before and after post-processing data is unavailable. MimickNet can be used out of the box or retrained to serve as a clinical post-processing baseline to compare against for future works in ultrasound image formation. To this end, we have made the MimickNet software open source at https://github.com/ouwen/mimicknet.", "publication_location": "Ieee International Ultrasonics Symposium, Ius", "link": "http://dx.doi.org/10.1109/ULTSYM.2019.8925597", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "On the stability of deep networks", "authors": "Giryes, R; Sapiro, G; Bronstein, AM", "published_date": "January 1, 2015", "doi": " ", "abstract": "© 2015 International Conference on Learning Representations, ICLR. All rights reserved. In this work we study the properties of deep neural networks (DNN) with random weights. We formally prove that these networks perform a distance-preserving embedding of the data. Based on this we then draw conclusions on the size of the training data and the networks’ structure. A longer version of this paper with more results and details can be found in (Giryes et al., 2015). In particular, we formally prove in (Giryes et al., 2015) that DNN with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data.", "publication_location": "3rd International Conference on Learning Representations, Iclr 2015   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "On the stability of deep networks", "authors": "Giryes, R; Sapiro, G; Bronstein, AM", "published_date": "January 1, 2015", "doi": " ", "abstract": "© 2015 International Conference on Learning Representations, ICLR. All rights reserved. In this work we study the properties of deep neural networks (DNN) with random weights. We formally prove that these networks perform a distance-preserving embedding of the data. Based on this we then draw conclusions on the size of the training data and the networks’ structure. A longer version of this paper with more results and details can be found in (Giryes et al., 2015). In particular, we formally prove in (Giryes et al., 2015) that DNN with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data.", "publication_location": "3rd International Conference on Learning Representations, Iclr 2015   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Motivation, learning, and transformative experience: A study of deep engagement in science", "authors": " ", "published_date": "2010", "doi": "10.1002/sce.20344", "abstract": "This study investigated the prevalence of transformative experiences, antecedents of transformative experience, and the relation between transformative experience and deep-level learning (conceptual change and transfer) for high school biology students (N = 166). Results suggested that the high school students in our sample typically engaged in low levels of transformative experience with respect to biology, but those students who strongly identified with science and who endorsed a mastery goal orientation were more likely to report engagement in higher levels of transformative experience. Furthermore, a higher level of engagement in transformative experience was positively associated with (a) conceptual change in understanding the concept of natural selection, but not inheritance, at the post- and follow-up assessments and (b) transfer at the follow-up assessment. © 2009 Wiley Periodicals, Inc.", "publication_location": "Sci Educ", "link": "http://dx.doi.org/10.1002/sce.20344", "citations": "81", "readership": "240", "tweets": "1", "news_mentions": " "},
{"title": "Constrained attractor selection using deep reinforcement learning", "authors": "Wang, XS; Turner, JD; Mann, BP", "published_date": "January 1, 2020", "doi": "10.1177/1077546320930144", "abstract": "© The Author(s) 2020. This study describes an approach for attractor selection (or multistability control) in nonlinear dynamical systems with constrained actuation. Attractor selection is obtained using two different deep reinforcement learning methods: (1) the cross-entropy method and (2) the deep deterministic policy gradient method. The framework and algorithms for applying these control methods are presented. Experiments were performed on a Duffing oscillator, as it is a classic nonlinear dynamical system with multiple attractors. Both methods achieve attractor selection under various control constraints. Although these methods have nearly identical success rates, the deep deterministic policy gradient method has the advantages of a high learning rate, low performance variance, and a smooth control approach. This study demonstrates the ability of two reinforcement learning approaches to achieve constrained attractor selection.", "publication_location": "Journal of Vibration and Control", "link": "http://dx.doi.org/10.1177/1077546320930144", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning-Based Control Design for Deep Brain Stimulation", "authors": "Jovanov, I; Nauman, M; Kumaravelu, K; Lesi, V; Zutshi, A; Grill, WM; Pajic, M", "published_date": "August 21, 2018", "doi": "10.1109/ICCPS.2018.00048", "abstract": "© 2018 IEEE. By employing low-voltage electrical stimulation of the basal ganglia (BG) regions of the brain, deep brain stimulation (DBS) devices are used to alleviate the symptoms of several neurological disorders, including Parkinson's disease (PD). Recently, we have developed a Basal Ganglia Model (BGM) that can be utilized for design and evaluation of DBS devices. In this work, we focus on the use of a hardware (FPGA) implementation of the BGM platform to facilitate development of new control policies. Specifically, we introduce a design-time framework that allows for development of suitable control policies, in the form of electrical pulses with variable temporal patterns, while supporting tradeoffs between energy efficiency and efficacy (i.e., Quality-of-Control) of the therapy. The developed framework exploits machine learning and optimization based methods for design-space exploration where predictive behavior for any control configuration (i.e., temporal pattern) is obtained using the BGM platform that simulates physiological response to the considered control in real-time. To illustrate the use of the developed framework, in our demonstration we present how the BGM can be utilized for physiologically relevant BG modeling and design-state exploration for DBS controllers, as well as show the effectiveness of obtained controllers that significantly outperform conventional DBS controllers.", "publication_location": "Proceedings   9th Acm/Ieee International Conference on Cyber Physical Systems, Iccps 2018", "link": "http://dx.doi.org/10.1109/ICCPS.2018.00048", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Tensor-dictionary learning with deep Kruskal-factor analysis", "authors": "Stevens, A; Pu, Y; Sun, Y; Spell, G; Carin, L", "published_date": "January 1, 2017", "doi": " ", "abstract": "Copyright 2017 by the author(s). A multi-way factor analysis model is introduced for tensor-variate data of any order. Each data item is represented as a (sparse) sum of Kruskal decompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can infer both the tensor-rank of each dictionary atom and the number of dictionary atoms. The model is adapted for online learning, which allows dictionary learning on large data sets. After KFA is introduced, the model is extended to a deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The experiments section demonstrates the improvement of KFA over vectorized approaches (e.g., BPFA), tensor decompositions, and convolutional neural networks (CNN) in multi-way denoising, blind inpainting, and image classification. The improvement in PSNR for the inpainting results over other methods exceeds 1dB in several cases and we achieve state of the art results on Caltech101 image classification.", "publication_location": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, Aistats 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Tensor-dictionary learning with deep Kruskal-factor analysis", "authors": "Stevens, A; Pu, Y; Sun, Y; Spell, G; Carin, L", "published_date": "January 1, 2017", "doi": " ", "abstract": "Copyright 2017 by the author(s). A multi-way factor analysis model is introduced for tensor-variate data of any order. Each data item is represented as a (sparse) sum of Kruskal decompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can infer both the tensor-rank of each dictionary atom and the number of dictionary atoms. The model is adapted for online learning, which allows dictionary learning on large data sets. After KFA is introduced, the model is extended to a deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The experiments section demonstrates the improvement of KFA over vectorized approaches (e.g., BPFA), tensor decompositions, and convolutional neural networks (CNN) in multi-way denoising, blind inpainting, and image classification. The improvement in PSNR for the inpainting results over other methods exceeds 1dB in several cases and we achieve state of the art results on Caltech101 image classification.", "publication_location": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, Aistats 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning deep sigmoid belief networks with data augmentation", "authors": "Gan, Z; Henao, R; Carlson, D; Carin, L", "published_date": "January 1, 2015", "doi": " ", "abstract": "Copyright 2015 by the authors. Deep directed generative models are developed. The multi-layered model is designed by stacking sigmoid belief networks, with sparsity-encouraging priors placed on the model parameters. Learning and inference of layer-wise model parameters are implemented in a Bayesian setting. By exploring the idea of data augmentation and introducing auxiliary Polya-Gamma variables, simple and efficient Gibbs sampling and mean-field variational Bayes (VB) inference are implemented. To address large-scale datasets, an online version of VB is also developed. Experimental results are presented for three publicly available datasets: MNIST, Caltech 101 Silhouettes and OCR letters.", "publication_location": "Journal of Machine Learning Research", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning better deep features for the prediction of occult invasive disease in ductal carcinoma in situ through transfer learning", "authors": "Shi, B; Hou, R; Mazurowski, MA; Grimm, LJ; Ren, Y; Marks, JR; King, LM; Maley, CC; Hwang, ES; Lo, JY", "published_date": "January 1, 2018", "doi": "10.1117/12.2293594", "abstract": "© 2018 SPIE. Purpose: To determine whether domain transfer learning can improve the performance of deep features extracted from digital mammograms using a pre-trained deep convolutional neural network (CNN) in the prediction of occult invasive disease for patients with ductal carcinoma in situ (DCIS) on core needle biopsy. Method: In this study, we collected digital mammography magnification views for 140 patients with DCIS at biopsy, 35 of which were subsequently upstaged to invasive cancer. We utilized a deep CNN model that was pre-trained on two natural image data sets (ImageNet and DTD) and one mammographic data set (INbreast) as the feature extractor, hypothesizing that these data sets are increasingly more similar to our target task and will lead to better representations of deep features to describe DCIS lesions. Through a statistical pooling strategy, three sets of deep features were extracted using the CNNs at different levels of convolutional layers from the lesion areas. A logistic regression classifier was then trained to predict which tumors contain occult invasive disease. The generalization performance was assessed and compared using repeated random sub-sampling validation and receiver operating characteristic (ROC) curve analysis. Result: The best performance of deep features was from CNN model pre-trained on INbreast, and the proposed classifier using this set of deep features was able to achieve a median classification performance of ROC-AUC equal to 0.75, which is significantly better (p<=0.05) than the performance of deep features extracted using ImageNet data set (ROCAUC = 0.68). Conclusion: Transfer learning is helpful for learning a better representation of deep features, and improves the prediction of occult invasive disease in DCIS.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2293594", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "The cerebellum: a neuronal learning machine?", "authors": "Raymond, JL; Lisberger, SG; Mauk, MD", "published_date": "May 24, 1996", "doi": "10.1126/science.272.5265.1126", "abstract": "Comparison of two seemingly quite different behaviors yields a surprisingly consistent picture of the role of the cerebellum in motor learning. Behavioral and physiological data about classical conditioning of the eyelid response and motor learning in the vestibulo-ocular reflex suggests that (i) plasticity is distributed between the cerebellar cortex and the deep cerebellar nuclei; (ii) the cerebellar cortex plays a special role in learning the timing of movement; and (iii) the cerebellar cortex guides learning in the deep nuclei, which may allow learning to be transferred from the cortex to the deep nuclei. Because many of the similarities in the data from the two systems typify general features of cerebellar organization, the cerebellar mechanisms of learning in these two systems may represent principles that apply to many motor systems.", "publication_location": "Science (New York, N.Y.)", "link": "http://dx.doi.org/10.1126/science.272.5265.1126", "citations": "476", "readership": "380", "tweets": " ", "news_mentions": " "},
{"title": "Model-based design of closed loop deep brain stimulation controller using reinforcement learning", "authors": "Gao, Q; Naumann, M; Jovanov, I; Lesi, V; Kamaravelu, K; Grill, WM; Pajic, M", "published_date": "April 1, 2020", "doi": "10.1109/ICCPS48487.2020.00018", "abstract": "© 2020 IEEE. Parkinson's disease (PD) currently Influences around one million people in the US. Deep brain stimulation (DBS) is a surgical treatment for the motor symptoms of PD that delivers electrical stimulation to the basal ganglia (BG) region of the brain. Existing commercial DBS devices employ stimulation based only on fixed-frequency periodic pulses. While such periodic high-frequency DBS controllers provide effective relief of PD symptoms, they are very inefficient in terms of energy consumption, and the lifetime of these battery- operated devices is limited to 4 years. Furthermore, fixed high- frequency stimulation may have side effects, such as speech impairment. Consequently, there is a need to move beyond (1) fixed stimulation pulse controllers, and (2) 'one-size-fits- all' patient-agnostic treatments, to provide energy efficient and effective (in terms of relieving PD symptoms) DBS controllers. In this work, we introduce a deep reinforcement learning (RL)- based approach that can derive patient-specific DBS patterns that are both effective in reducing a model-based proxy for PD symptoms, as well as energy-efficient. Specifically, we model the BG regions as a Markov decision process (MDP), and define the state and action space as state of the neurons in the BG regions and the stimulation patterns, respectively. Thereafter, we define the reward functions over the state space, and the learning objective is set to maximize the accumulated reward over a finite horizon (i.e., the treatment duration), while bounding average stimulation frequency. We evaluate the performance of our methodology using a Brain-on-Chip (BoC) FPGA platform that implements the physiologically-relevant basal ganglia model (BGM). We show that our RL-based DBS controllers significantly outperform existing fixed frequency controllers in terms of energy efficiency (e.g., by using 70% less energy than common periodic controllers), while providing suitable reduction of model-based proxy for PD symptoms.", "publication_location": "Proceedings   2020 Acm/Ieee 11th International Conference on Cyber Physical Systems, Iccps 2020", "link": "http://dx.doi.org/10.1109/ICCPS48487.2020.00018", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Zero-shot learning via class-conditioned deep generative models", "authors": "Wang, W; Pu, Y; Verma, VK; Fan, K; Zhang, Y; Chen, C; Rai, P; Carin, L", "published_date": "January 1, 2018", "doi": " ", "abstract": "Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. We present a deep generative model for Zero-Shot Learning (ZSL). Unlike most existing methods for this problem, that represent each class as a point (via a semantic embedding), we represent each seen/unseen class using a class-specific latent-space distribution, conditioned on class attributes. We use these latent-space distributions as a prior for a supervised variational autoencoder (VAE), which also facilitates learning highly discriminative feature representations for the inputs. The entire framework is learned end-to-end using only the seen-class training data. At test time, the label for an unseen-class test input is the class that maximizes the VAE lower bound. We further extend the model to a (i) semi-supervised/transductive setting by leveraging unlabeled unseen-class data via an unsupervised learning module, and (ii) few-shot learning where we also have a small number of labeled inputs from the unseen classes. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of benchmark data sets.", "publication_location": "32nd Aaai Conference on Artificial Intelligence, Aaai 2018", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep hypothermic circulatory arrest for complex cerebral aneurysms: Lessons learned - Commentary", "authors": " ", "published_date": "2007", "doi": "10.1227/01.NEU.0000255452.20602.C9", "abstract": " ", "publication_location": "Neurosurgery", "link": "http://dx.doi.org/10.1227/01.NEU.0000255452.20602.C9", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A deep generative deconvolutional image model", "authors": "Pu, Y; Yuan, X; Stevens, A; Li, C; Carin, L", "published_date": "January 1, 2016", "doi": " ", "abstract": "Copyright 2016 by the authors. A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic unpooling is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm; the algorithm is implemented on graphical processor units (GPU) to enable large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.", "publication_location": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, Aistats 2016", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Linear feature encoding for reinforcement learning", "authors": "Song, Z; Parr, R; Liao, X; Carin, L", "published_date": "January 1, 2016", "doi": " ", "abstract": "© 2016 NIPS Foundation - All Rights Reserved. Feature construction is of vital importance in reinforcement learning, as the quality of a value function or policy is largely determined by the corresponding features. The recent successes of deep reinforcement learning (RL) only increase the importance of understanding feature construction. Typical deep RL approaches use a linear output layer, which means that deep RL can be interpreted as a feature construction/encoding network followed by linear value function approximation. This paper develops and evaluates a theory of linear feature encoding. We extend theoretical results on feature quality for linear value function approximation from the uncontrolled case to the controlled case. We then develop a supervised linear feature encoding method that is motivated by insights from linear value function approximation theory, as well as empirical successes from deep RL. The resulting encoder is a surprisingly effective method for linear value function approximation using raw images as inputs.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "DCFNet: Deep Neural Network with Decomposed Convolutional Filters", "authors": "Qiu, Q; Cheng, X; Calderbank, R; Sapiro, G", "published_date": "January 1, 2018", "doi": " ", "abstract": "©35th International Conference on Machine Learning, ICML 2018.All Rights Reserved. Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation. Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients. The analysis is consistent with the empirical observations.", "publication_location": "35th International Conference on Machine Learning, Icml 2018", "link": "https://hdl.handle.net/10161/17837", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Discriminative robust transformation learning", "authors": "Huang, J; Qiu, Q; Sapiro, G; Calderbank, R", "published_date": "January 1, 2015", "doi": " ", "abstract": "This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of training samples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K, ∈)-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization framework is used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild, demonstrate the value of being able to balance discrimination and robustness.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Ecological restoration in the deep sea: Desiderata", "authors": "Van Dover, CL; Aronson, J; Pendleton, L; Smith, S; Arnaud-Haond, S; Moreno-Mateos, D; Barbier, E; Billett, D; Bowers, K; Danovaro, R; Edwards, A; Kellert, S; Morato, T; Pollard, E; Rogers, A; Warner, R", "published_date": "February 1, 2014", "doi": "10.1016/j.marpol.2013.07.006", "abstract": "An era of expanding deep-ocean industrialization is before us, with policy makers establishing governance frameworks for sustainable management of deep-sea resources while scientists learn more about the ecological structure and functioning of the largest biome on the planet. Missing from discussion of the stewardship of the deep ocean is ecological restoration. If existing activities in the deep sea continue or are expanded and new deep-ocean industries are developed, there is need to consider what is required to minimize or repair resulting damages to the deep-sea environment. In addition, thought should be given as to how any past damage can be rectified. This paper develops the discourse on deep-sea restoration and offers guidance on planning and implementing ecological restoration projects for deep-sea ecosystems that are already, or are at threat of becoming, degraded, damaged or destroyed. Two deep-sea restoration case studies or scenarios are described (deep-sea stony corals on the Darwin Mounds off the west coast of Scotland, deep-sea hydrothermal vents in Manus Basin, Papua New Guinea) and are contrasted with on-going saltmarsh restoration in San Francisco Bay. For these case studies, a set of socio-economic, ecological, and technological decision parameters that might favor (or not) their restoration are examined. Costs for hypothetical restoration scenarios in the deep sea are estimated and first indications suggest they may be two to three orders of magnitude greater per hectare than costs for restoration efforts in shallow-water marine systems. © 2013 The Authors.", "publication_location": "Marine Policy", "link": "http://dx.doi.org/10.1016/j.marpol.2013.07.006", "citations": "83", "readership": "302", "tweets": "4", "news_mentions": " "},
{"title": "RoTDCF: Decomposition of convolutional filters for rotation-equivariant deep networks", "authors": "Cheng, X; Qiu, Q; Calderbank, R; Sapiro, G", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved. Explicit encoding of group actions in deep features makes it possible for convolutional neural networks (CNNs) to handle global deformations of images, which is critical to success in many vision tasks. This paper proposes to decompose the convolutional filters over joint steerable bases across the space and the group geometry simultaneously, namely a rotation-equivariant CNN with decomposed convolutional filters (RotDCF). This decomposition facilitates computing the joint convolution, which is proved to be necessary for the group equivariance. It significantly reduces the model size and computational complexity while preserving performance, and truncation of the bases expansion serves implicitly to regularize the filters. On datasets involving in-plane and out-of-plane object rotations, RotDCF deep features demonstrate greater robustness and interpretability than regular CNNs. The stability of the equivariant representation to input variations is also proved theoretically. The RotDCF framework can be extended to groups other than rotations, providing a general approach which achieves both group equivariance and representation stability at a reduced model size.", "publication_location": "7th International Conference on Learning Representations, Iclr 2019", "link": "https://hdl.handle.net/10161/17836", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "RoTDCF: Decomposition of convolutional filters for rotation-equivariant deep networks", "authors": "Cheng, X; Qiu, Q; Calderbank, R; Sapiro, G", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved. Explicit encoding of group actions in deep features makes it possible for convolutional neural networks (CNNs) to handle global deformations of images, which is critical to success in many vision tasks. This paper proposes to decompose the convolutional filters over joint steerable bases across the space and the group geometry simultaneously, namely a rotation-equivariant CNN with decomposed convolutional filters (RotDCF). This decomposition facilitates computing the joint convolution, which is proved to be necessary for the group equivariance. It significantly reduces the model size and computational complexity while preserving performance, and truncation of the bases expansion serves implicitly to regularize the filters. On datasets involving in-plane and out-of-plane object rotations, RotDCF deep features demonstrate greater robustness and interpretability than regular CNNs. The stability of the equivariant representation to input variations is also proved theoretically. The RotDCF framework can be extended to groups other than rotations, providing a general approach which achieves both group equivariance and representation stability at a reduced model size.", "publication_location": "7th International Conference on Learning Representations, Iclr 2019", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Scalable deep Poisson factor analysis for topic modeling", "authors": "Gan, Z; Chen, C; Henao, R; Carlson, D; Carin, L", "published_date": "January 1, 2015", "doi": " ", "abstract": "© Copyright 2015 by International Machine Learning Society (IMLS). All rights reserved. A new framework for topic modeling is developed, based on deep graphical models, where interactions between topics are inferred through deep latent binary hierarchies. The proposed multi-layer model employs a deep sigmoid belief network or restricted Boltzmann machine, the bottom binary layer of which selects topics for use in a Poisson factor analysis model. Under this setting, topics live on the bottom layer of the model, while the deep specification serves as a flexible prior for revealing topic structure. Scalable inference algorithms are derived by applying Bayesian conditional density filtering algorithm, in addition to extending recently proposed work on stochastic gradient thermostats. Experimental results on several corpora show that the proposed approach readily handles very large collections of text documents, infers structured topic representations, and obtains superior test perplexities when compared with related models.", "publication_location": "32nd International Conference on Machine Learning, Icml 2015", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Stronger generalization bounds for deep nets via a compression approach", "authors": "Arora, S; Ge, R; Neyshabur, B; Zhang, Y", "published_date": "January 1, 2018", "doi": " ", "abstract": "© Copyright 2018 by the Authors. All rights reserved. Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that are orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net - a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified \"noise stability\"properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.", "publication_location": "35th International Conference on Machine Learning, Icml 2018", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep generative models for relational data with side information", "authors": "Hu, C; Rai, P; Carin, L", "published_date": "January 1, 2017", "doi": " ", "abstract": "© Copyright 2017 by the author(s). We present a probabilistic framework for overlapping community discovery and link prediction for relational data, given as a graph. The proposed framework has: (1) a deep architecture which enables us to infer multiple layers of latent features/communities for each node, providing superior link prediction performance on more complex networks and better interpretability of the latent features; and (2) a regression model which allows directly conditioning the node latent features on the side information available in form of node attributes. Our framework handles both (1) and (2) via a clean, unified model, which enjoys full local conjugacy via data augmentation, and facilitates efficient inference via closed form Gibbs sampling. Moreover, inference cost scales in the number of edges which is attractive for massive but sparse networks. Our framework is also easily extendable to model weighted networks with count-valued edges. We compare with various state-of-the-art methods and report results, both quantitative and qualitative, on several benchmark data sets.", "publication_location": "34th International Conference on Machine Learning, Icml 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Electronic health record analysis via deep poisson factor models", "authors": "Henao, R; Lu, JT; Lucas, JE; Ferranti, J; Carin, L", "published_date": "April 1, 2016", "doi": " ", "abstract": "© 2016 Ricardo Henao, James T. Lu, Joseph E. Lucas, Je rey Ferranti and Lawrence Carin. Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli-Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial.", "publication_location": "Journal of Machine Learning Research", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Validation of a fully automated liver segmentation algorithm using multi-scale deep reinforcement learning and comparison versus manual segmentation.", "authors": " ", "published_date": "May 2020", "doi": "10.1016/j.ejrad.2020.108918", "abstract": "PURPOSE: To evaluate the performance of an artificial intelligence (AI) based software solution tested on liver volumetric analyses and to compare the results to the manual contour segmentation. MATERIALS AND METHODS: We retrospectively obtained 462 multiphasic CT datasets with six series for each patient: three different contrast phases and two slice thickness reconstructions (1.5/5 mm), totaling 2772 series. AI-based liver volumes were determined using multi-scale deep-reinforcement learning for 3D body markers detection and 3D structure segmentation. The algorithm was trained for liver volumetry on approximately 5000 datasets. We computed the absolute error of each automatically- and manually-derived volume relative to the mean manual volume. The mean processing time/dataset and method was recorded. Variations of liver volumes were compared using univariate generalized linear model analyses. A subgroup of 60 datasets was manually segmented by three radiologists, with a further subgroup of 20 segmented three times by each, to compare the automatically-derived results with the ground-truth. RESULTS: The mean absolute error of the automatically-derived measurement was 44.3 mL (representing 2.37 % of the averaged liver volumes). The liver volume was neither dependent on the contrast phase (p = 0.697), nor on the slice thickness (p = 0.446). The mean processing time/dataset with the algorithm was 9.94 s (sec) compared to manual segmentation with 219.34 s. We found an excellent agreement between both approaches with an ICC value of 0.996. CONCLUSION: The results of our study demonstrate that AI-powered fully automated liver volumetric analyses can be done with excellent accuracy, reproducibility, robustness, speed and agreement with the manual segmentation.", "publication_location": "Eur J Radiol", "link": "http://dx.doi.org/10.1016/j.ejrad.2020.108918", "citations": " ", "readership": " ", "tweets": "1", "news_mentions": " "},
{"title": "Automated feature learning using deep convolutional auto-encoder neural network for clustering electroencephalograms into sleep stages", "authors": "Prabhudesai, KS; Collins, LM; Mainsah, BO", "published_date": "May 16, 2019", "doi": "10.1109/NER.2019.8716996", "abstract": "© 2019 IEEE. Deep neural networks have emerged as popular machine learning tools due to their ability to automatically learn feature representations from raw input data. An auto-encoder neural network is a special network that can be trained in an unsupervised manner for automated feature learning. Unsupervised analysis of EEG signals is highly desirable since supervised analysis requires manual labeling of EEG signals which can be labor intensive and time consuming given the large amount of EEG data collected. We present a deep convolutional auto-encoder neural network to automatically learn feature representations from raw EEG signals in an unsupervised manner. We use the features extracted from the auto-encoder neural network for clustering EEG signals into sleep stages. For clustering, we test two algorithms: K-means - which is a single-membership model, and the latent Dirichlet allocation (LDA) topic model - which is a mixed membership model. Results are presented demonstrating an improvement in clustering performance using auto-encoder features compared to standard manually extracted features.", "publication_location": "International Ieee/Embs Conference on Neural Engineering, Ner", "link": "http://dx.doi.org/10.1109/NER.2019.8716996", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Automatic localization of the subthalamic nucleus on patient-specific clinical MRI by incorporating 7 T MRI and machine learning: Application in deep brain stimulation.", "authors": "Kim, J; Duchin, Y; Shamir, RR; Patriat, R; Vitek, J; Harel, N; Sapiro, G", "published_date": "February 2019", "doi": "10.1002/hbm.24404", "abstract": "Deep brain stimulation (DBS) of the subthalamic nucleus (STN) has shown clinical potential for relieving the motor symptoms of advanced Parkinson's disease. While accurate localization of the STN is critical for consistent across-patients effective DBS, clear visualization of the STN under standard clinical MR protocols is still challenging. Therefore, intraoperative microelectrode recordings (MER) are incorporated to accurately localize the STN. However, MER require significant neurosurgical expertise and lengthen the surgery time. Recent advances in 7 T MR technology facilitate the ability to clearly visualize the STN. The vast majority of centers, however, still do not have 7 T MRI systems, and fewer have the ability to collect and analyze the data. This work introduces an automatic STN localization framework based on standard clinical MRIs without additional cost in the current DBS planning protocol. Our approach benefits from a large database of 7 T MRI and its clinical MRI pairs. We first model in the 7 T database, using efficient machine learning algorithms, the spatial and geometric dependency between the STN and its adjacent structures (predictors). Given a standard clinical MRI, our method automatically computes the predictors and uses the learned information to predict the patient-specific STN. We validate our proposed method on clinical T2 W MRI of 80 subjects, comparing with experts-segmented STNs from the corresponding 7 T MRI pairs. The experimental results show that our framework provides more accurate and robust patient-specific STN localization than using state-of-the-art atlases. We also demonstrate the clinical feasibility of the proposed technique assessing the post-operative electrode active contact locations.", "publication_location": "Human Brain Mapping", "link": "http://dx.doi.org/10.1002/hbm.24404", "citations": "6", "readership": "38", "tweets": "2", "news_mentions": " "},
{"title": "Principles of operation of a cerebellar learning circuit.", "authors": "Herzfeld, DJ; Hall, NJ; Tringides, M; Lisberger, SG", "published_date": "April 30, 2020", "doi": "10.7554/elife.55217", "abstract": "We provide behavioral evidence using monkey smooth pursuit eye movements for four principles of cerebellar learning. Using a circuit-level model of the cerebellum, we link behavioral data to learning's neural implementation. The four principles are: (1) early, fast, acquisition driven by climbing fiber inputs to the cerebellar cortex, with poor retention; (2) learned responses of Purkinje cells guide transfer of learning from the cerebellar cortex to the deep cerebellar nucleus, with excellent retention; (3) functionally different neural signals are subject to learning in the cerebellar cortex versus the deep cerebellar nuclei; and (4) negative feedback from the cerebellum to the inferior olive reduces the magnitude of the teaching signal in climbing fibers and limits learning. Our circuit-level model, based on these four principles, explains behavioral data obtained by strategically manipulating the signals responsible for acquisition and recall of direction learning in smooth pursuit eye movements across multiple timescales.", "publication_location": "Elife", "link": "http://dx.doi.org/10.7554/elife.55217", "citations": "3", "readership": " ", "tweets": "28", "news_mentions": " "},
{"title": "Removing the target network from deep Q-networks with the mellowmax operator", "authors": "Kim, S; Asadi, K; Littman, M; Konidaris, G", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. Deep Q-Network (DQX) is a learning algorithm that achieves humanlevel performance in high-dimensional domains like Atari games. We propose that using an softmax operator, Mellowmax, in DQN reduces its need for a separate target network, which is otherwise necessary to stabilize learning. We empirically show that, in the absence of a target network, the combination of Mellowmax and DQN outperforms DQN alone.", "publication_location": "Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, Aamas", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?", "authors": "Giryes, R; Sapiro, G; Bronstein, AM", "published_date": "July 1, 2016", "doi": "10.1109/TSP.2016.2546221", "abstract": "© 1991-2012 IEEE. Three important properties of a classification machinery are i) the system preserves the core information of the input data; ii) the training examples convey information about unseen data; and iii) the system is able to treat differently points from different classes. In this paper, we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.", "publication_location": "Ieee Transactions on Signal Processing", "link": "http://dx.doi.org/10.1109/TSP.2016.2546221", "citations": "39", "readership": "194", "tweets": "22", "news_mentions": " "},
{"title": "Variational learning of individual survival distributions", "authors": "Xiu, Z; Tao, C; University, D; Henao, R", "published_date": "February 4, 2020", "doi": "10.1145/3368555.3384454", "abstract": "© 2020 ACM. The abundance of modern health data provides many opportunities for the use of machine learning techniques to build better statistical models to improve clinical decision making. Predicting time-to-event distributions, also known as survival analysis, plays a key role in many clinical applications. We introduce a variational time-to-event prediction model, named Variational Survival Inference (VSI), which builds upon recent advances in distribution learning techniques and deep neural networks. VSI addresses the challenges of non-parametric distribution estimation by ($i$) relaxing the restrictive modeling assumptions made in classical models, and ($ii$) efficiently handling the censored observations, i.e. events that occur outside the observation window, all within the variational framework. To validate the effectiveness of our approach, an extensive set of experiments on both synthetic and real-world datasets is carried out, showing improved performance relative to competing solutions.", "publication_location": "Acm Chil 2020   Proceedings of the 2020 Acm Conference on Health, Inference, and Learning", "link": "http://dx.doi.org/10.1145/3368555.3384454", "citations": " ", "readership": " ", "tweets": "7", "news_mentions": " "},
{"title": "Deep temporal sigmoid belief networks for sequence modeling", "authors": "Gan, Z; Li, C; Henao, R; Carlson, D; Carin, L", "published_date": "January 1, 2015", "doi": " ", "abstract": "Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Classification of crystallization outcomes using deep convolutional neural networks.", "authors": " ", "published_date": "January 2018", "doi": "10.1371/journal.pone.0198883", "abstract": "The Machine Recognition of Crystallization Outcomes (MARCO) initiative has assembled roughly half a million annotated images of macromolecular crystallization experiments from various sources and setups. Here, state-of-the-art machine learning algorithms are trained and tested on different parts of this data set. We find that more than 94% of the test images can be correctly labeled, irrespective of their experimental origin. Because crystal recognition is key to high-density screening and the systematic analysis of crystallization experiments, this approach opens the door to both industrial and fundamental research applications.", "publication_location": "Plos One", "link": "http://dx.doi.org/10.1371/journal.pone.0198883", "citations": "15", "readership": "65", "tweets": "27", "news_mentions": "3"},
{"title": "A Survey of Accelerator Architectures for Deep Neural Networks", "authors": " ", "published_date": "March 1, 2020", "doi": "10.1016/j.eng.2020.01.007", "abstract": "© 2020 Recently, due to the availability of big data and the rapid growth of computing power, artificial intelligence (AI) has regained tremendous attention and investment. Machine learning (ML) approaches have been successfully applied to solve many problems in academia and in industry. Although the explosion of big data applications is driving the development of ML, it also imposes severe challenges of data processing speed and scalability on conventional computer systems. Computing platforms that are dedicatedly designed for AI applications have been considered, ranging from a complement to von Neumann platforms to a “must-have” and stand-alone technical solution. These platforms, which belong to a larger category named “domain-specific computing,” focus on specific customization for AI. In this article, we focus on summarizing the recent advances in accelerator designs for deep neural networks (DNNs)—that is, DNN accelerators. We discuss various architectures that support DNN executions in terms of computing units, dataflow optimization, targeted network topologies, architectures on emerging technologies, and accelerators for emerging applications. We also provide our visions on the future trend of AI chip designs.", "publication_location": "Engineering", "link": "http://dx.doi.org/10.1016/j.eng.2020.01.007", "citations": "1", "readership": " ", "tweets": "3", "news_mentions": " "},
{"title": "Recom: An efficient resistive accelerator for compressed deep neural networks", "authors": "Ji, H; Song, L; Jiang, L; Li, HH; Chen, Y", "published_date": "April 19, 2018", "doi": "10.23919/DATE.2018.8342009", "abstract": "© 2018 EDAA. Deep Neural Networks (DNNs) play a key role in prevailing machine learning applications. Resistive random-Access memory (ReRAM) is capable of both computation and storage, contributing to the acceleration on DNNs by processing in memory. Besides, a significant amount of zero weights is observed in DNNs, providing a space to reduce computation cost further by skipping ineffectual calculations associated with them. However, the irregular distribution of zero weights in DNNs makes it difficult for resistive accelerators to take advantage of the sparsity as expected efficiently, because of its high reliance on regular matrix-vector multiplication in ReRAM. In this work, we propose ReCom, the first resistive accelerator to support sparse DNN processing. ReCom is an efficient resistive accelerator for compressed deep neural networks, where DNN weights are structurally compressed to eliminate zero parameters and become hardware-friendly. Zero DNN activation is also considered at the same time. Two technologies, Structurally-compressed Weight Oriented Fetching (SWOF) and In-layer Pipeline for Memory and Computation (IPMC), are particularly proposed. In our evaluation, ReCom can achieve 3.37x speedup and 2.41x energy efficiency compared to a state-of-The-Art resistive accelerator.", "publication_location": "Proceedings of the 2018 Design, Automation and Test in Europe Conference and Exhibition, Date 2018", "link": "http://dx.doi.org/10.23919/DATE.2018.8342009", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Malignant microcalcification clusters detection using unsupervised deep autoencoders", "authors": "Hou, R; Ren, Y; Grimm, LJ; Mazurowski, MA; Marks, JR; King, L; Maley, CC; Shelley Hwang, E; Lo, JY", "published_date": "January 1, 2019", "doi": "10.1117/12.2512829", "abstract": "© 2019 SPIE. Detection and localization of microcalcification (MC) clusters are very important in mammography diagnosis. Supervised MC detectors require learning from extracted individual MCs and MC clusters. However, they are limited by number of datasets given that MC images are hard to obtain. In this work, we propose a method to detect malignant microcalcification (MC) clusters using unsupervised, one-class, deep convolutional autoencoder. Specifically, we designed a deep autoencoder model where only patches extracted from normal cases' mammograms are used during training. We then applied our trained model on patches extracted from testing images. Our training dataset contains 408 normal subjects, including 1961 full-field digital mammography images. Our testing datasets contains 276 subjects. Specifically, 106 of them were patients diagnosed with Ductal Carcinoma In-Situ (DCIS); 70 of them were diagnosed with Invasive Ductal Carcinoma (IDC); the rest 100 are normal cases containing 484 negative screening mammograms. Patches extracted from DCIS and IDC cases (positive patches) contain MC clusters, whereas patches extracted from normal cases (negative patches) don't. As the model is trained only on negative images that do not contain MCs, it cannot reconstruct MCs well, and thus, the reconstruction error will be larger on positive patches than negative patches. Our detection algorithm's decision is made based on Max-Squared Error between autoencoder's input and output patches. To confirm the results were not simply due to blurring, we then compared our designed detector with unsharp mask with Gaussian blur results. The results using the unsupervised autoencoder on testing patches with size 64×64 achieves an AUC result of 0.93. The best performance on testing patches using Gaussian blur with kernel size equal to 11has an overall AUC of 0.82.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2512829", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Robust control on crawling of deep seabed moving mining robot", "authors": " ", "published_date": "May 1, 2009", "doi": " ", "abstract": "The control problem of deep seabed mining robot is very hard for model uncertainty and external environment uncertainty. Therefore, radius basis function (RBF) neural network is used to learn the unknown bounds of system uncertainties adaptively. Adaptive heading robust control algorithm is presented based on status feedback and virtual input. And control method on the crawling of deep seabed moving mining robot is described after the discussion on the fuzzy rule between virtual input and tracks velocity. Simulation results show the feasibility of this method.", "publication_location": "Kongzhi Yu Juece/Control and Decision", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Nonlinear interference mitigation via deep neural networks", "authors": "Hager, C; Pfister, HD", "published_date": "June 13, 2018", "doi": " ", "abstract": "© 2018 OSA. A neural-network-based approach is presented to efficiently implement digital backpropagation (DBP). For a 32×100 km fiber-optic link, the resulting 'learned' DBP significantly reduces the complexity compared to conventional DBP implementations.", "publication_location": "2018 Optical Fiber Communications Conference and Exposition, Ofc 2018   Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Nonlinear interference mitigation via deep neural networks", "authors": "Häger, C; Pfister, HD", "published_date": "January 1, 2018", "doi": "10.1364/OFC.2018.W3A.4", "abstract": "© OSA 2018. A neural-network-based approach is presented to efficiently implement digital backpropagation (DBP). For a 32×100 km fiber-optic link, the resulting “learned“ DBP significantly reduces the complexity compared to conventional DBP implementations.", "publication_location": "Optics Infobase Conference Papers", "link": "http://dx.doi.org/10.1364/OFC.2018.W3A.4", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Unsupervised learning with truncated Gaussian graphical models", "authors": "Su, Q; Liao, X; Li, C; Gan, Z; Carin, L", "published_date": "January 1, 2017", "doi": " ", "abstract": "Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Gaussian graphical models (GGMs) are widely used for statistical modeling, because of ease of inference and the ubiquitous use of the normal distribution in practical approximations. However, they are also known for their limited modeling abilities, due to the Gaussian assumption. In this paper, we introduce a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits efficient inference. Specifically, we impose a bipartite structure on the GGM and govern the hidden variables by truncated normal distributions. The nonlinearity of the model is revealed by its connection to rectified linear unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and appealing properties of truncated normals, we are able to train the models efficiently using contrastive divergence. We consider three output constructs, accounting for real-valued, binary and count data. We further extend the model to deep constructions and show that deep models can be used for unsupervised pre-training of rectifier neural networks. Extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models.", "publication_location": "31st Aaai Conference on Artificial Intelligence, Aaai 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Microelectrode Recordings Validate the Clinical Visualization of Subthalamic-Nucleus Based on 7T Magnetic Resonance Imaging and Machine Learning for Deep Brain Stimulation Surgery.", "authors": "Shamir, RR; Duchin, Y; Kim, J; Patriat, R; Marmor, O; Bergman, H; Vitek, JL; Sapiro, G; Bick, A; Eliahou, R; Eitan, R; Israel, Z; Harel, N", "published_date": "March 2019", "doi": "10.1093/neuros/nyy212", "abstract": "BACKGROUND:Deep brain stimulation (DBS) of the subthalamic nucleus (STN) is a proven and effective therapy for the management of the motor symptoms of Parkinson's disease (PD). While accurate positioning of the stimulating electrode is critical for success of this therapy, precise identification of the STN based on imaging can be challenging. We developed a method to accurately visualize the STN on a standard clinical magnetic resonance imaging (MRI). The method incorporates a database of 7-Tesla (T) MRIs of PD patients together with machine-learning methods (hereafter 7 T-ML). OBJECTIVE:To validate the clinical application accuracy of the 7 T-ML method by comparing it with identification of the STN based on intraoperative microelectrode recordings. METHODS:Sixteen PD patients who underwent microelectrode-recordings guided STN DBS were included in this study (30 implanted leads and electrode trajectories). The length of the STN along the electrode trajectory and the position of its contacts to dorsal, inside, or ventral to the STN were compared using microelectrode-recordings and the 7 T-ML method computed based on the patient's clinical 3T MRI. RESULTS:All 30 electrode trajectories that intersected the STN based on microelectrode-recordings, also intersected it when visualized with the 7 T-ML method. STN trajectory average length was 6.2 ± 0.7 mm based on microelectrode recordings and 5.8 ± 0.9 mm for the 7 T-ML method. We observed a 93% agreement regarding contact location between the microelectrode-recordings and the 7 T-ML method. CONCLUSION:The 7 T-ML method is highly consistent with microelectrode-recordings data. This method provides a reliable and accurate patient-specific prediction for targeting the STN.", "publication_location": "Neurosurgery", "link": "http://dx.doi.org/10.1093/neuros/nyy212", "citations": "7", "readership": "40", "tweets": "4", "news_mentions": " "},
{"title": "Clinical deep brain stimulation region prediction using regression forests from high-field MRI", "authors": "Kim, J; Duchin, Y; Sapiro, G; Vitek, J; Harel, N", "published_date": "December 9, 2015", "doi": "10.1109/ICIP.2015.7351248", "abstract": "© 2015 IEEE. This paper presents a prediction framework of brain subcortical structures which are invisible on clinical low-field MRI, learning detailed information from ultrahigh-field MR training data. Volumetric segmentation of Deep Brain Stimulation (DBS) structures within the Basal ganglia is a prerequisite process for reliable DBS surgery. While ultrahigh-field MR imaging (7 Tesla) allows direct visualization of DBS targeting structures, such ultrahigh-fields are not always clinically available, and therefore the relevant structures need to be predicted from the clinical data. We address the shape prediction problem with a regression forest, non-linearly mapping predictors to target structures with high confidence, exploiting ultrahigh-field MR training data. We consider an application for the subthalamic nucleus (STN) prediction as a crucial DBS target. Experimental results on Parkinson's patients validate that the proposed approach enables reliable estimation of the STN from clinical 1.5T MRI.", "publication_location": "Proceedings   International Conference on Image Processing, Icip", "link": "http://dx.doi.org/10.1109/ICIP.2015.7351248", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Robust prediction of clinical deep brain stimulation target structures via the estimation of influential high-field MR atlases", "authors": "Kim, J; Duchin, Y; Kim, H; Vitek, J; Harel, N; Sapiro, G", "published_date": "January 1, 2015", "doi": "10.1007/978-3-319-24571-3_70", "abstract": "© Springer International Publishing Switzerland 2015. This work introduces a robust framework for predicting Deep Brain Stimulation (DBS) target structures which are not identifiable on standard clinical MRI. While recent high-field MR imaging allows clear visualization of DBS target structures, such high-fields are not clinically available, and therefore DBS targeting needs to be performed on the standard clinical low contrast data. We first learn via regression models the shape relationships between DBS targets and their potential predictors from high-field (7 Tesla) MR training sets. A bagging procedure is utilized in the regression model, reducing the variability of learned dependencies. Then, given manually or automatically detected predictors on the clinical patient data, the target structure is predicted using the learned high quality information. Moreover, we derive a robust way to properly weight different training subsets, yielding higher accuracy when using an ensemble of predictions. The subthalamic nucleus (STN), the most common DBS target for Parkinson’s disease, is used to exemplify within our framework. Experimental validation from Parkinson’s patients shows that the proposed approach enables reliable prediction of the STN from the clinical 1.5T MR data.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-319-24571-3_70", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Our digital age: Implications for learning and its (online) institutions", "authors": "Davidson, CN; Goldberg, DT", "published_date": "August 30, 2012", "doi": "10.2304/elea.2012.9.3.249", "abstract": "Over the past two decades, the way we learn has changed dramatically. We have new sources of information and new ways to exchange and to interact with information. But our schools and the way we teach have remained largely the same for years, even centuries. What happens to traditional educational institutions when learning also takes place on a vast range of Internet sites, from Pokemon Web pages to Wikipedia? This chapter, excerpted from our book, The Future of Thinking, does not promote change for the sake of change. Implicit in its sincere plea for transformation is an awareness that the current situation needs improvement. In advocating change for learning institutions, this chapter makes assumptions about the deep structure of learning, about cognition, about the way youth today learn about their world in informal settings, and about a mismatch between the excitement generated by informal learning and the routinization of learning common to many of our institutions of formal education. It advocates institutional change because our current formal educational institutions are not taking enough advantage of the modes of digital and participatory learning available to students today.", "publication_location": "E Learning and Digital Media", "link": "http://dx.doi.org/10.2304/elea.2012.9.3.249", "citations": "2", "readership": "21", "tweets": " ", "news_mentions": " "},
{"title": "The student productivity paradox: Technology mediated learning in schools", "authors": "Hikmet, N; Taylor, EZ; Davis, CJ", "published_date": "September 1, 2008", "doi": "10.1145/1378727.1389974", "abstract": "Many school administrators and school boards are strongly investing in information and communications technologies (ICT) to meet the increasing pressure on educational institutions to enhance learning outcomes and effectiveness. The value of investment in ICT depends on creative or other productive outcomes from its use, while, to be truly successful, ICT investment must match deep interests with powerful ideas. How students use the ICT provided will impact their own learning and also the success of the ICT program in the schools, and in turn, society at large. ICT investment was prompted by the assumption that encouraging students to improve their productivity would increase learning opportunities. Researchers should thereby investigate the specific needs and expectation of students, teachers, and parents, who adopt ICT for use in education, before investing heavily in ICT, as outcomes of investing in ICT are uncertain and may not always align with the intentions of school administrators.", "publication_location": "Communications of the Acm", "link": "http://dx.doi.org/10.1145/1378727.1389974", "citations": "5", "readership": "24", "tweets": " ", "news_mentions": " "},
{"title": "Learning from colleagues about healthcare IT implementation and optimization: lessons from a medical informatics listserv.", "authors": "Adams, MB; Kaplan, B; Sobko, HJ; Kuziemsky, C; Ravvaz, K; Koppel, R", "published_date": "January 2015", "doi": "10.1007/s10916-014-0157-3", "abstract": "Communication among medical informatics communities can suffer from fragmentation across multiple forums, disciplines, and subdisciplines; variation among journals, vocabularies and ontologies; cost and distance. Online communities help overcome these obstacles, but may become onerous when listservs are flooded with cross-postings. Rich and relevant content may be ignored. The American Medical Informatics Association successfully addressed these problems when it created a virtual meeting place by merging the membership of four working groups into a single listserv known as the \"Implementation and Optimization Forum.\" A communication explosion ensued, with thousands of interchanges, hundreds of topics, commentaries from \"notables,\" neophytes, and students--many from different disciplines, countries, traditions. We discuss the listserv's creation, illustrate its benefits, and examine its lessons for others. We use examples from the lively, creative, deep, and occasionally conflicting discussions of user experiences--interchanges about medication reconciliation, open source strategies, nursing, ethics, system integration, and patient photos in the EMR--all enhancing knowledge, collegiality, and collaboration.", "publication_location": "J Med Syst", "link": "http://dx.doi.org/10.1007/s10916-014-0157-3", "citations": "9", "readership": "67", "tweets": "7", "news_mentions": " "},
{"title": "Experience-dependent changes in cerebellar contributions to motor sequence learning.", "authors": "Doyon, J; Song, AW; Karni, A; Lalonde, F; Adams, MM; Ungerleider, LG", "published_date": "January 22, 2002", "doi": "10.1073/pnas.022615199", "abstract": "Studies in experimental animals and humans have stressed the role of the cerebellum in motor skill learning. Yet, the relative importance of the cerebellar cortex and deep nuclei, as well as the nature of the dynamic functional changes occurring between these and other motor-related structures during learning, remains in dispute. Using functional magnetic resonance imaging and a motor sequence learning paradigm in humans, we found evidence of an experience-dependent shift of activation from the cerebellar cortex to the dentate nucleus during early learning, and from a cerebellar-cortical to a striatal-cortical network with extended practice. The results indicate that intrinsic modulation within the cerebellum, in concert with activation of motor-related cortical regions, serves to set up a procedurally acquired sequence of movements that is then maintained elsewhere in the brain.", "publication_location": "Proceedings of the National Academy of Sciences of the United States of America", "link": "http://dx.doi.org/10.1073/pnas.022615199", "citations": "353", "readership": "378", "tweets": " ", "news_mentions": " "},
{"title": "Factored temporal sigmoid belief networks for sequence learning", "authors": "Song, J; Gan, Z; Carin, L", "published_date": "January 1, 2016", "doi": " ", "abstract": "Deep conditional generative models are developed to simultaneously learn the temporal dependencies of multiple sequences. The model is designed by introducing a three-way weight tensor to capture the multiplicative interactions between side information and sequences. The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). The transition matrices are further factored to reduce the number of parameters and improve generalization. When side information is not available, a general framework for semi-supervised learning based on the proposed model is constituted, allowing robust sequence classification. Experimental results show that the proposed approach achieves state-of-theart predictive and classification performance on sequential data, and has the capacity to synthesize sequences, with controlled style transitioning and blending.", "publication_location": "33rd International Conference on Machine Learning, Icml 2016", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning structured weight uncertainty in Bayesian neural networks", "authors": "Sun, S; Chen, C; Carin, L", "published_date": "January 1, 2017", "doi": " ", "abstract": "© 2017 PMLR. All rights reserved. Deep neural networks (DNNs) are increasingly popular in modern machine learning. Bayesian learning affords the opportunity to quantify posterior uncertainty on DNN model parameters. Most existing work adopts independent Gaussian priors on the model weights, ignoring possible structural information. In this paper, we consider the matrix variate Gaussian (MVG) distribution to model structured correlations within the weights of a DNN. To make posterior inference feasible, a reparametrization is proposed for the MVG prior, simplifying the complex MVG-based model to an equivalent yet simpler model with independent Gaussian priors on the transformed weights. Consequently, we develop a scalable Bayesian online inference algorithm by adopting the recently proposed probabilistic backpropagation framework. Experiments on several synthetic and real datasets indicate the superiority of our model, achieving competitive performance in terms of model likelihood and predictive root mean square error. Importantly, it also yields faster convergence speed compared to related Bayesian DNN models.", "publication_location": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, Aistats 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning structured weight uncertainty in Bayesian neural networks", "authors": "Sun, S; Chen, C; Carin, L", "published_date": "January 1, 2017", "doi": " ", "abstract": "© 2017 PMLR. All rights reserved. Deep neural networks (DNNs) are increasingly popular in modern machine learning. Bayesian learning affords the opportunity to quantify posterior uncertainty on DNN model parameters. Most existing work adopts independent Gaussian priors on the model weights, ignoring possible structural information. In this paper, we consider the matrix variate Gaussian (MVG) distribution to model structured correlations within the weights of a DNN. To make posterior inference feasible, a reparametrization is proposed for the MVG prior, simplifying the complex MVG-based model to an equivalent yet simpler model with independent Gaussian priors on the transformed weights. Consequently, we develop a scalable Bayesian online inference algorithm by adopting the recently proposed probabilistic backpropagation framework. Experiments on several synthetic and real datasets indicate the superiority of our model, achieving competitive performance in terms of model likelihood and predictive root mean square error. Importantly, it also yields faster convergence speed compared to related Bayesian DNN models.", "publication_location": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, Aistats 2017", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adversarial learning of a sampler based on an unnormalized distribution", "authors": "Li, C; Bai, K; Li, J; Wang, G; Chen, C; Carin, L", "published_date": "January 1, 2020", "doi": " ", "abstract": "© 2019 by the author(s). We investigate adversarial learning in the case when only an unnormalized form of the density can be accessed, rather than samples. With insights so garnered, adversarial learning is extended to the case for which one has access to an unnormalized form u(x) of the target density function, but no samples. Further, new concepts in GAN regularization are developed, based on learning from samples or from u(x). The proposed method is compared to alternative approaches, with encouraging results demonstrated across a range of applications, including deep soft Q-learning.", "publication_location": "Aistats 2019   22nd International Conference on Artificial Intelligence and Statistics", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Learning weight uncertainty with stochastic gradient MCMC for shape classification", "authors": "Li, C; Stevens, A; Chen, C; Pu, Y; Gan, Z; Carin, L", "published_date": "December 9, 2016", "doi": "10.1109/CVPR.2016.611", "abstract": "© 2016 IEEE. Learning the representation of shape cues in 2D & 3D objects for recognition is a fundamental task in computer vision. Deep neural networks (DNNs) have shown promising performance on this task. Due to the large variability of shapes, accurate recognition relies on good estimates of model uncertainty, ignored in traditional training of DNNs, typically learned via stochastic optimization. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) to learn weight uncertainty in DNNs. It yields principled Bayesian interpretations for the commonly used Dropout/DropConnect techniques and incorporates them into the SG-MCMC framework. Extensive experiments on 2D & 3D shape datasets and various DNN models demonstrate the superiority of the proposed approach over stochastic optimization. Our approach yields higher recognition accuracy when used in conjunction with Dropout and Batch-Normalization.", "publication_location": "Proceedings of the Ieee Computer Society Conference on Computer Vision and Pattern Recognition", "link": "http://dx.doi.org/10.1109/CVPR.2016.611", "citations": "7", "readership": "150", "tweets": " ", "news_mentions": " "},
{"title": "Data discovery with DATS: exemplar adoptions and lessons learned.", "authors": "Gonzalez-Beltran, AN; Campbell, J; Dunn, P; Guijarro, D; Ionescu, S; Kim, H; Lyle, J; Wiser, J; Sansone, S-A; Rocca-Serra, P", "published_date": "January 2018", "doi": "10.1093/jamia/ocx119", "abstract": "The DAta Tag Suite (DATS) is a model supporting dataset description, indexing, and discovery. It is available as an annotated serialization with schema.org, a vocabulary used by major search engines, thus making the datasets discoverable on the web. DATS underlies DataMed, the National Institutes of Health Big Data to Knowledge Data Discovery Index prototype, which aims to provide a \"PubMed for datasets.\" The experience gained while indexing a heterogeneous range of >60 repositories in DataMed helped in evaluating DATS's entities, attributes, and scope. In this work, 3 additional exemplary and diverse data sources were mapped to DATS by their representatives or experts, offering a deep scan of DATS fitness against a new set of existing data. The procedure, including feedback from users and implementers, resulted in DATS implementation guidelines and best practices, and identification of a path for evolving and optimizing the model. Finally, the work exposed additional needs when defining datasets for indexing, especially in the context of clinical and observational information.", "publication_location": "Journal of the American Medical Informatics Association : Jamia", "link": "http://dx.doi.org/10.1093/jamia/ocx119", "citations": "1", "readership": "32", "tweets": "18", "news_mentions": " "},
{"title": "Adversarially learned representations for information obfuscation and inference", "authors": "Bertran, M; Martinez, N; Papadaki, A; Qiu, Q; Rodrigues, M; Reeves, G; Sapiro, G", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 36th International Conference on Machine Learning, ICML 2019. All rights reserved. Data collection and sharing are pervasive aspects of modern society. This process can either be voluntary, as in the case of a person taking a facial image to unlock his/her phone, or incidental, such as traffic cameras collecting videos on pedestrians. An undesirable side effect of these processes is that shared data can carry information about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design procedures that minimize sensitive information leakage. Balancing the competing objectives of providing meaningful individualized service levels and inference while obfuscating sensitive information is still an open problem. In this work, we take an information theoretic approach that is implemented as an unconstrained adversarial game between Deep Neural Networks in a principled, data-driven manner. This approach enables us to learn domain-preserving stochastic transformations that maintain performance on existing algorithms while minimizing sensitive information leakage.", "publication_location": "36th International Conference on Machine Learning, Icml 2019", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Revisiting Efficient Multi-Step Nonlinearity Compensation with Machine Learning: An Experimental Demonstration", "authors": "Oliari, V; Goossens, S; Hager, C; Liga, G; Butler, RM; Hout, MVD; Heide, SVD; Pfister, HD; Okonkwo, C; Alvarado, A", "published_date": "June 15, 2020", "doi": "10.1109/JLT.2020.2994220", "abstract": "© 1983-2012 IEEE. Efficient nonlinearity compensation in fiber-optic communication systems is considered a key element to go beyond the 'capacity crunch'. One guiding principle for previous work on the design of practical nonlinearity compensation schemes is that fewer steps lead to better systems. In this paper, we challenge this assumption and show how to carefully design multi-step approaches that provide better performance-complexity trade-offs than their few-step counterparts. We consider the recently proposed learned digital backpropagation (LDBP) approach, where the linear steps in the split-step method are re-interpreted as general linear functions, similar to the weight matrices in a deep neural network. Our main contribution lies in an experimental demonstration of this approach for a 25 Gbaud single-channel optical transmission system. It is shown how LDBP can be integrated into a coherent receiver DSP chain and successfully trained in the presence of various hardware impairments. Our results show that LDBP with limited complexity can achieve better performance than standard DBP by using very short, but jointly optimized, finite-impulse response filters in each step. This paper also provides an overview of recently proposed extensions of LDBP and we comment on potentially interesting avenues for future work.", "publication_location": "Journal of Lightwave Technology", "link": "http://dx.doi.org/10.1109/JLT.2020.2994220", "citations": " ", "readership": " ", "tweets": "1", "news_mentions": " "},
{"title": "The total ankle arthroplasty learning curve with third-generation implants: a single surgeon's experience.", "authors": "Clement, RC; Krynetskiy, E; Parekh, SG", "published_date": "August 2013", "doi": "10.1177/1938640013493463", "abstract": "BACKGROUND: Renewed interest in total ankle arthroplasty (TAA) has developed globally as a result of recent literature supporting new-generation implants as a viable alternative to arthrodesis. The literature also demonstrates a learning curve among surgeons adopting TAA. The purpose of this study is to better define this learning curve for surgeons using third-generation implants. METHODS: Charts and radiographs were reviewed for the initial 26 TAA procedures performed by the senior author. Three third-generation implants were used: SBi (Small Bone Innovations) STAR, Salto Talaris, and Wright Medical INBONE. We report perioperative and early postoperative complications. RESULTS: Two perioperative fractures occurred in the first 9 cases, and the incidence subsequently dropped to 0 (P = .0431). Two cases of component malalignment occurred in the first 3 patients receiving the STAR implant, and the incidence then dropped to 0 (P = .0034). Five wound complications (4 minor and 1 major) occurred, all in the final 14 patients. No cases of nerve injury, tendon laceration, or deep vein thrombosis occurred. Two patients returned to the operating room as a result of complications, and the total perioperative and early postoperative complication rate was 27%. CONCLUSION: The observed rate of perioperative and early postoperative complications in this case series was low relative to other similar-sized studies, suggesting that third-generation implants can reduce adverse events. Our results demonstrate that some common complications could be avoided altogether (nerve/tendon injuries), some decreased quickly with experience (intraoperative fractures and component malpositioning), and some persisted unchanged throughout this study (wound complications). These findings should influence surgical training, surgeon willingness to adopt this procedure, and patient counseling. LEVELS OF EVIDENCE: Therapeutic, Level IV, Retrospective Case Series.", "publication_location": "Foot Ankle Spec", "link": "http://dx.doi.org/10.1177/1938640013493463", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Machine learning approaches for slum detection using very high resolution satellite images", "authors": "Gadiraju, KK; Vatsavai, RR; Kaza, N; Wibbels, E; Krishna, A", "published_date": "February 7, 2019", "doi": "10.1109/ICDMW.2018.00198", "abstract": "© 2018 IEEE. Detecting informal settlements has become an important area of research in the past decade, owing to the availability of high resolution satellite imagery. Traditional per-pixel based classification methods provide high degree of accuracy in distinguishing primitive instances such as buildings, roads, forests and water. However, these methods fail to capture the complex relationships between neighboring pixels that is necessary for distinguishing complex objects such as informal and formal settlements. In this paper, we perform several experiments to compare and contrast how various per-pixel based classification methods, when combined with various features perform in detecting slums. In addition, we also explored a deep neural network, which showed better accuracy than the pixel based methods.", "publication_location": "Ieee International Conference on Data Mining Workshops, Icdmw", "link": "http://dx.doi.org/10.1109/ICDMW.2018.00198", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "A National Survey of Learning Activities and Instructional Strategies Used to Teach Occupation: Implications for Signature Pedagogies.", "authors": "Krishnagiri, S; Hooper, B; Price, P; Taff, SD; Bilics, A", "published_date": " ", "doi": "10.5014/ajot.2019.032789", "abstract": "IMPORTANCE: Occupational therapy practitioners' professional identities and distinctive contributions to health care connect essentially to their knowledge of occupation. Thus, the strategies educators use to convey occupation to students and the perspectives embedded in those strategies are critical topics for researchers. OBJECTIVE: To generalize findings from a previous qualitative study of how educators in 25 U.S. occupational therapy assistant and occupational therapy programs addressed occupation to a national sample of educators. DESIGN: As part of an exploratory sequential design, a national survey of U.S. occupational therapy and occupational therapy assistant educators explored activities and strategies used to teach occupation. Data were analyzed using descriptive statistics. SETTING: An online survey about educators' practices in the academic education setting. PARTICIPANTS: Occupational therapy and occupational therapy assistant educators (N = 1,590) from all programs in the United States. Of these, 634 returned surveys, 315 of which were complete and included in the analysis, for an overall response rate of 19.8%. RESULTS: Respondents identified similar learning activities and instructional strategies as those identified in the qualitative phase of the design. Most instruction was active and experiential, requiring students to integrate various skills and content areas. Definitions of occupation, as a basis for teaching, varied. CONCLUSIONS AND RELEVANCE: The combined survey and qualitative results offered initial empirical support for occupational therapy's proposed signature pedagogies and the importance of attending to the deep and implicit structures within those pedagogies. Such structures are believed to support students' formation of a professional identity and an occupational perspective. WHAT THIS ARTICLE ADDS: This study provides evidence for the instructional strategies that educators use to convey knowledge of occupation to students. The predominant strategies support proposed signature pedagogies in occupational therapy: relational learning, affective learning, and highly contextualized active learning.", "publication_location": "The American Journal of Occupational Therapy : Official Publication of the American Occupational Therapy Association", "link": "http://dx.doi.org/10.5014/ajot.2019.032789", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Supernova Photometric Classification Pipelines Trained on Spectroscopically Classified Supernovae from the Pan-STARRS1 Medium-deep Survey", "authors": "Villar, VA; Berger, E; Miller, G; Chornock, R; Rest, A; Jones, DO; Drout, MR; Foley, RJ; Kirshner, R; Lunnan, R; Magnier, E; Milisavljevic, D; Sanders, N; Scolnic, D", "published_date": "October 10, 2019", "doi": "10.3847/1538-4357/ab418c", "abstract": "© 2019. The American Astronomical Society. All rights reserved. Photometric classification of supernovae (SNe) is imperative as recent and upcoming optical time-domain surveys, such as the Large Synoptic Survey Telescope (LSST), overwhelm the available resources for spectrosopic follow-up. Here we develop a range of light curve (LC) classification pipelines, trained on 513 spectroscopically classified SNe from the Pan-STARRS1 Medium-Deep Survey (PS1-MDS): 357 Type Ia, 93 Type II, 25 Type IIn, 21 Type Ibc, and 17 Type I superluminous SNe (SLSNe). We present a new parametric analytical model that can accommodate a broad range of SN LC morphologies, including those with a plateau, and fit this model to data in four PS1 filters (g P1 r P1 i P1 z P1). We test a number of feature extraction methods, data augmentation strategies, and machine-learning algorithms to predict the class of each SN. Our best pipelines result in ≈90% average accuracy, ≈70% average purity, and ≈80% average completeness for all SN classes, with the highest success rates for SNe Ia and SLSNe and the lowest for SNe Ibc. Despite the greater complexity of our classification scheme, the purity of our SN Ia classification, ≈95%, is on par with methods developed specifically for Type Ia versus non-Type Ia binary classification. As the first of its kind, this study serves as a guide to developing and training classification algorithms for a wide range of SN types with a purely empirical training set, particularly one that is similar in its characteristics to the expected LSST main survey strategy. Future work will implement this classification pipeline on ≈3000 PS1/MDS LCs that lack spectroscopic classification.", "publication_location": "The Astrophysical Journal", "link": "http://dx.doi.org/10.3847/1538-4357/ab418c", "citations": " ", "readership": "18", "tweets": "1", "news_mentions": "2"},
{"title": "A deep convolutional neural network and a random forest classifier for solar photovoltaic array detection in aerial imagery", "authors": "Malof, JM; Collins, LM; Bradbury, K; Newell, RG", "published_date": "January 1, 2016", "doi": "10.1109/ICRERA.2016.7884415", "abstract": "© 2016 IEEE. Power generation from distributed solar photovoltaic PV arrays has grown rapidly in recent years. As a result, there is interest in collecting information about the quantity, power capacity, and energy generated by such arrays; and to do so over small geo-spatial regions (e.g., counties, cities, or even smaller regions). Unfortunately, existing sources of such information are dispersed, limited in geospatial resolution, and otherwise incomplete or publically unavailable. As result, we recently proposed a new approach for collecting such distributed PV information that relies on computer algorithms to automatically detect PV arrays in high resolution aerial imagery [1], Here we build on this work by investigating two machine learning algorithms for PV array detection: a Random Forest classifier (RF) [2] and a deep convolutional neural network (CNN) [3]. We use the RF algorithm as a benchmark, or baseline, for comparison with a CNN model. The two models are developed and tested using a large collection of publicly available [4] aerial imagery, covering 135 km2, and including over 2,700 manually annotated distributed PV array locations. The results indicate that the CNN substantially improves over the RF. The CNN is capable of excellent performance, detecting nearly 80% of true panels with a precision measure of 72%.", "publication_location": "2016 Ieee International Conference on Renewable Energy Research and Applications, Icrera 2016", "link": "http://dx.doi.org/10.1109/ICRERA.2016.7884415", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Hybrid Bayesian Eigenobjects: Combining Linear Subspace and Deep Network Methods for 3D Robot Vision", "authors": "Burchfiel, B; Konidaris, G", "published_date": "December 27, 2018", "doi": "10.1109/IROS.2018.8593795", "abstract": "© 2018 IEEE. We introduce Hybrid Bayesian Eigenobjects (HBEOs), a novel representation for 3D objects designed to allow a robot to jointly estimate the pose, class, and full 3D geometry of a novel object observed from a single viewpoint in a single practical framework. By combining both linear subspace methods and deep convolutional prediction, HBEOs efficiently learn nonlinear object representations without directly regressing into high-dimensional space. HBEOs also remove the onerous and generally impractical necessity of input data voxelization prior to inference. We experimentally evaluate the suitability of HBEOs to the challenging task of joint pose, class, and shape inference on novel objects and show that, compared to preceding work, HBEOs offer dramatically improved performance in all three tasks along with several orders of magnitude faster runtime performance.", "publication_location": "Ieee International Conference on Intelligent Robots and Systems", "link": "http://dx.doi.org/10.1109/IROS.2018.8593795", "citations": "1", "readership": "47", "tweets": "7", "news_mentions": " "},
{"title": "Deriving lung perfusion directly from CT image using deep convolutional neural network: A preliminary study", "authors": "Ren, G; Ho, WY; Qin, J; Cai, J", "published_date": "January 1, 2019", "doi": "10.1007/978-3-030-32486-5_13", "abstract": "© Springer Nature Switzerland AG 2019. Functional avoidance radiation therapy for lung cancer patients aims to limit dose delivery to highly functional lung. However, the clinical functional imaging suffers from many shortcomings, including the need of exogenous contrasts, longer processing time, etc. In this study, we present a new approach to derive the lung functional images, using a deep convolutional neural network to learn and exploit the underlying functional information in the CT image and generate functional perfusion image. In this study, 99mTc MAA SPECT/CT scans of 30 lung cancer patients were retrospectively analyzed. The CNN model was trained using randomly selected dataset of 25 patients and tested using the remaining 5 subjects. Our study showed that it is feasible to derive perfusion images from CT image. Using the deep neural network with discrete labels, the main defect regions can be predicted. This technique holds the promise to provide lung function images for image guided functional lung avoidance radiation therapy.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-030-32486-5_13", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Deep convolutional neural network applied to the liver imaging reporting and data system (LI-RADS) version 2014 category classification: a pilot study.", "authors": "Yamashita, R; Mittendorf, A; Zhu, Z; Fowler, KJ; Santillan, CS; Sirlin, CB; Bashir, MR; Do, RKG", "published_date": "January 2020", "doi": "10.1007/s00261-019-02306-7", "abstract": "PURPOSE: To develop a deep convolutional neural network (CNN) model to categorize multiphase CT and MRI liver observations using the liver imaging reporting and data system (LI-RADS) (version 2014). METHODS: A pre-existing dataset comprising 314 hepatic observations (163 CT, 151 MRI) with corresponding diameters and LI-RADS categories (LR-1-5) assigned in consensus by two LI-RADS steering committee members was used to develop two CNNs: pre-trained network with an input of triple-phase images (training with transfer learning) and custom-made network with an input of quadruple-phase images (training from scratch). The dataset was randomly split into training, validation, and internal test sets (70:15:15 split). The overall accuracy and area under receiver operating characteristic curve (AUROC) were assessed for categorizing LR-1/2, LR-3, LR-4, and LR-5. External validation was performed for the model with the better performance on the internal test set using two external datasets (EXT-CT and EXT-MR: 68 and 44 observations, respectively). RESULTS: The transfer learning model outperformed the custom-made model: overall accuracy of 60.4% and AUROCs of 0.85, 0.90, 0.63, 0.82 for LR-1/2, LR-3, LR-4, LR-5, respectively. On EXT-CT, the model had an overall accuracy of 41.2% and AUROCs of 0.70, 0.66, 0.60, 0.76 for LR-1/2, LR-3, LR-4, LR-5, respectively. On EXT-MR, the model had an overall accuracy of 47.7% and AUROCs of 0.88, 0.74, 0.69, 0.79 for LR-1/2, LR-3, LR-4, LR-5, respectively. CONCLUSION: Our study shows the feasibility of CNN for assigning LI-RADS categories from a relatively small dataset but highlights the challenges of model development and validation.", "publication_location": "Abdom Radiol (Ny)", "link": "http://dx.doi.org/10.1007/s00261-019-02306-7", "citations": "1", "readership": "12", "tweets": "1", "news_mentions": " "},
{"title": "Occupational Radiation Exposure of Anesthesia Providers: A Summary of Key Learning Points and Resident-Led Radiation Safety Projects.", "authors": "Wang, RR; Kumar, AH; Tanaka, P; Macario, A", "published_date": "June 2017", "doi": "10.1177/1089253217692110", "abstract": "Anesthesia providers are frequently exposed to radiation during routine patient care in the operating room and remote anesthetizing locations. Eighty-two percent of anesthesiology residents (n = 57 responders) at our institution had a \"high\" or \"very high\" concern about the level of ionizing radiation exposure, and 94% indicated interest in educational materials about radiation safety. This article highlights key learning points related to basic physical principles, effects of ionizing radiation, radiation exposure measurement, occupational dose limits, considerations during pregnancy, sources of exposure, factors affecting occupational exposure such as positioning and shielding, and monitoring. The principle source of exposure is through scattered radiation as opposed to direct exposure from the X-ray beam, with the patient serving as the primary source of scatter. As a result, maximizing the distance between the provider and the patient is of great importance to minimize occupational exposure. Our dosimeter monitoring project found that anesthesiology residents (n = 41) had low overall mean measured occupational radiation exposure. The highest deep dose equivalent value for a resident was 0.50 mSv over a 3-month period, less than 10% of the International Commission on Radiological Protection occupational limit, with the eye dose equivalent being 0.52 mSv, approximately 4% of the International Commission on Radiological Protection recommended limit. Continued education and awareness of the risks of ionizing radiation and protective strategies will reduce exposure and potential for associated sequelae.", "publication_location": "Semin Cardiothorac Vasc Anesth", "link": "http://dx.doi.org/10.1177/1089253217692110", "citations": "7", "readership": "15", "tweets": "47", "news_mentions": " "},
{"title": "Anomaly detection for medical images based on a one-class classification", "authors": "Wei, Q; Ren, Y; Hou, R; Shi, B; Lo, JY; Carin, L", "published_date": "January 1, 2018", "doi": "10.1117/12.2293408", "abstract": "© 2018 SPIE. Detecting an anomaly such as a malignant tumor or a nodule from medical images including mammogram, CT or PET images is still an ongoing research problem drawing a lot of attention with applications in medical diagnosis. A conventional way to address this is to learn a discriminative model using training datasets of negative and positive samples. The learned model can be used to classify a testing sample into a positive or negative class. However, in medical applications, the high unbalance between negative and positive samples poses a difficulty for learning algorithms, as they will be biased towards the majority group, i.e., the negative one. To address this imbalanced data issue as well as leverage the huge amount of negative samples, i.e., normal medical images, we propose to learn an unsupervised model to characterize the negative class. To make the learned model more flexible and extendable for medical images of different scales, we have designed an autoencoder based on a deep neural network to characterize the negative patches decomposed from large medical images. A testing image is decomposed into patches and then fed into the learned autoencoder to reconstruct these patches themselves. The reconstruction error of one patch is used to classify this patch into a binary class, i.e., a positive or a negative one, leading to a one-class classifier. The positive patches highlight the suspicious areas containing anomalies in a large medical image. The proposed method has been tested on InBreast dataset and achieves an AUC of 0.84. The main contribution of our work can be summarized as follows. 1) The proposed one-class learning requires only data from one class, i.e., the negative data; 2) The patch-based learning makes the proposed method scalable to images of different sizes and helps avoid the large scale problem for medical images; 3) The training of the proposed deep convolutional neural network (DCNN) based auto-encoder is fast and stable.", "publication_location": "Progress in Biomedical Optics and Imaging   Proceedings of Spie", "link": "http://dx.doi.org/10.1117/12.2293408", "citations": "9", "readership": "27", "tweets": "3", "news_mentions": " "},
{"title": "Improved survival of patients undergoing palliation of hypoplastic left heart syndrome: lessons learned from 115 consecutive patients.", "authors": " ", "published_date": "September 2002", "doi": " ", "abstract": "BACKGROUND: Outcome of stage 1 palliation (S1P) for hypoplastic left heart syndrome (HLHS) has improved coincident with application of treatment strategies including continuous superior vena cava oximetry (SvO2), phenoxybenzamine (POB), strategies to minimize the duration of deep hypothermic circulatory arrest (DHCA) and efforts to ameliorate the inflammatory response to cardiopulmonary bypass (CPB) using aprotinin and modified ultrafiltration. METHODS AND RESULTS: Analysis of a consecutive series of 115 patients undergoing S1P was done to identify the risk factors for mortality and the impact of new treatment strategies. For the current era, July 1996 to October 2001, hospital survival was 93% (75/81) compared with 53% (18/34) for the time period, January 1992 to June 1996, P<0.001. Survival to stage 2 palliation (S2P) was also significantly improved in the current era, 81% (66/81) versus 44% (15/34), P<0.01. Anti-inflammatory treatment strategies demonstrated improved survival by univariate analysis (P<0.001). Multivariate analysis identified continuous SvO2 monitoring as a factor favoring S1P survival (P=0.02) and use of POB as a factor favoring survival to S2P (P=0.003). In the current era shorter duration of DHCA was associated with improved survival to S2P (P=0.02). CONCLUSIONS: Improved survival following S1P can be achieved with strategies that allow for early identification of decreased systemic output and the use of afterload reduction to stabilize systemic vascular resistance and therefore the pulmonary to systemic flow ratio. Strategies to ameliorate the inflammatory response to CPB may decrease the degree and duration of postoperative support. Strategies to minimize duration of DHCA may improve intermediate survival and merit additional studies.", "publication_location": "Circulation", "link": "http://www.ncbi.nlm.nih.gov/pmc/articles/12354714", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino  Experiment (DUNE) Conceptual Design Report Volume 1: The LBNF and DUNE  Projects", "authors": "Acciarri, R; Acero, MA; Adamowski, M; Adams, C; Adamson, P; Adhikari, S; Ahmad, Z; Albright, CH; Alion, T; Amador, E; Anderson, J; Anderson, K; Andreopoulos, C; Andrews, M; Andrews, R; Anghel, I; Anjos, JD; Ankowski, A; Antonello, M; ArandaFernandez, A; Ariga, A; Ariga, T; Aristizabal, D; Arrieta-Diaz, E; Aryal, K; Asaadi, J; Asner, D; Athar, MS; Auger, M; Aurisano, A; Aushev, V; Autiero, D; Avila, M; Back, JJ; Bai, X; Baibussinov, B; Baird, M; Balantekin, AB; Baller, B; Ballett, P; Bambah, B; Bansal, M; Bansal, S; Barker, GJ; Barletta, WA; Barr, G; Barros, N; Bartoszek, L; Bashyal, A; Bass, M; Bay, F; Beacom, J; Behera, BR; Bellettini, G; Bellini, V; Beltramello, O; Benetti, PA; Bercellie, A; Bergevin, M; Berman, E; Berns, H; Bernstein, R; Bertolucci, S; Bhandari, B; Bhatnagar, V; Bhuyan, B; Bian, J; Biery, K; Bishai, M; Blackburn, T; Blake, A; Blaszczyk, FDM; Blaufuss, E; Bleakley, B; Blucher, E; Bocean, V; Boffelli, F; Boissevain, J; Bolognesi, S; Bolton, T; Bonesini, M; Boone, T; Booth, C; Bordoni, S; Borysova, M; Bourguille, B; Boyd, SB; Brailsford, D; Brandt, A; Bremer, J; Brice, S; Bromberg, C; Brooijmans, G; Brown, G; Brown, R; Brunetti, G; Bu, X; Buchanan, N; Budd, H; Bugg, B; Calafiura, P; Calligarich, E; Calvo, E; Camilleri, L; Campanelli, M; Cantini, C; Carls, B; Carr, R; Cascella, M; Castromonte, C; CatanoMur, E; Cavanna, F; Centro, S; CerveraVillanueva, A; Chandratre, VB; Chatterjee, A; Chattopadhyay, S; Chaussard, L; Chembra, S; Chen, H; Chen, K; Chen, M; Cherdack, D; Chi, C; Childress, S; Choubey, S; Choudhary, BC; Christodoulou, G; Christofferson, C; Church, E; Cianci, D; Cline, D; Coan, T; Cocco, A; Coelho, J; Cole, P; Collin, G; Conrad, JM; Convery, M; Corey, R; Corwin, L; Cranshaw, J; Crivelli, P; Cronin-Hennessy, D; Curioni, A; Cushing, J; Adams, DL; Dale, D; Das, SR; Davenne, T; Davies, GS; Davies, J; Dawson, J; De, K; deGouvea, A; deJong, JK; deJong, P; DeLurgio, P; Decowski, M; Delbart, A; Densham, C; Dharmapalan, R; Dhingra, N; DiLuise, S; Diamantopoulou, M; Diaz, JS; DiazBautista, G; Diwan, M; Djurcic, Z; Dolph, J; Drake, G; Duchesneau, D; Duvernois, M; Duyang, H; Dwyer, DA; Dye, S; Dytman, S; Eberly, B; Edgecock, R; Edmunds, D; Elliott, S; Elnimr, M; Emery, S; Endress, E; Eno, S; Ereditato, A; Escobar, CO; Evans, J; Falcone, A; Falk, L; Farbin, A; Farnese, C; Farzan, Y; Fava, A; Favilli, L; Felde, J; Felix, J; Fernandes, S; Fields, L; Finch, A; Fitton, M; Fleming, B; Forest, T; Fowler, J; Fox, W; Fried, J; Friedland, A; Fuess, S; Fujikawa, B; Gago, A; Gallagher, H; Galymov, S; Gamble, T; Gandhi, R; Garcia-Gamez, D; Gardiner, S; Garvey, G; Gehman, VM; Gendotti, A; Geronimo, GD; Ghag, C; Ghoshal, P; Gibin, D; Gil-Botella, I; Gill, R; Girardelli, D; Giri, A; Glavin, S; Goeldi, D; Golapinni, S; Gold, M; Gomes, RA; GomezCadenas, JJ; Goodman, MC; Gorbunov, D; Goswami, S; Graf, N; Graham, M; Gramelini, E; Gran, R; Grant, C; Grant, N; Greco, V; Greenlee, H; Greenler, L; Greenley, C; Groh, M; Grullon, S; Grundy, T; Grzelak, K; Guardincerri, E; Guarino, V; Guarnaccia, E; Guedes, GP; Guenette, R; Guglielmi, A; Habig, AT; Hackenburg, RW; Hackenburg, A; Hadavand, H; Haenni, R; Hahn, A; Haigh, MD; Haines, T; Hamernik, T; Handler, T; Hans, S; Harris, D; Hartnell, J; Hasegawa, T; Hatcher, R; Hatzikoutelis, A; Hays, S; Hazen, E; Headley, M; Heavey, A; Heeger, K; Heise, J; Hennessy, K; Hewes, J; Higuera, A; Hill, T; Himmel, A; Hogan, M; Holanda, P; Holin, A; Honey, W; Horikawa, S; Horton-Smith, G; Howard, B; Howell, J; Hurh, P; Huston, J; Hylen, J; Imlay, R; Insler, J; Introzzi, G; Ioanisyan, D; Ioannisian, A; Iwamoto, K; Izmaylov, A; Jackson, C; Jaffe, DE; James, C; James, E; Jediny, F; Jen, C; Jhingan, A; Jiménez, S; Jo, JH; Johnson, M; Johnson, R; Johnstone, J; Jones, BJ; Joshi, J; Jostlein, H; Jung, CK; Junk, T; Kaboth, A; Kadel, R; Kafka, T; Kalousis, L; Kamyshkov, Y; Karagiorgi, G; Karasavvas, D; Karyotakis, Y; Kaur, A; Kaur, P; Kayser, B; Kazaryan, N; Kearns, E; Keener, P; Kemboi, S; Kemp, E; Kettell, SH; Khabibullin, M; Khandaker, M; Khotjantsev, A; Kirby, B; Kirby, M; Klein, J; Kobilarcik, T; Kohn, S; Koizumi, G; Kopylov, A; Kordosky, M; Kormos, L; Kose, U; Kostelecky, VA; Kramer, M; Kreslo, I; Kriske, R; Kropp, W; Kudenko, Y; Kudryavtsev, VA; Kulagin, S; Kumar, A; Kumar, GK; Kumar, J; Kumar, L; Kutter, T; Laminack, A; Lande, K; Lane, C; Lang, K; Lanni, F; Learned, J; Lebrun, P; Lee, D; Lee, H; Lee, K; Lee, WM; LeiguideOliveira, MA; Li, Q; Li, S; Li, X; Li, Y; Li, Z; Libo, J; Lin, CS; Lin, S; Ling, J; Link, J; Liptak, Z; Lissauer, D; Littenberg, L; Littlejohn, B; Liu, Q; Liu, T; Lockwitz, S; Lockyer, N; Loew, T; Lokajicek, M; Long, K; Lopes, MDL; Lopez, JP; Losecco, J; Louis, W; Lowery, J; Luethi, M; Luk, KB; Lundberg, B; Lundin, T; Luo, X; Lux, T; Lykken, J; Machado, AA; Macier, JR; Magill, S; Mahler, G; Mahn, K; Malek, M; Malhotra, S; Malon, D; Mammoliti, F; Mancina, S; Mandal, SK; Mandodi, S; Manly, SL; Mann, A; Marchionni, A; Marciano, W; Mariani, C; Maricic, J; Marino, A; Marshak, M; Marshall, C; Marshall, J; Marteau, J; Martin-Albo, J; Martinez, D; Matsuno, S; Matthews, J; Mauger, C; Mavrokoridis, K; Mayilyan, D; Mazzucato, E; McCauley, N; McCluskey, E; McConkey, N; McDonald, K; McFarland, KS; McGowan, AM; McGrew, C; McKeown, R; McNulty, D; McTaggart, R; Mefodiev, A; Mehrian, M; Mehta, P; Mei, D; Mena, O; Menary, S; Mendez, H; Menegolli, A; Meng, G; Meng, Y; Merritt, H; Mertins, D; Messier, M; Metcalf, W; Mewes, M; Meyer, H; Miao, T; Milincic, R; Miller, W; Mills, G; Mineev, O; Miranda, O; Mishra, CS; Mishra, SR; Mitrica, B; Mladenov, D; Mocioiu, I; Mohanta, R; Mokhov, N; Montanari, C; Montanari, D; Moon, J; Mooney, M; Moore, C; Morfin, J; Morgan, B; Morris, C; Morse, W; Moss, Z; Mossey, C; Moura, CA; Mousseau, J; Mualem, L; Muether, M; Mufson, S; Murphy, S; Musser, J; Musser, R; Nakajima, Y; Naples, D; Navarro, J; Navas, D; Nelson, J et al.", "published_date": " ", "doi": " ", "abstract": "This document presents the Conceptual Design Report (CDR) put forward by aninternational neutrino community to pursue the Deep Underground NeutrinoExperiment at the Long-Baseline Neutrino Facility (LBNF/DUNE), a groundbreakingscience experiment for long-baseline neutrino oscillation studies and forneutrino astrophysics and nucleon decay searches. The DUNE far detector will bea very large modular liquid argon time-projection chamber (LArTPC) located deepunderground, coupled to the LBNF multi-megawatt wide-band neutrino beam. DUNEwill also have a high-resolution and high-precision near detector.", "publication_location": " ", "link": "http://arxiv.org/abs/1601.05471v1", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino  Experiment (DUNE) Conceptual Design Report, Volume 4 The DUNE Detectors at  LBNF", "authors": "Acciarri, R; Acero, MA; Adamowski, M; Adams, C; Adamson, P; Adhikari, S; Ahmad, Z; Albright, CH; Alion, T; Amador, E; Anderson, J; Anderson, K; Andreopoulos, C; Andrews, M; Andrews, R; Anghel, I; Anjos, JD; Ankowski, A; Antonello, M; ArandaFernandez, A; Ariga, A; Ariga, T; Aristizabal, D; Arrieta-Diaz, E; Aryal, K; Asaadi, J; Asner, D; Athar, MS; Auger, M; Aurisano, A; Aushev, V; Autiero, D; Avila, M; Back, JJ; Bai, X; Baibussinov, B; Baird, M; Balantekin, AB; Baller, B; Ballett, P; Bambah, B; Bansal, M; Bansal, S; Barker, GJ; Barletta, WA; Barr, G; Barros, N; Bartoszek, L; Bashyal, A; Bass, M; Bay, F; Beacom, J; Behera, BR; Bellettini, G; Bellini, V; Beltramello, O; Benetti, PA; Bercellie, A; Bergevin, M; Berman, E; Berns, H; Bernstein, R; Bertolucci, S; Bhandari, B; Bhatnagar, V; Bhuyan, B; Bian, J; Biery, K; Bishai, M; Blackburn, T; Blake, A; Blaszczyk, FDM; Blaufuss, E; Bleakley, B; Blucher, E; Bocean, V; Boffelli, F; Boissevain, J; Bolognesi, S; Bolton, T; Bonesini, M; Boone, T; Booth, C; Bordoni, S; Borysova, M; Bourguille, B; Boyd, SB; Brailsford, D; Brandt, A; Bremer, J; Brice, S; Bromberg, C; Brooijmans, G; Brown, G; Brown, R; Brunetti, G; Bu, X; Buchanan, N; Budd, H; Bugg, B; Calafiura, P; Calligarich, E; Calvo, E; Camilleri, L; Campanelli, M; Cantini, C; Carls, B; Carr, R; Cascella, M; Castromonte, C; CatanoMur, E; Cavanna, F; Centro, S; CerveraVillanueva, A; Chandratre, VB; Chatterjee, A; Chattopadhyay, S; Chaussard, L; Chembra, S; Chen, H; Chen, K; Chen, M; Cherdack, D; Chi, C; Childress, S; Choubey, S; Choudhary, BC; Christodoulou, G; Christofferson, C; Church, E; Cianci, D; Cline, D; Coan, T; Cocco, A; Coelho, J; Cole, P; Collin, G; Conrad, JM; Convery, M; Corey, R; Corwin, L; Cranshaw, J; Crivelli, P; Cronin-Hennessy, D; Curioni, A; Cushing, J; Adams, DL; Dale, D; Das, SR; Davenne, T; Davies, GS; Davies, J; Dawson, J; De, K; deGouvea, A; deJong, JK; deJong, P; DeLurgio, P; Decowski, M; Delbart, A; Densham, C; Dharmapalan, R; Dhingra, N; DiLuise, S; Diamantopoulou, M; Diaz, JS; DiazBautista, G; Diwan, M; Djurcic, Z; Dolph, J; Drake, G; Duchesneau, D; Duvernois, M; Duyang, H; Dwyer, DA; Dye, S; Dytman, S; Eberly, B; Edgecock, R; Edmunds, D; Elliott, S; Elnimr, M; Emery, S; Endress, E; Eno, S; Ereditato, A; Escobar, CO; Evans, J; Falcone, A; Falk, L; Farbin, A; Farnese, C; Farzan, Y; Fava, A; Favilli, L; Felde, J; Felix, J; Fernandes, S; Fields, L; Finch, A; Fitton, M; Fleming, B; Forest, T; Fowler, J; Fox, W; Fried, J; Friedland, A; Fuess, S; Fujikawa, B; Gago, A; Gallagher, H; Galymov, S; Gamble, T; Gandhi, R; Garcia-Gamez, D; Gardiner, S; Garvey, G; Gehman, VM; Gendotti, A; Geronimo, GD; Ghag, C; Ghoshal, P; Gibin, D; Gil-Botella, I; Gill, R; Girardelli, D; Giri, A; Glavin, S; Goeldi, D; Golapinni, S; Gold, M; Gomes, RA; GomezCadenas, JJ; Goodman, MC; Gorbunov, D; Goswami, S; Graf, N; Graham, M; Gramelini, E; Gran, R; Grant, C; Grant, N; Greco, V; Greenlee, H; Greenler, L; Greenley, C; Groh, M; Grullon, S; Grundy, T; Grzelak, K; Guardincerri, E; Guarino, V; Guarnaccia, E; Guedes, GP; Guenette, R; Guglielmi, A; Habig, AT; Hackenburg, RW; Hackenburg, A; Hadavand, H; Haenni, R; Hahn, A; Haigh, MD; Haines, T; Hamernik, T; Handler, T; Hans, S; Harris, D; Hartnell, J; Hasegawa, T; Hatcher, R; Hatzikoutelis, A; Hays, S; Hazen, E; Headley, M; Heavey, A; Heeger, K; Heise, J; Hennessy, K; Hewes, J; Higuera, A; Hill, T; Himmel, A; Hogan, M; Holanda, P; Holin, A; Honey, W; Horikawa, S; Horton-Smith, G; Howard, B; Howell, J; Hurh, P; Huston, J; Hylen, J; Imlay, R; Insler, J; Introzzi, G; Ioanisyan, D; Ioannisian, A; Iwamoto, K; Izmaylov, A; Jackson, C; Jaffe, DE; James, C; James, E; Jediny, F; Jen, C; Jhingan, A; Jiménez, S; Jo, JH; Johnson, M; Johnson, R; Johnstone, J; Jones, BJ; Joshi, J; Jostlein, H; Jung, CK; Junk, T; Kaboth, A; Kadel, R; Kafka, T; Kalousis, L; Kamyshkov, Y; Karagiorgi, G; Karasavvas, D; Karyotakis, Y; Kaur, A; Kaur, P; Kayser, B; Kazaryan, N; Kearns, E; Keener, P; Kemboi, S; Kemp, E; Kettell, SH; Khabibullin, M; Khandaker, M; Khotjantsev, A; Kirby, B; Kirby, M; Klein, J; Kobilarcik, T; Kohn, S; Koizumi, G; Kopylov, A; Kordosky, M; Kormos, L; Kose, U; Kostelecky, A; Kramer, M; Kreslo, I; Kriske, R; Kropp, W; Kudenko, Y; Kudryavtsev, VA; Kulagin, S; Kumar, A; Kumar, G; Kumar, J; Kumar, L; Kutter, T; Laminack, A; Lande, K; Lane, C; Lang, K; Lanni, F; Learned, J; Lebrun, P; Lee, D; Lee, H; Lee, K; Lee, WM; LeiguideOliveira, MA; Li, Q; Li, S; Li, X; Li, Y; Li, Z; Libo, J; Lin, CS; Lin, S; Ling, J; Link, J; Liptak, Z; Lissauer, D; Littenberg, L; Littlejohn, B; Liu, Q; Liu, T; Lockwitz, S; Lockyer, N; Loew, T; Lokajicek, M; Long, K; Lopes, MDL; Lopez, JP; Losecco, J; Louis, W; Lowery, J; Luethi, M; Luk, K; Lundberg, B; Lundin, T; Luo, X; Lux, T; Lykken, J; Machado, AA; Macier, JR; Magill, S; Mahler, G; Mahn, K; Malek, M; Malhotra, S; Malon, D; Mammoliti, F; Mancina, S; Mandal, SK; Mandodi, S; Manly, SL; Mann, A; Marchionni, A; Marciano, W; Mariani, C; Maricic, J; Marino, A; Marshak, M; Marshall, C; Marshall, J; Marteau, J; Martin-Albo, J; Martinez, D; Matsuno, S; Matthews, J; Mauger, C; Mavrokoridis, K; Mayilyan, D; Mazzucato, E; McCauley, N; McCluskey, E; McConkey, N; McDonald, K; McFarland, KS; McGowan, AM; McGrew, C; McKeown, R; McNulty, D; McTaggart, R; Mefodiev, A; Mehrian, M; Mehta, P; Mei, D; Mena, O; Menary, S; Mendez, H; Menegolli, A; Meng, G; Meng, Y; Merritt, H; Mertins, D; Messier, M; Metcalf, W; Mewes, M; Meyer, H; Miao, T; Milincic, R; Miller, W; Mills, G; Mineev, O; Miranda, O; Mishra, CS; Mishra, SR; Mitrica, B; Mladenov, D; Mocioiu, I; Mohanta, R; Mokhov, N; Montanari, C; Montanari, D; Moon, J; Mooney, M; Moore, C; Morfin, J; Morgan, B; Morris, C; Morse, W; Moss, Z; Mossey, C; Moura, CA; Mousseau, J; Mualem, L; Muether, M; Mufson, S; Murphy, S; Musser, J; Musser, R; Nakajima, Y; Naples, D; Navarro, J; Navas, D; Nelson, J et al.", "published_date": " ", "doi": " ", "abstract": "A description of the proposed detector(s) for DUNE at LBNF", "publication_location": " ", "link": "http://arxiv.org/abs/1601.02984v1", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino  Experiment (DUNE) Conceptual Design Report Volume 2: The Physics Program for  DUNE at LBNF", "authors": "Collaboration, DUNE; Acciarri, R; Acero, MA; Adamowski, M; Adams, C; Adamson, P; Adhikari, S; Ahmad, Z; Albright, CH; Alion, T; Amador, E; Anderson, J; Anderson, K; Andreopoulos, C; Andrews, M; Andrews, R; Anghel, I; Anjos, JD; Ankowski, A; Antonello, M; ArandaFernandez, A; Ariga, A; Ariga, T; Aristizabal, D; Arrieta-Diaz, E; Aryal, K; Asaadi, J; Asner, D; Athar, MS; Auger, M; Aurisano, A; Aushev, V; Autiero, D; Avila, M; Back, JJ; Bai, X; Baibussinov, B; Baird, M; Balantekin, AB; Baller, B; Ballett, P; Bambah, B; Bansal, M; Bansal, S; Barker, GJ; Barletta, WA; Barr, G; Barros, N; Bartoszek, L; Bashyal, A; Bass, M; Bay, F; Beacom, J; Behera, BR; Bellettini, G; Bellini, V; Beltramello, O; Benetti, PA; Bercellie, A; Bergevin, M; Berman, E; Berns, H; Bernstein, R; Bertolucci, S; Bhandari, B; Bhatnagar, V; Bhuyan, B; Bian, J; Biery, K; Bishai, M; Blackburn, T; Blake, A; Blaszczyk, FDM; Blaufuss, E; Bleakley, B; Blucher, E; Bocean, V; Boffelli, F; Boissevain, J; Bolognesi, S; Bolton, T; Bonesini, M; Boone, T; Booth, C; Bordoni, S; Borysova, M; Bourguille, B; Boyd, SB; Brailsford, D; Brandt, A; Bremer, J; Brice, S; Bromberg, C; Brooijmans, G; Brown, G; Brown, R; Brunetti, G; Bu, X; Buchanan, N; Budd, H; Bugg, B; Calafiura, P; Calligarich, E; Calvo, E; Camilleri, L; Campanelli, M; Cantini, C; Carls, B; Carr, R; Cascella, M; Castromonte, C; CatanoMur, E; Cavanna, F; Centro, S; CerveraVillanueva, A; Chandratre, VB; Chatterjee, A; Chattopadhyay, S; Chaussard, L; Chembra, S; Chen, H; Chen, K; Chen, M; Cherdack, D; Chi, C; Childress, S; Choubey, S; Choudhary, BC; Christodoulou, G; Christofferson, C; Church, E; Cianci, D; Cline, D; Coan, T; Cocco, A; Coelho, J; Cole, P; Collin, G; Conrad, JM; Convery, M; Corey, R; Corwin, L; Cranshaw, J; Crivelli, P; Cronin-Hennessy, D; Curioni, A; Cushing, J; Adams, DL; Dale, D; Das, SR; Davenne, T; Davies, GS; Davies, J; Dawson, J; De, K; deGouvea, A; deJong, JK; deJong, P; DeLurgio, P; Decowski, M; Delbart, A; Densham, C; Dharmapalan, R; Dhingra, N; DiLuise, S; Diamantopoulou, M; Diaz, JS; DiazBautista, G; Diwan, M; Djurcic, Z; Dolph, J; Drake, G; Duchesneau, D; Duvernois, M; Duyang, H; Dwyer, DA; Dye, S; Dytman, S; Eberly, B; Edgecock, R; Edmunds, D; Elliott, S; Elnimr, M; Emery, S; Endress, E; Eno, S; Ereditato, A; Escobar, CO; Evans, J; Falcone, A; Falk, L; Farbin, A; Farnese, C; Farzan, Y; Fava, A; Favilli, L; Felde, J; Felix, J; Fernandes, S; Fields, L; Finch, A; Fitton, M; Fleming, B; Forest, T; Fowler, J; Fox, W; Fried, J; Friedland, A; Fuess, S; Fujikawa, B; Gago, A; Gallagher, H; Galymov, S; Gamble, T; Gandhi, R; Garcia-Gamez, D; Gardiner, S; Garvey, G; Gehman, VM; Gendotti, A; Geronimo, GD; Ghag, C; Ghoshal, P; Gibin, D; Gil-Botella, I; Gill, R; Girardelli, D; Giri, A; Glavin, S; Goeldi, D; Golapinni, S; Gold, M; Gomes, RA; GomezCadenas, JJ; Goodman, MC; Gorbunov, D; Goswami, S; Graf, N; Graham, M; Gramelini, E; Gran, R; Grant, C; Grant, N; Greco, V; Greenlee, H; Greenler, L; Greenley, C; Groh, M; Grullon, S; Grundy, T; Grzelak, K; Guardincerri, E; Guarino, V; Guarnaccia, E; Guedes, GP; Guenette, R; Guglielmi, A; Habig, AT; Hackenburg, RW; Hackenburg, A; Hadavand, H; Haenni, R; Hahn, A; Haigh, MD; Haines, T; Hamernik, T; Handler, T; Hans, S; Harris, D; Hartnell, J; Hasegawa, T; Hatcher, R; Hatzikoutelis, A; Hays, S; Hazen, E; Headley, M; Heavey, A; Heeger, K; Heise, J; Hennessy, K; Hewes, J; Higuera, A; Hill, T; Himmel, A; Hogan, M; Holanda, P; Holin, A; Honey, W; Horikawa, S; Horton-Smith, G; Howard, B; Howell, J; Hurh, P; Huston, J; Hylen, J; Imlay, R; Insler, J; Introzzi, G; Ioanisyan, D; Ioannisian, A; Iwamoto, K; Izmaylov, A; Jackson, C; Jaffe, DE; James, C; James, E; Jediny, F; Jen, C; Jhingan, A; Jiménez, S; Jo, JH; Johnson, M; Johnson, R; Johnstone, J; Jones, BJ; Joshi, J; Jostlein, H; Jung, CK; Junk, T; Kaboth, A; Kadel, R; Kafka, T; Kalousis, L; Kamyshkov, Y; Karagiorgi, G; Karasavvas, D; Karyotakis, Y; Kaur, A; Kaur, P; Kayser, B; Kazaryan, N; Kearns, E; Keener, P; Kemboi, S; Kemp, E; Kettell, SH; Khabibullin, M; Khandaker, M; Khotjantsev, A; Kirby, B; Kirby, M; Klein, J; Kobilarcik, T; Kohn, S; Koizumi, G; Kopylov, A; Kordosky, M; Kormos, L; Kose, U; Kostelecky, VA; Kramer, M; Kreslo, I; Kriske, R; Kropp, W; Kudenko, Y; Kudryavtsev, VA; Kulagin, S; Kumar, A; Kumar, G; Kumar, J; Kumar, L; Kutter, T; Laminack, A; Lande, K; Lane, C; Lang, K; Lanni, F; Learned, J; Lebrun, P; Lee, D; Lee, H; Lee, K; Lee, WM; LeiguideOliveira, MA; Li, Q; Li, S; Li, X; Li, Y; Li, Z; Libo, J; Lin, CS; Lin, S; Ling, J; Link, J; Liptak, Z; Lissauer, D; Littenberg, L; Littlejohn, B; Liu, Q; Liu, T; Lockwitz, S; Lockyer, N; Loew, T; Lokajicek, M; Long, K; Lopes, MDL; Lopez, JP; Losecco, J; Louis, W; Lowery, J; Luethi, M; Luk, K; Lundberg, B; Lundin, T; Luo, X; Lux, T; Lykken, J; Machado, AA; Macier, JR; Magill, S; Mahler, G; Mahn, K; Malek, M; Malhotra, S; Malon, D; Mammoliti, F; Mancina, S; Mandal, SK; Mandodi, S; Manly, SL; Mann, A; Marchionni, A; Marciano, W; Mariani, C; Maricic, J; Marino, A; Marshak, M; Marshall, C; Marshall, J; Marteau, J; Martin-Albo, J; Martinez, D; Matsuno, S; Matthews, J; Mauger, C; Mavrokoridis, K; Mayilyan, D; Mazzucato, E; McCauley, N; McCluskey, E; McConkey, N; McDonald, K; McFarland, KS; McGowan, AM; McGrew, C; McKeown, R; McNulty, D; McTaggart, R; Mefodiev, A; Mehrian, M; Mehta, P; Mei, D; Mena, O; Menary, S; Mendez, H; Menegolli, A; Meng, G; Meng, Y; Merritt, H; Mertins, D; Messier, M; Metcalf, W; Mewes, M; Meyer, H; Miao, T; Milincic, R; Miller, W; Mills, G; Mineev, O; Miranda, O; Mishra, CS; Mishra, SR; Mitrica, B; Mladenov, D; Mocioiu, I; Mohanta, R; Mokhov, N; Montanari, C; Montanari, D; Moon, J; Mooney, M; Moore, C; Morfin, J; Morgan, B; Morris, C; Morse, W; Moss, Z; Mossey, C; Moura, CA; Mousseau, J; Mualem, L; Muether, M; Mufson, S; Murphy, S; Musser, J; Musser, R; Nakajima, Y; Naples, D; Navarro, J; Navas, D et al.", "published_date": " ", "doi": " ", "abstract": "The Physics Program for the Deep Underground Neutrino Experiment (DUNE) atthe Fermilab Long-Baseline Neutrino Facility (LBNF) is described.", "publication_location": " ", "link": "http://arxiv.org/abs/1512.06148v2", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Intraocular Pressure and Big Bubble Diameter in Deep Anterior Lamellar Keratoplasty: An Ex-Vivo Microscope-Integrated OCT With Heads-Up Display Study.", "authors": "Bhullar, PK; Carrasco-Zevallos, OM; Dandridge, A; Pasricha, ND; Keller, B; Shen, L; Izatt, JA; Toth, CA; Kuo, AN", "published_date": "September 2017", "doi": "10.22608/APO.2017265", "abstract": "PURPOSE: To investigate the relationship between intraocular pressure (IOP) and big bubble (BB) formation in a model of deep anterior lamellar keratoplasty (DALK). DESIGN: Ex-vivo. METHODS: Corneoscleral buttons from human donors were loaded onto an artificial anterior chamber connected to a column of balanced salt solution. A surgeon-in-training learned to perform DALK via the BB technique using swept-source microscope-integrated optical coherence tomography (SS-MIOCT) with heads-up display (HUD). DALK procedures were performed at 6 different IOPs (5, 10, 15, 20, 30, or 40 mm Hg; n = 6 per group) in a randomized fashion, with the surgeon-in-training masked to the pressure and guided by SS-MIOCT with HUD. For a subset of corneas within each pressure group, DALK was performed on matching donor tissue at a control IOP. BB diameter was recorded, and a diameter exceeding the trephine diameter was considered optimal. RESULTS: Wilcoxon rank sum test showed a difference in BB diameter among the different pressure groups (mean ± SD of 7.75 ± 1.60, 8.33 ± 1.99, 10.9 ± 0.92, 9.08 ± 1.07, 6.67 ± 3.33, and 3.42 ± 3.77 mm in the 5, 10, 15, 20, 30, and 40 mm Hg groups, respectively; P = 0.0014). Per Tukey test, this difference was attributable to comparisons between the 40 mm Hg group and the 5, 10, 15, or 20 mm Hg groups (P = 0.04, 0.02, 0.0001, 0.004, respectively). CONCLUSIONS: In this ex-vivo model of DALK, the BB technique guided by SS-MIOCT with HUD yielded bubbles of optimal diameters only at physiologic pressures (10‒20 mm Hg). Extremely high IOP (40 mm Hg) resulted in BBs of significantly smaller diameter than BBs obtained at physiologic and low (5 mm Hg) IOPs.", "publication_location": "Asia Pacific Journal of Ophthalmology (Philadelphia, Pa.)", "link": "http://dx.doi.org/10.22608/APO.2017265", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Revisiting the softmax bellman operator: New benefits and new perspective", "authors": " ", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 2019 International Machine Learning Society (IMLS). The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the Bellman operator. Surprisingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when combined with Deep Q-learning, leads to Q-functions with superior policies in practice, even outperforming its double Q-learning counterpart. To better understand how and why this occurs, wc revisit theoretical properties of the softmax Bellman operator, and prove that (i) it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and (ii) the distance of its Q function from the optimal one can be bounded. These alone do not explain its superior performance, so we also show that the softmax operator can reduce the over-estimation error, which may give some insight into why a sub-optimal operator leads to better performance in the presence of value function approximation. A comparison among different Bellman operators is then presented, showing the trade-offs when selecting them.", "publication_location": "36th International Conference on Machine Learning, Icml 2019", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Topic Modeling with Nonparametric Markov Tree.", "authors": "Chen, H; Dunson, DB; Carin, L", "published_date": "January 2011", "doi": " ", "abstract": "A new hierarchical tree-based topic model is developed, based on nonparametric Bayesian techniques. The model has two unique attributes: (i) a child node in the tree may have more than one parent, with the goal of eliminating redundant sub-topics deep in the tree; and (ii) parsimonious sub-topics are manifested, by removing redundant usage of words at multiple scales. The depth and width of the tree are unbounded within the prior, with a retrospective sampler employed to adaptively infer the appropriate tree size based upon the corpus under study. Excellent quantitative results are manifested on five standard data sets, and the inferred tree structure is also found to be highly interpretable.", "publication_location": "Proceedings of the ... International Conference on Machine Learning. International Conference on Machine Learning", "link": "http://www.ncbi.nlm.nih.gov/pmc/articles/25279387", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adversarial time-to-event modeling", "authors": "Chapfuwa, P; Tao, C; Li, C; Page, C; Goldstein, B; Carin, L; Henao, R", "published_date": "January 1, 2018", "doi": " ", "abstract": "© 2018 by the Authors. All rights reserved. Modern health data science applications leverage abundant molecular and electronic health data; providing opportunities for machine learning to build statistical models to support clinical practice. Time-to-event analysis, also called survival analysis, stands as one of the most representative examples of such statistical models. We present a deep-network-based approach that leverages ad-versarial learning to address a key challenge in modern time-to-event modeling: nonparametric estimation of event-time distributions. We also introduce a principled cost function to exploit in-formation from censored events (events that occur subsequent to the observation window). Unlike most time-to-event models, we focus on the estimation of time-to-event distributions, rather than time ordering. We validate our model on both benchmark and real datasets, demonstrating that the proposed formulation yields significant performance gains relative to a parametric alternative, which we also propose.", "publication_location": "35th International Conference on Machine Learning, Icml 2018", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adaptive feature abstraction for translating video to language", "authors": "Pu, Y; Gan, Z; Carin, L; Min, MR", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings. All Rights Reserved. A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. A novel attention mechanism with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature “abstraction”), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on the YouTube2Text benchmark. Experimental results demonstrate quantitatively the effectiveness of our proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantic structures.", "publication_location": "5th International Conference on Learning Representations, Iclr 2017   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adaptive feature abstraction for translating video to language", "authors": "Pu, Y; Gan, Z; Carin, L; Min, MR", "published_date": "January 1, 2019", "doi": " ", "abstract": "© 5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings. All Rights Reserved. A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. A novel attention mechanism with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature “abstraction”), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on the YouTube2Text benchmark. Experimental results demonstrate quantitatively the effectiveness of our proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantic structures.", "publication_location": "5th International Conference on Learning Representations, Iclr 2017   Workshop Track Proceedings", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Restless mind, restless body.", "authors": "Seli, P; Carriere, JSA; Thomson, DR; Cheyne, JA; Martens, KAE; Smilek, D", "published_date": "May 2014", "doi": "10.1037/a0035260", "abstract": "In the present work, we investigate the hypothesis that failures of task-related executive control that occur during episodes of mind wandering are associated with an increase in extraneous movements (fidgeting). In 2 studies, we assessed mind wandering using thought probes while participants performed the metronome response task (MRT), which required them to synchronize button presses with tones. Participants performed this task while sitting on a Wii Balance Board providing us with an index of fidgeting. Results of Study 1 demonstrate that relative to on-task periods, mind wandering is indeed accompanied by increases in fidgeting, as well as increased response variability in the MRT. In Study 2, we observed that only deep mind wandering was associated with increases in fidgeting, whereas task-related response variability increased even during mild mind wandering. We interpret these findings in the context of current theories of mind wandering and suggest that (a) mind wandering is associated with costs not only to primary-task performance but also to secondary-task goals (e.g., controlling extraneous movements) and (b) these costs may depend on the degree to which task-related executive control processes are disengaged during mind wandering (i.e., depth of mind wandering).", "publication_location": "Journal of Experimental Psychology. Learning, Memory, and Cognition", "link": "http://dx.doi.org/10.1037/a0035260", "citations": "43", "readership": "105", "tweets": "4", "news_mentions": " "},
{"title": "Analyzing the role of model uncertainty for electronic health records", "authors": "Dusenberry, MW; Tran, D; Choi, E; Kemp, J; Nixon, J; Jerfel, G; Heller, K; Dai, AM", "published_date": "February 4, 2020", "doi": "10.1145/3368555.3384457", "abstract": "© 2020 Owner/Author. In medicine, both ethical and monetary costs of incorrect predictions can be significant, and the complexity of the problems often necessitates increasingly complex models. Recent work has shown that changing just the random seed is enough for otherwise well-tuned deep neural networks to vary in their individual predicted probabilities. In light of this, we investigate the role of model uncertainty methods in the medical domain. Using RNN ensembles and various Bayesian RNNs, we show that population-level metrics, such as AUC-PR, AUC-ROC, log-likelihood, and calibration error, do not capture model uncertainty. Meanwhile, the presence of significant variability in patient-specific predictions and optimal decisions motivates the need for capturing model uncertainty. Understanding the uncertainty for individual patients is an area with clear clinical impact, such as determining when a model decision is likely to be brittle. We further show that RNNs with only Bayesian embeddings can be a more efficient way to capture model uncertainty compared to ensembles, and we analyze how model uncertainty is impacted across individual input features and patient subgroups.", "publication_location": "Acm Chil 2020   Proceedings of the 2020 Acm Conference on Health, Inference, and Learning", "link": "http://dx.doi.org/10.1145/3368555.3384457", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "High school biology students transfer of the concept of natural selection: A mixed-methods approach", "authors": " ", "published_date": "2014", "doi": "10.1080/00219266.2013.801873", "abstract": "The concept of natural selection serves as a foundation for understanding diverse biological concepts and has broad applicability to other domains. However, we know little about students abilities to transfer (i.e. apply to a new context or use generatively) this concept and the relation between students conceptual understanding and transfer ability. Consequently, the purposes of this study were to describe the patterns of transfer displayed by high school biology students learning about natural selection over time, evaluate their overall level of success at transferring the concept across knowledge domains and examine the relation between conceptual understanding and level of transfer of the concept. Transfer ability and conceptual understanding were assessed using open-response items administered to 138 students in the United States. Based on the responses to these items, we identified particular patterns of surface and deep-level transfer and found that deep-level transfer was uncommon. Further, we found that deep-level transfer and conceptual understanding shared a small but significant relation; surface-level transfer was unrelated. Based on these results, we recommend that teachers explicitly focus on fostering transfer of the concept of natural selection and use specific teaching for transfer strategies, in addition to teaching for conceptual understanding. © 2013 © 2013 Society of Biology.", "publication_location": "Journal of Biological Education", "link": "http://dx.doi.org/10.1080/00219266.2013.801873", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "On the local minima of the empirical risk", "authors": "Jin, C; Ge, R; Liu, LT; Jordan, MI", "published_date": "January 1, 2018", "doi": " ", "abstract": "© 2018 Curran Associates Inc..All rights reserved. Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex nonsmooth losses (such as modern deep networks), the population risk is generally significantly more well-behaved from an optimization point of view than the empirical risk. In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function F (population risk) given only access to an approximation f (empirical risk) that is pointwise close to F (i.e., kF − fk∞ ≤ ν). Our objective is to find the -approximate local minima of the underlying function F while avoiding the shallow local minima-arising because of the tolerance ν-which exist only in f. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of f that is guaranteed to achieve our goal as long as ν ≤ O(1.5/d). We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance ν among all algorithms making a polynomial number of queries of f. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit.", "publication_location": "Advances in Neural Information Processing Systems", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Sparse modeling of human actions from motion imagery", "authors": "Castrodad, A; Sapiro, G", "published_date": "October 1, 2012", "doi": "10.1007/s11263-012-0534-7", "abstract": "An efficient sparse modeling pipeline for the classification of human actions from video is here developed. Spatio-temporal features that characterize local changes in the image are first extracted. This is followed by the learning of a class-structured dictionary encoding the individual actions of interest. Classification is then based on reconstruction, where the label assigned to each video comes from the optimal sparse linear combination of the learned basis vectors (action primitives) representing the actions. A low computational cost deep-layer model learning the inter-class correlations of the data is added for increasing discriminative power. In spite of its simplicity and low computational cost, the method outperforms previously reported results for virtually all standard datasets. © 2012 Springer Science+Business Media, LLC (outside the USA).", "publication_location": "International Journal of Computer Vision", "link": "http://dx.doi.org/10.1007/s11263-012-0534-7", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Venous thromboembolism: a public health concern.", "authors": "Beckman, MG; Hooper, WC; Critchley, SE; Ortel, TL", "published_date": "April 2010", "doi": "10.1016/j.amepre.2009.12.017", "abstract": "Venous thromboembolism (VTE), defined as deep vein thrombosis, pulmonary embolism, or both, affects an estimated 300,000-600,000 individuals in the U.S. each year, causing considerable morbidity and mortality. It is a disorder that can occur in all races and ethnicities, all age groups, and both genders. With many of the known risk factors-advanced age, immobility, surgery, obesity-increasing in society, VTE is an important and growing public health problem. Recently, a marked increase has occurred in federal and national efforts to raise awareness and acknowledge the need for VTE prevention. Yet, many basic public health functions-surveillance, research, and awareness-are still needed. Learning and understanding more about the burden and causes of VTE, and raising awareness among the public and healthcare providers through a comprehensive public health approach, has enormous potential to prevent and reduce death and morbidity from deep vein thrombosis and pulmonary embolism throughout the U.S.", "publication_location": "Am J Prev Med", "link": "http://dx.doi.org/10.1016/j.amepre.2009.12.017", "citations": "441", "readership": "366", "tweets": "3", "news_mentions": " "},
{"title": "Designing a network of critical zone observatories to explore the living skin of the terrestrial Earth", "authors": "Brantley, SL; McDowell, WH; Dietrich, WE; White, TS; Kumar, P; Anderson, SP; Chorover, J; Ann Lohse, K; Bales, RC; Richter, DD; Grant, G; Gaillardet, J", "published_date": "December 18, 2017", "doi": "10.5194/esurf-5-841-2017", "abstract": "The critical zone (CZ), the dynamic living skin of the Earth, extends from the top of the vegetative canopy through the soil and down to fresh bedrock and the bottom of the groundwater. All humans live in and depend on the CZ. This zone has three co-evolving surfaces: the top of the vegetative canopy, the ground surface, and a deep subsurface below which Earth's materials are unweathered. The network of nine CZ observatories supported by the US National Science Foundation has made advances in three broad areas of CZ research relating to the co-evolving surfaces. First, monitoring has revealed how natural and anthropogenic inputs at the vegetation canopy and ground surface cause subsurface responses in water, regolith structure, minerals, and biotic activity to considerable depths. This response, in turn, impacts aboveground biota and climate. Second, drilling and geophysical imaging now reveal how the deep subsurface of the CZ varies across landscapes, which in turn influences aboveground ecosystems. Third, several new mechanistic models now provide quantitative predictions of the spatial structure of the subsurface of the CZ.", "publication_location": "Earth Surface Dynamics", "link": "http://dx.doi.org/10.5194/esurf-5-841-2017", "citations": "25", "readership": "77", "tweets": "1", "news_mentions": " "},
{"title": "fMRI of the conscious rabbit during unilateral classical eyeblink conditioning reveals bilateral cerebellar activation.", "authors": " ", "published_date": "December 17, 2003", "doi": " ", "abstract": "The relative contributions of the ipsilateral and contralateral cerebellar cortex and deep nuclei to delay eyeblink conditioning have been debated and are difficult to survey entirely using typical electrophysiological and lesion techniques. To address these issues, we used single-event functional magnetic resonance imaging (fMRI) in the conscious rabbit to visualize the entire cerebellum simultaneously during eyeblink conditioning sessions. Examination of the blood oxygenation level-dependent (BOLD) response to a visual conditioning stimulus early in training revealed significant bilateral learning-related increases in the BOLD response in the anterior interpositus nucleus (IPA) and significant bilateral deactivation in hemispheric lobule VI (HVI) of the cerebellar cortex. Later in training, the BOLD response remained bilateral in the cortex and predominantly ipsilateral in the IPA. Conditioning stimulus-alone trials after conditioning revealed that both sides of HVI were affected similarly but that only the ipsilateral interpositus nucleus was activated. These results suggest that both sides of HVI normally influence the side of the IPA being conditioned and illustrate how fMRI can be used to examine multiple brain regions simultaneously in an awake, behaving animal to discover more rapidly the neural substrates of learning and memory.", "publication_location": "Journal of Neuroscience", "link": "http://www.ncbi.nlm.nih.gov/pmc/articles/14684877", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Optimal market intelligence strategy when management attention is scarce", "authors": "Christen, M; Boulding, W; Staelin, R", "published_date": "April 1, 2009", "doi": "10.1287/mnsc.1080.0988", "abstract": "This paper extends the theoretical literature on firms' optimal information strategies to the situation when a firm's management attention capacity to process available data is scarce. In this case, a firm's optimal market intelligence strategy must trade off learning a little about a broad range of markets (a broad strategy) with gaining a very deep understanding of one or a few markets (a focused strategy). This trade-off is not present when data are scarce, an assumption made in most of the existing literature on optimal information search strategies. However, in data-rich environments, which are of increasing relevance given technology changes, we show a focused market intelligence strategy is always best when managers need to process a substantial amount of data before beginning to gain insights; i.e., there are increasing returns to attention. Interestingly, this focused strategy can also be best with decreasing returns to attention when (a) managers are sufficiently efficient in processing the available data and (b) managers have sufficiently strong initial priors on the unknown market parameters. We show a broad market intelligence strategy is only optimal when new data points are sufficiently redundant, i.e., when the learning rate is sufficiently decreasing with the allocation of more attention. Our results also indicate that advances in information technology can account for the pressure on firms to become more focused and that competition increases the likelihood of a focused strategy. Competition can lead to asymmetric outcomes where firms focus on different markets. Finally, we note that a focused market intelligence strategy, and thus an asymmetric allocation of attention, does not require a priori differences between firms, markets, or market-specific core capabilities. Consequently, a focused market intelligence strategy can result in marketspecific core competencies and produce firm differences from equivalent starting conditions. © 2009 INFORMS.", "publication_location": "Management Science", "link": "http://dx.doi.org/10.1287/mnsc.1080.0988", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adaptive fog-based output security for augmented reality", "authors": "Ahn, S; Gorlatova, M; Naghizadeh, P; Chiang, M; Mittal, P", "published_date": "August 7, 2018", "doi": "10.1145/3229625.3229626", "abstract": "© 2018 Copyright held by the owner/author(s). Augmented reality (AR) technologies are rapidly being adopted across multiple sectors, but little work has been done to ensure the security of such systems against potentially harmful or distracting visual output produced by malicious or bug-ridden applications. Past research has proposed to incorporate manually specified policies into AR devices to constrain their visual output. However, these policies can be cumbersome to specify and implement, and may not generalize well to complex and unpredictable environmental conditions. We propose a method for generating adaptive policies to secure visual output in AR systems using deep reinforcement learning. This approach utilizes a local fog computing node, which runs training simulations to automatically learn an appropriate policy for filtering potentially malicious or distracting content produced by an application. Through empirical evaluations, we show that these policies are able to intelligently displace AR content to reduce obstruction of real-world objects, while maintaining a favorable user experience.", "publication_location": "Vr/Ar Network 2018   Proceedings of the 2018 Morning Workshop on Virtual Reality and Augmented Reality Network, Part of Sigcomm 2018", "link": "http://dx.doi.org/10.1145/3229625.3229626", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Confidence level estimation in multi-target classification problems", "authors": "Chang, S; Isaacs, J; Fu, B; Shin, J; Zhu, P; Ferrari, S", "published_date": "January 1, 2018", "doi": "10.1117/12.2319988", "abstract": "© 2018 SPIE. This paper presents an approach for estimating the confidence level in automatic multi-target classification performed by an imaging sensor on an unmanned vehicle. An automatic target recognition algorithm comprised of a deep convolutional neural network in series with a support vector machine classifier detects and classifies targets based on the image matrix. The joint posterior probability mass function of target class, features, and classification estimates is learned from labeled data, and recursively updated as additional images become available. Based on the learned joint probability mass function, the approach presented in this paper predicts the expected confidence level of future target classifications, prior to obtaining new images. The proposed approach is tested with a set of simulated sonar image data. The numerical results show that the estimated confidence level provides a close approximation to the actual confidence level value determined a posteriori, i.e. after the new image is obtained by the on-board sensor. Therefore, the expected confidence level function presented in this paper can be used to adaptively plan the path of the unmanned vehicle so as to optimize the expected confidence levels and ensure that all targets are classified with satisfactory confidence after the path is executed.", "publication_location": "Smart Structures and Materials 2005: Active Materials: Behavior and Mechanics", "link": "http://dx.doi.org/10.1117/12.2319988", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "EXPERIMENTAL STUDIES OF THE ACOUSTIC SIGNATURE OF PROTON BEAMS TRAVERSING FLUID MEDIA.", "authors": "Levi, M; Armstrong, T; Baranger, H; Bregman, M; Mael, D; Strait, J; Sulak, L; Bowen, T; Pifer, B; Polakos, P; Bradner, H; Parvulescu, A; Jones, H; Learned, J", "published_date": "1977", "doi": " ", "abstract": "This work establishes that a detectable sonic signal is produced by protons while traversing through or stopping in a fluid medium. Experiments exploring the global characteristics of both the acoustic generation mechanism and the radiation pattern performed at three different accelerators. The results are consistent with a simple thermal model for the transformation of the energy of moving charged particles into acoustic energy. This phenomenon could be exploited in several applications: (1) as a charged particle monitor in accelerator beams, (2) as a heavy ion detector sensitive to nuclear charge, e. g. , in measuring the cosmic ray isotope as an inexpensive shower detector in massive neutrino detectors at the next generation of high energy accelerators, e. g, the Fermilab energy doubler and (4) as the shower calorimeter (and perhaps the muon detector) in massive deep underwater detectors of cosmic neutrino and muon interactions.", "publication_location": "Ieee Transactions on Nuclear Science", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Program Evaluation and Causal Inference With High-Dimensional Data", "authors": "Belloni, A; Chernozhukov, V; Fernández-Val, I; Hansen, C", "published_date": "January 1, 2017", "doi": "10.3982/ECTA12723", "abstract": "© 2017 The Econometric Society In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced-form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced-form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets. The results on program evaluation are obtained as a consequence of more general results on honest inference in a general moment-condition framework, which arises from structural equation models in econometrics. Here, too, the crucial ingredient is the use of orthogonal moment conditions, which can be constructed from the initial moment conditions. We provide results on honest inference for (function-valued) parameters within this general framework where any high-quality, machine learning methods (e.g., boosted trees, deep neural networks, random forest, and their aggregated and hybrid versions) can be used to learn the nonparametric/high-dimensional components of the model. These include a number of supporting auxiliary results that are of major independent interest: namely, we (1) prove uniform validity of a multiplier bootstrap, (2) offer a uniformly valid functional delta method, and (3) provide results for sparsity-based estimation of regression functions for function-valued outcomes.", "publication_location": "Econometrica", "link": "http://dx.doi.org/10.3982/ECTA12723", "citations": "55", "readership": "237", "tweets": "11", "news_mentions": " "},
{"title": "Clinical subthalamic nucleus prediction from high-field brain MRI", "authors": "Kim, J; Duchin, Y; Sapiro, G; Vitek, J; Harel, N", "published_date": "January 1, 2015", "doi": "10.1109/ISBI.2015.7164104", "abstract": "© 2015 IEEE. The subthalamic nucleus (STN) within the sub-cortical region of the Basal ganglia is a crucial targeting structure for Parkinson's Deep brain stimulation (DBS) surgery. Volumetric segmentation of such small and complex structure, which is elusive in clinical MRI protocols, is thereby a pre-requisite process for reliable DBS direct targeting. While direct visualization of the STN is facilitated with advanced ultrahigh-field MR imaging (7 Tesla), such high fields are not always clinically available. In this paper, we aim at the automatic prediction of the STN region on clinical low-field MRI, exploiting dependencies between the STN and its adjacent structures, learned from ultrahigh-field MRI. We present a framework based on a statistical shape model to learn such shape relationship on high quality MR data sets. This allows for an accurate prediction and visualization of the STN structure, given detectable predictors on the low-field MRI. Experimental results on Parkinson's patients demonstrate that the proposed approach enables accurate estimation of the STN on clinical 1.5T MRI.", "publication_location": "Proceedings   International Symposium on Biomedical Imaging", "link": "http://dx.doi.org/10.1109/ISBI.2015.7164104", "citations": "3", "readership": "10", "tweets": " ", "news_mentions": " "},
{"title": "Memory retraining in brain-damaged patients: the airplane list.", "authors": " ", "published_date": "March 1, 1979", "doi": "10.1016/s0010-9452(79)80013-1", "abstract": "Recall failures can be traced to inadequate encoding and unfocussed retrieval search. A method is given for encouraging deep and elaborate encoding of a list of words, and two brain-damaged patients who were unable to recall the list at first, were shown to retrieve many of the words when retrieval cues were given by the examiner.", "publication_location": "Cortex; a Journal Devoted to the Study of the Nervous System and Behavior", "link": "http://dx.doi.org/10.1016/s0010-9452(79)80013-1", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Distinct contributions of small and large conductance Ca2+-activated K+ channels to rat Purkinje neuron function.", "authors": " ", "published_date": "April 2003", "doi": "10.1113/jphysiol.2002.027854", "abstract": "The cerebellum is important for many aspects of behaviour, from posture maintenance and goal-oriented reaching movements to timing tasks and certain forms of learning. In every case, information flowing through the cerebellum passes through Purkinje neurons, which receive input from the two primary cerebellar afferents and generate continuous streams of action potentials that constitute the sole output from the cerebellar cortex to the deep nuclei. The tonic firing behaviour observed in Purkinje neurons in vivo is maintained in brain slices even when synaptic inputs are blocked, suggesting that Purkinje neuron activity relies to a significant extent on intrinsic conductances. Previous research has suggested that the interplay between Ca2+ currents and Ca2+-activated K+ channels (KCa channels) is important for Purkinje cell activity, but how many different KCa channel types are present and what each channel type contributes to cell behaviour remains unclear. In order to better understand the ionic mechanisms that control the behaviour of these neurons, we investigated the effects of different Ca2+ channel and KCa channel antagonists on Purkinje neurons in acute slices of rat cerebellum. Our data show that Ca2+ entering through P-type voltage-gated Ca2+ channels activates both small-conductance (SK) and large-conductance (BK) KCa channels. SK channels play a role in setting the intrinsic firing frequency, while BK channels regulate action potential shape and may contribute to the unique climbing fibre response.", "publication_location": "The Journal of Physiology", "link": "http://dx.doi.org/10.1113/jphysiol.2002.027854", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Model-based and data-driven approaches for building automation and control", "authors": "Wei, T; Chen, X; Li, X; Zhu, Q", "published_date": "November 5, 2018", "doi": "10.1145/3240765.3243485", "abstract": "© 2018 ACM. Smart buildings in the future are complex cyber-physical-human systems that involve close interactions among embedded platform (for sensing, computation, communication and control), mechanical components, physical environment, building architecture, and occupant activities. The design and operation of such buildings require a new set of methodologies and tools that can address these heterogeneous domains in a holistic, quantitative and automated fashion. In this paper, we will present our design automation methods for improving building energy efficiency and offering comfortable services to occupants at low cost. In particular, we will highlight our work in developing both model-based and data-driven approaches for building automation and control, including methods for co-scheduling heterogeneous energy demands and supplies, for integrating intelligent building energy management with grid optimization through a proactive demand response framework, for optimizing HVAC control with deep reinforcement learning, and for accurately measuring in-building temperature by combining prior modeling information with few sensor measurements based upon Bayesian inference.", "publication_location": "Ieee/Acm International Conference on Computer Aided Design, Digest of Technical Papers, Iccad", "link": "http://dx.doi.org/10.1145/3240765.3243485", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "From Neosentience to Recombinant Informatics, A Research Overview", "authors": "Seaman, W", "published_date": "2013", "doi": " ", "abstract": "An overview of Seaman’s research relevant to new forms of Computer Science, Technology and Application covers a number of different theoretical research areas and approaches including the examination of new (and historical) studies that seek to look holistically at particular relations at operation in the mind/brain/body/environment, functioning across different scales. Seaman is interested in what he calls a multi-perspective approach to knowledge production. He is deeply interested in the trajectory of research started in Cybernetics at the Biological Research Lab headed by Heinz von Foerster [1][2][3][4][5][6], yet points to work by Rashevsky [7] as an important precursor, along with McCulloch and Pitts[8]. Cyberneticist Gordon Pask is of particular interest because he also explored a range of pursuits form the sciences[9][10][11][12] to the arts [13] to architecture [14] and learning systems/conversation theory, and ‘teaching machines’[15]. Pask, like von Foerster was also interested in new forms of computation and dynamic interactivity. We can trace a lineage from studies in Cybernetics to the study of contemporary complex systems. From this broad research agenda Seaman seeks to work toward the creation of new computational systems and in particular, new approaches to transdisciplinary research that point at forms of computation in the body, and explore these through biomimetics and bio-abstraction. As a media researcher, Seaman seeks to bring his knowledge of computational media, and broad historical research drawn from multiple disciplinary fields, into dynamic relationality. Central to this broad set of studies has been the articulation with scientist Otto Rössler, of a series of disparate research areas that might be enfolded in the creation of a new form of robotic leaning system, that again draws from biomimetics and bio-abstraction, discussed in their book – Neosentience | The Benevolence Engine.[16] This book provides a series of micro-chapters that let researchers explore a set of disparate foci in a non-linear manner. Seaman describes this approach as “Recombinant Informatics”[17] — the juxtaposition, combination and recombination of differing informational contexts in the service of insight production. Seaman’s broad theoretical research agenda focuses in part on entailment structures [18] and the deep study of multiple physical processes as they contribute to “computation” in the body[19]; the long-term expansion of computational forms relevant to Neuroscience—exploring the notion of creating an electrochemical computer inspired by biomimetics and bio-abstraction[20][21]; the development of a network of computers inspired by computation in the body— The Engine of Engines; the bringing to light of relevant biologically related historical research that to some degree has been left out of the central discourses in multiple scientific disciplines and in computer science— in particular work done at the Biological Computer Lab headed by Heintz von Foerster [22]. In order to deal with the complexity of these issues in a pragmatic manner Seaman along with Todd Berreth and Olivier Perriquet are working on the creation of a computational system to help facilitate this research and other disciplinary, interdisciplinary, and transdisciplinary research relevant to Computer Science, Technology and Application, entitled the Insight Engine.[23] [24]", "publication_location": "Computer Science, Technology, and Application", "link": null, "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "The INTERSPEECH 2019 computational paralinguistics challenge: Styrian dialects, continuous sleepiness, baby sounds & Orca activity", "authors": "Schuller, BW; Batliner, A; Bergler, C; Pokorny, FB; Krajewski, J; Cychosz, M; Vollmann, R; Roelen, SD; Schnieder, S; Bergelson, E; Cristia, A; Seidl, A; Warlaumont, AS; Yankowitz, L; Nöth, E; Amiriparian, S; Hantke, S; Schmitt, M", "published_date": "January 1, 2019", "doi": "10.21437/Interspeech.2019-1122", "abstract": "© 2019 ISCA The INTERSPEECH 2019 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the Styrian Dialects Sub-Challenge, three types of Austrian-German dialects have to be classified; in the Continuous Sleepiness Sub-Challenge, the sleepiness of a speaker has to be assessed as regression problem; in the Baby Sound Sub-Challenge, five types of infant sounds have to be classified; and in the Orca Activity Sub-Challenge, orca sounds have to be detected. We describe the Sub-Challenges and baseline feature extraction and classifiers, which include data-learnt (supervised) feature representations by the 'usual' ComParE and BoAW features, and deep unsupervised representation learning using the AUDEEP toolkit.", "publication_location": "Proceedings of the Annual Conference of the International Speech Communication Association, Interspeech", "link": "http://dx.doi.org/10.21437/Interspeech.2019-1122", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Ultrasound examination of the achilles tendon", "authors": "Reach, JS; Nunley, JA", "published_date": "December 1, 2009", "doi": "10.1007/978-0-387-79205-7_2", "abstract": "There is growing evidence that the clinical and operative use of ultrasound imaging can benefit patients with Achilles tendon pathology. Although ultrasonography has been stigmatized as too operator dependent and necessitating a steep learning curve, it has been our experience that the modality is relatively straightforward. Surgeons are in a unique position to utilize this technology, as they have the firm grasp of anatomy that is essential to the interpretation of sonograms. Ultrasound provides real-time dynamic imaging in the office and in the operative setting that directly benefits our patients. Ultrasound images can assist the clinician in determining the exact pathologic process, including location of symptoms, assessment of concurrent pathology, response to treatment, and the planning and intraoperative assessment of tendinous pathology. The Achilles tendon is the largest superficially located tendon of the body and is ideally suited to ultrasound examination. Achilles tendon morphology is best visualized with a high-frequency linear transducer (5 to 10 MHz). After an appropriate history and physical is obtained, the patient is positioned prone with both lower extremities disrobed (Fig. 2.1). As with all tendons, images of the Achilles tendon should be obtained in two orthogonal planes. Longitudinal and transverse images should be obtained from the musculotendinous junction to the distal tendon insertion on the calcaneus. To obtain an orderly and reproducible examination, imaging begins directly over the area of pain or tenderness with the foot in the gravity neutral position. In general, transverse imaging is followed by longitudinal imaging. Because ultrasound is a dynamic imaging modality, movie clips of the examination can be saved in the patient's electronic record. After the static examination of the area of interest, the surgeon grasps the patient's foot, introducing passive dorsiflexion and plantarflexion. This is followed by visualization of the symptomatic area with active range of motion. Finally, a full examination of the tendon from its most distal insertion proximally to the myotendinous junction is performed. Obviously, examination may be extended to the gastrocnemius and soleus muscles, depending on the clinical situation. The normal Achilles tendon, as shown in Figure 2.2, appears echogenic (bright) with an organized fibrillar ultrastructure. The paratenon envelops the tendon as a thin, echogenic tissue layer clearly distinct from the tendon under dynamic imaging. The sural nerve and accompanying lesser saphenous vein course from medial to lateral and proximally to distally along the tendon. The plantaris tendon runs along the medial side of the tendon distally. Deep to the Achilles tendon lies the bipennate muscle belly of the flexor hallucis longus. In most cases, a diagnosis can be made based on the two-dimensional (2D) real-time appearance of the tendon; an acutely ruptured Achilles tendon is clearly identified by a discontinuity of collagen fibrils separated by hypoechoic hematoma (Fig. 2.3). Chronic ruptures demonstrate attenuation of the tendon and echogenic fat herniating into the defect (Fig. 2.4). The calcification and tendinosis pathognomonic of insertional disease can be easily correlated with physical examination findings (Fig. 2.5). In insertional disease, the examiner may use the transducer to palpate and image the osseotendinous junction simultaneously. It should be stressed that all findings must be confirmed in two planes. © Springer Science+Business Media, LLC 2009.", "publication_location": " ", "link": "http://dx.doi.org/10.1007/978-0-387-79205-7_2", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Neuroscience and the fallacies of functionalism.", "authors": "Reddy, WM", "published_date": "January 2010", "doi": "10.1111/j.1468-2303.2010.00551.x", "abstract": "Smail's \"On Deep History and the Brain\" is rightly critical of the functionalist fallacies that have plagued evolutionary theory, sociobiology, and evolutionary psychology. However, his attempt to improve on these efforts relies on functional explanations that themselves oversimplify the lessons of neuroscience. In addition, like explanations in evolutionary psychology, they are highly speculative and cannot be confirmed or disproved by evidence. Neuroscience research is too diverse to yield a single picture of brain functioning. Some recent developments in neuroscience research, however, do suggest that cognitive processing provides a kind of “operating system” that can support a great diversity of cultural material. These developments include evidence of “top-down” processing in motor control, in visual processing, in speech recognition, and in “emotion regulation.” The constraints that such a system may place on cultural learning and transmission are worth investigating. At the same time, historians are well advised to remain wary of the pitfalls of functionalism.", "publication_location": "History and Theory", "link": "http://dx.doi.org/10.1111/j.1468-2303.2010.00551.x", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Quantitative assessment of procedural competence. A prospective study of training in endoscopic retrograde cholangiopancreatography.", "authors": "Jowell, PS; Baillie, J; Branch, MS; Affronti, J; Browning, CL; Bute, BP", "published_date": "December 15, 1996", "doi": "10.7326/0003-4819-125-12-199612150-00009", "abstract": "BACKGROUND: Endoscopic retrograde cholangiopancreatography (ERCP) is a technically demanding procedure that can cause substantial complications. Competence in performing ERCP and the learning curve for achieving competence are poorly understood. OBJECTIVE: To evaluate the number of supervised ERCPs that physicians must do to achieve procedural competence. Competence was defined as a 0.8 probability of successfully completing specific technical components of ERCP and an overall grading of competence as judged by the attending physician. DESIGN: Prospective study. SETTING: University training program for gastroenterologists. PARTICIPANTS: 17 gastroenterology fellows at various stages of training. MEASURES: Experienced therapeutic endoscopists prospectively graded gastroenterology fellows during 1796 consecutive ERCPs. Fellows were graded on their overall level of competence for the procedure and on specific technical components of ERCP. RESULTS: Grading data were available for 1450 ERCPs (81%). The number of ERCPs done before adequate skill was achieved was 160 for cholangiography, 140 for pancreatography, 160 for deep cannulation of the pancreatic duct, 120 for stone extraction, and 60 for stent insertion. Fellows achieved overall competence after completing 180 to 200 ERCPs. The predicted probability of overall competence was 0.8 after 137 ERCPs and 0.9 after 185 ERCPs. CONCLUSIONS: At least 180 ERCPs were required before these gastroenterology fellows could be considered competent in ERCP. This number is much greater than that previously recommended, and these findings have substantial implications for training guidelines and issues of competence and certification in ERCP. The methods used to define and evaluate competence in ERCP could also be used to assess competence in other medical procedures.", "publication_location": "Annals of Internal Medicine", "link": "http://dx.doi.org/10.7326/0003-4819-125-12-199612150-00009", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Adaptive mitigation of parameter variations", "authors": "Firouzi, F; Ye, F; Kiamehr, S; Chakrabarty, K; Tahoori, MB", "published_date": "January 1, 2014", "doi": "10.1109/ATS.2014.21", "abstract": "© 2014 IEEE. In the deep nanoscale regime, process and runtime variations have emerged as the major sources of uncertainty and unpredictability in circuit operation. Static mitigation approaches do not consider the dependence of variations on workload and chip usage, while adaptive techniques do not incorporate detailed circuit-level information. We propose a fine-grained adaptive technique in which machine learning is exploited to perform circuit clustering and obtain a representative for each cluster. By monitoring the representative in each cluster at runtime, performance variations in the entire cluster can be tracked such that appropriate fine-grained adaptation can be applied to each cluster. Experimental results for ISCAS'89, IWLS'05, and ITC'99 benchmarks as well as the LEON processor show that the proposed approach introduces negligible overhead significantly extends circuit lifetime, facilitates higher operating frequencies, and reduces the leakage power.", "publication_location": "Proceedings of the Asian Test Symposium", "link": "http://dx.doi.org/10.1109/ATS.2014.21", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Dynamic evolution of base composition: causes and consequences in avian phylogenomics.", "authors": "Nabholz, B; Künstner, A; Wang, R; Jarvis, ED; Ellegren, H", "published_date": "August 2011", "doi": "10.1093/molbev/msr047", "abstract": "Resolving the phylogenetic relationships among birds is a classical problem in systematics, and this is particularly so when it comes to understanding the relationships among Neoaves. Previous phylogenetic inference of birds has been limited to mitochondrial genomes or a few nuclear genes. Here, we apply deep brain transcriptome sequencing of nine bird species (several passerines, hummingbirds, dove, parrot, and emu), using next-generation sequencing technology to understand features of transcriptome evolution in birds and how this affects phylogenetic inference, and combine with data from two bird species using first generation technology. The phylogenomic data matrix comprises 1,995 genes and a total of 0.77 Mb of exonic sequence. First, we find an unexpected heterogeneity in the evolution of base composition among avian lineages. There is a pronounced increase in guanine + cytosine (GC) content in the third codon position in several independent lineages, with the strongest effect seen in passerines. Second, we evaluate the effect of GC content variation on phylogenetic reconstruction. We find important inconsistencies between the topologies obtained with or without taking GC variation into account, each supporting different conclusions of past studies and also influencing hypotheses on the evolution of the trait of vocal learning. Third, we demonstrate a link between GC content evolution and recombination rate and, focusing on the zebra finch lineage, find that recombination seems to drive GC content. Although we cannot reveal the causal relationships, this observation is consistent with the model of GC-biased gene conversion. Finally, we use this unparalleled amount of avian sequence data to study the rate of molecular evolution, calibrated by fossil evidence and augmented with data from alligator transcriptome sequencing. There is a 2- to 3-fold variation in substitution rate among lineages with passerines being the most rapidly evolving and ratites the slowest. This study illustrates the potential of next-generation sequencing for phylogenomic studies but also the pitfalls when using genome-wide data with heterogeneous base composition.", "publication_location": "Molecular Biology and Evolution", "link": "http://dx.doi.org/10.1093/molbev/msr047", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "An efficient re-scaled perceptron algorithm for conic systems", "authors": "Belloni, A; Freund, RM; Vempala, SS", "published_date": "January 1, 2007", "doi": "10.1007/978-3-540-72927-3_29", "abstract": "The classical perceptron algorithm is an elementary algorithm for solving a homogeneous linear inequality system Ax > 0, with many important applications in learning theory (e.g., [11,8]). A natural condition measure associated with this algorithm is the Euclidean width T of the cone of feasible solutions, and the iteration complexity of the perceptron algorithm is bounded by 1/τ2. Dunagan and Vempala [5] have developed a re-scaled version of the perceptron algorithm with an improved complexity of O(n ln(1/τ)) iterations (with high probability), which is theoretically efficient in τ, and in particular is polynomial-time in the bit-length model. We explore extensions of the concepts of these perceptron methods to the general homogeneous conic system Ax ∈ int K where if is a regular convex cone. We provide a conic extension of the re-scaled perceptron algorithm based on the notion of a deep-separation oracle of a cone, which essentially computes a certificate of strong separation. We give a general condition under which the re-scaled perceptron algorithm is theoretically efficient, i.e., polynomial-time; this includes the cases when K is the cross-product of half-spaces, second-order cones, and the positive semi-definite cone. © Springer-Verlag Berlin Heidelberg 2007.", "publication_location": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "link": "http://dx.doi.org/10.1007/978-3-540-72927-3_29", "citations": " ", "readership": " ", "tweets": " ", "news_mentions": " "},
{"title": "Autistic-like behaviour and cerebellar dysfunction in Purkinje cell Tsc1 mutant mice.", "authors": "Tsai, PT; Hull, C; Chu, Y; Greene-Colozzi, E; Sadowski, AR; Leech, JM; Steinberg, J; Crawley, JN; Regehr, WG; Sahin, M", "published_date": "August 30, 2012", "doi": "10.1038/nature11310", "abstract": "Autism spectrum disorders (ASDs) are highly prevalent neurodevelopmental disorders, but the underlying pathogenesis remains poorly understood. Recent studies have implicated the cerebellum in these disorders, with post-mortem studies in ASD patients showing cerebellar Purkinje cell (PC) loss, and isolated cerebellar injury has been associated with a higher incidence of ASDs. However, the extent of cerebellar contribution to the pathogenesis of ASDs remains unclear. Tuberous sclerosis complex (TSC) is a genetic disorder with high rates of comorbid ASDs that result from mutation of either TSC1 or TSC2, whose protein products dimerize and negatively regulate mammalian target of rapamycin (mTOR) signalling. TSC is an intriguing model to investigate the cerebellar contribution to the underlying pathogenesis of ASDs, as recent studies in TSC patients demonstrate cerebellar pathology and correlate cerebellar pathology with increased ASD symptomatology. Functional imaging also shows that TSC patients with ASDs display hypermetabolism in deep cerebellar structures, compared to TSC patients without ASDs. However, the roles of Tsc1 and the sequelae of Tsc1 dysfunction in the cerebellum have not been investigated so far. Here we show that both heterozygous and homozygous loss of Tsc1 in mouse cerebellar PCs results in autistic-like behaviours, including abnormal social interaction, repetitive behaviour and vocalizations, in addition to decreased PC excitability. Treatment of mutant mice with the mTOR inhibitor, rapamycin, prevented the pathological and behavioural deficits. These findings demonstrate new roles for Tsc1 in PC function and define a molecular basis for a cerebellar contribution to cognitive disorders such as autism.", "publication_location": "Nature", "link": "http://dx.doi.org/10.1038/nature11310", "citations": "471", "readership": "694", "tweets": "16", "news_mentions": "6"}
]